# AutoGen Overview

AutoGen is a framework for building AI agents and applications, offering tools for creating conversational agents, multi-agent systems, and more. It includes several components:

## Magentic-One CLI
A console-based multi-agent assistant for web and file-based tasks.
```bash
pip install -U magentic-one-cli
m1 "Find flights from Seattle to Paris and format the result in a table"
```

## Studio
An app for prototyping and managing agents without writing code.
```bash
pip install -U autogenstudio
autogenstudio ui --port 8080 --appdir ./myapp
```

## AgentChat
A framework for building conversational single and multi-agent applications. Requires Python 3.10+.
```python
# pip install -U "autogen-agentchat" "autogen-ext[openai]"

import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    agent = AssistantAgent("assistant", OpenAIChatCompletionClient(model="gpt-4o"))
    print(await agent.run(task="Say 'Hello World!'"))

asyncio.run(main())
```

## Core
An event-driven framework for building scalable multi-agent AI systems, suitable for workflows and distributed agent systems.

## Extensions
Implementations of Core and AgentChat components that interface with external services. Examples include:
- `LangChainToolAdapter` for LangChain tools.
- `OpenAIAssistantAgent` for Assistant API.
- `DockerCommandLineCodeExecutor` for running code in Docker.
- `GrpcWorkerAgentRuntime` for distributed agents.

# Magentic-One Overview

Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks. It uses `autogen-agentchat` for a modular interface.

### Getting Started
Install the required packages:
```bash
pip install "autogen-agentchat" "autogen-ext[magentic-one,openai]"
playwright install --with-deps chromium
```

### Example Usage
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    team = MagenticOneGroupChat([AssistantAgent("Assistant", model_client=model_client)], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof for Fermat's Last Theorem"))

asyncio.run(main())
```

### Architecture
Magentic-One uses a multi-agent architecture with agents like:
- **Orchestrator**: Leads task decomposition and planning.
- **WebSurfer**: Manages web browser actions.
- **FileSurfer**: Reads and navigates local files.
- **Coder**: Specializes in writing code.
- **ComputerTerminal**: Executes code and installs libraries.

### Caution
Ensure a safe environment by using containers, virtual environments, monitoring logs, and limiting access to sensitive data.

# Citation
For academic use, refer to the following citation:
```plaintext
@misc{fourney2024magenticonegeneralistmultiagentsolving,
  title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},
  author={Adam Fourney and others},
  year={2024},
  eprint={2411.04468},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2411.04468},
}
```

# autogen_agentchat.teams Module

This module provides implementations of various pre-defined multi-agent teams, inheriting from `BaseGroupChat`.

### Example using `RoundRobinGroupChat`
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat

# Example code here
```

This summary provides an overview of the AutoGen framework and its components, focusing on key functionalities and usage examples.

The provided documentation outlines the usage of various classes and methods for creating and managing group chat teams using AI agents. Here's a concise summary of the key components and their functionalities:

### Key Classes and Methods

1. **OpenAIChatCompletionClient**: Used to interact with OpenAI's GPT models, such as "gpt-4o".

2. **AssistantAgent**: Represents an AI agent that can participate in group chats.

3. **RoundRobinGroupChat**: Manages a group chat where participants take turns in a round-robin fashion. It can be initialized with a list of agents and a termination condition.

   ```python
   class RoundRobinGroupChat(participants: List[ChatAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None)
   ```

4. **MagenticOneGroupChat**: A group chat managed by the MagenticOneOrchestrator, designed for complex task-solving with multiple agents.

5. **SelectorGroupChat**: Uses a model to select the next speaker in a group chat based on a prompt.

6. **Swarm**: A group chat where the next speaker is selected based on a handoff message.

7. **MultimodalWebSurfer**: A multimodal agent that acts as a web surfer, capable of interacting with web pages using a browser.

   ```python
   class MultimodalWebSurfer(name: str, model_client: ChatCompletionClient, downloads_folder: str | None = None, ...)
   ```

### Example Usage

- **Running a RoundRobinGroupChat**:
  ```python
  async def main() -> None:
      model_client = OpenAIChatCompletionClient(model="gpt-4o")
      agent1 = AssistantAgent("Assistant1", model_client=model_client)
      agent2 = AssistantAgent("Assistant2", model_client=model_client)
      termination = MaxMessageTermination(3)
      team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
      stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
      async for message in stream:
          print(message)
      await team.reset()
      stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
      async for message in stream:
          print(message)
  asyncio.run(main())
  ```

- **Using a CancellationToken**:
  ```python
  async def main() -> None:
      model_client = OpenAIChatCompletionClient(model="gpt-4o")
      agent1 = AssistantAgent("Assistant1", model_client=model_client)
      agent2 = AssistantAgent("Assistant2", model_client=model_client)
      termination = MaxMessageTermination(3)
      team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
      cancellation_token = CancellationToken()
      run_task = asyncio.create_task(team.run(task="Count from 1 to 10, respond one at a time.", cancellation_token=cancellation_token))
      await asyncio.sleep(1)
      cancellation_token.cancel()
      await run_task
  asyncio.run(main())
  ```

- **Creating a MultimodalWebSurfer**:
  ```python
  async def main() -> None:
      web_surfer_agent = MultimodalWebSurfer(name="MultimodalWebSurfer", model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"))
      agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)
      stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
      await Console(stream)
      await web_surfer_agent.close()
  asyncio.run(main())
  ```

### Notes

- **Resuming Teams**: The `resume()` method allows resuming paused teams, requiring the implementation of `on_resume()` in agent classes.
- **State Management**: The `save_state()` method saves the state of the group chat team, useful for resuming or transferring states.
- **Experimental Features**: Some features are experimental and may change in future versions.

This documentation provides a comprehensive guide to setting up and managing AI-driven group chats using various orchestration strategies and tools.

The provided documentation outlines the functionality of various components and classes within a multi-agent system, specifically focusing on the `Magentic-One` system and its related components. Here's a concise summary of the key elements:

### Key Classes and Methods

#### `MultimodalWebSurferConfig`
- **Purpose**: Create a new instance of a component from a configuration object.
- **Methods**:
  - `odalWebSurferConfig(config: T) -> Self`: Initializes a new component instance.
  - `_to_config() -> MultimodalWebSurferConfig`: Dumps the configuration for creating a new instance.

#### `PlaywrightController`
- **Purpose**: Helper class for interacting with web pages using Playwright.
- **Methods**:
  - `async add_cursor_box(page: Page, identifier: str) -> None`: Adds a cursor box around an element.
  - `async click_id(page: Page, identifier: str) -> Page | None`: Clicks an element by identifier.
  - `async fill_id(page: Page, identifier: str, value: str, press_enter: bool = True) -> None`: Fills an element with a value.
  - `async visit_page(page: Page, url: str) -> Tuple[bool, bool]`: Visits a specified URL.

#### `FileSurfer`
- **Purpose**: Acts as a local file previewer, capable of reading various file types.
- **Methods**:
  - `classmethod _from_config(config: FileSurferConfig) -> Self`: Creates a new instance from a configuration.
  - `_to_config() -> FileSurferConfig`: Dumps the configuration for a new instance.

#### `MagenticOne`
- **Purpose**: A multi-agent system for solving complex tasks using various agents like `FileSurfer`, `WebSurfer`, and `Coder`.
- **Installation**:
  ```bash
  pip install "autogen-ext[magentic-one]"
  ```
- **Architecture**: Utilizes an Orchestrator agent for task planning and execution, supported by specialized agents for web browsing, file handling, and coding.

### Usage Examples

#### Basic Usage
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console

async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)

if __name__ == "__main__":
    asyncio.run(example_usage())
```

#### Human-in-the-Loop Mode
```python
async def example_usage_hil():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client, hil_mode=True)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)

if __name__ == "__main__":
    asyncio.run(example_usage_hil())
```

### Caution and Best Practices
- **Security**: Use containers and virtual environments to isolate agents and prevent unauthorized access.
- **Monitoring**: Keep logs and have human oversight to mitigate risks.
- **Access Control**: Limit internet and resource access to prevent unintended actions.

This summary provides an overview of the components and their usage within the `Magentic-One` system, highlighting the key functionalities and best practices for implementation.

To cancel a task in an asynchronous environment using the `autogen_agentchat` library, you can use the `CancellationToken` to manage task cancellation. Here's a concise example demonstrating how to set up a task with a cancellation token and cancel it after a delay:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task

asyncio.run(main())
```

### Key Points:
- **CancellationToken**: Used to signal cancellation of a task.
- **asyncio.create_task**: Runs the team task asynchronously.
- **await asyncio.sleep**: Delays the cancellation to allow the task to start.
- **cancellation_token.cancel()**: Cancels the task, raising a cancellation error.

This setup allows you to manage and cancel tasks effectively in an asynchronous chat environment.

# Magentic-One

Magentic-One is a generalist multi-agent system designed to solve open-ended web and file-based tasks across various domains. It is implemented using the `autogen-agentchat` library, providing a modular and user-friendly interface. The system includes several agents such as the Orchestrator, WebSurfer, FileSurfer, Coder, and ComputerTerminal, each with specific roles to handle complex tasks.

## Getting Started

To begin using Magentic-One, install the required packages:

```bash
pip install "autogen-agentchat" "autogen-ext[magentic-one,openai]"

# For MultimodalWebSurfer, install playwright dependencies:
playwright install --with-deps chromium
```

### Example Usage

Here's a basic example of setting up a Magentic-One group chat with an assistant agent:

```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    assistant = AssistantAgent("Assistant", model_client=model_client)
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof for Fermat's Last Theorem"))

asyncio.run(main())
```

### Using Different Agents

You can also use other agents like the WebSurfer in a team:

```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.agents.web_surfer import MultimodalWebSurfer

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    surfer = MultimodalWebSurfer("WebSurfer", model_client=model_client)
    team = MagenticOneGroupChat([surfer], model_client=model_client)
    await Console(team.run_stream(task="What is the UV index in Melbourne today?"))

asyncio.run(main())
```

### Using the MagenticOne Helper Class

For a bundled setup:

```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console

async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)

if __name__ == "__main__":
    asyncio.run(example_usage())
```

## Architecture

Magentic-One uses a multi-agent architecture with an Orchestrator agent for high-level planning and task management. The system includes:

- **Orchestrator**: Manages task decomposition and planning.
- **WebSurfer**: Commands a web browser for web-based tasks.
- **FileSurfer**: Handles file reading and navigation.
- **Coder**: Specializes in writing and analyzing code.
- **ComputerTerminal**: Executes code and manages programming libraries.

## Caution

Using Magentic-One involves risks such as interacting with web pages and executing code. To mitigate risks:

1. Use Docker containers for isolation.
2. Run agents in a virtual environment.
3. Monitor logs for risky behavior.
4. Ensure human oversight.
5. Limit internet and resource access.
6. Safeguard sensitive data.

## Citation

If you use Magentic-One in your research, please cite the following paper:

```plaintext
@misc{fourney2024magenticonegeneralistmultiagentsolving,
      title={Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},
      author={Adam Fourney and others},
      year={2024},
      eprint={2411.04468},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.04468},
}
```

For more details, refer to the [technical report](https://arxiv.org/abs/2411.04468) and [blog post](https://aka.ms/magentic-one-blog).

## AutoGen Studio Overview

AutoGen Studio is a low-code interface designed to help rapidly prototype AI agents, enhance them with tools, compose them into teams, and interact with them to accomplish tasks. It is built on the AutoGen AgentChat API, a high-level framework for building multi-agent applications.

### Key Features

1. **Team Builder**: Create agent teams using a visual interface or JSON, configure core components, and ensure compatibility with AgentChatâ€™s definitions.
2. **Playground**: Test and run agent teams interactively, with features like live message streaming and control transition graphs.
3. **Gallery**: Discover and import community-created components for easy integration.
4. **Deployment**: Export and run teams in Python, set up endpoints, and run in a Docker container.

### Installation

AutoGen Studio can be installed from PyPi or from source. Using a virtual environment is recommended to isolate dependencies.

#### From PyPi
```bash
pip install -U autogenstudio
```

#### From Source
- Ensure Python 3.10+ and Node.js are installed.
- Clone the repository and install Python dependencies:
  ```bash
  pip install -e .
  ```
- Build the UI:
  ```bash
  cd frontend
  yarn install
  yarn build
  ```

### Running the Application

Start the web UI with:
```bash
autogenstudio ui --port 8081
```
Access it at `http://localhost:8081/`.

### Security Note

AutoGen Studio is a research prototype and not intended for production use. Developers should implement necessary security features if building production applications.

### Contribution Guide

Contributions are welcome. Review the [contribution guide](https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md) and use the `proj-studio` tag for related issues and PRs.

### Citation

If using AutoGen Studio in research, cite the following paper:
```bibtex
@inproceedings{autogenstudio,
  title={AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},
  author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={72--79},
  year={2024}
}
```

### Migration Guide for v0.2 to v0.4

AutoGen v0.4 introduces a new asynchronous, event-driven architecture. Key changes include:
- **Model Client**: Use the `ChatCompletionClient` for OpenAI models.
- **AgentChat API**: Offers a task-driven framework for interactive applications.

For detailed migration steps, refer to the migration guide in the documentation.

name. If `None` is returned, the LLM-based selection method will be used.

```python
def selector_func(messages: Sequence[ChatMessage | AgentEvent]) -> str | None:
    # Implement your custom logic to determine the next speaker
    # For example, based on the last message content or a specific pattern
    # Return the name of the next agent to speak, or None to use default selection
    pass

# Create the group chat with the agents and the custom selector function
group_chat = SelectorGroupChat(
    agents=[planning_agent, web_search_agent, data_analyst_agent],
    termination_condition=termination,
    selector_func=selector_func
)

return group_chat
```

### Running the Group Chat

```python
async def main() -> None:
    # Create the team
    group_chat = create_team()

    # `run_stream` returns an async generator to stream the intermediate messages
    stream = group_chat.run_stream(
        task="Analyze the performance of Miami Heat players over the 2006-2009 seasons."
    )

    # `Console` is a simple UI to display the stream
    await Console(stream)

asyncio.run(main())
```

### Summary

In v0.4, the `SelectorGroupChat` allows for more flexible and powerful group chat configurations, including custom speaker selection logic. This makes it easier to implement complex workflows and scenarios, such as multi-agent collaboration with tool use and state-based decision-making.

```python
    = 
{
        "role": role,
        "content": None,
        "function_call": None,
        "function_execution": None,
        "images": None,
    }

    if isinstance(message, TextMessage):
        v02_message["content"] = message.content

    elif isinstance(message, MultiModalMessage):
        v02_message["content"] = message.content[0]
        if image_detail == "auto":
            v02_message["images"] = [img.to_dict() for img in message.content[1:]]
        elif image_detail == "high":
            v02_message["images"] = [img.to_dict(detail="high") for img in message.content[1:]]
        elif image_detail == "low":
            v02_message["images"] = [img.to_dict(detail="low") for img in message.content[1:]]

    elif isinstance(message, ToolCallRequestEvent):
        v02_message["function_call"] = {
            "name": message.function_call.name,
            "arguments": message.function_call.arguments,
        }

    elif isinstance(message, ToolCallExecutionEvent):
        v02_message["function_execution"] = {
            "name": message.function_execution.name,
            "result": message.function_execution.result,
        }

    elif isinstance(message, ToolCallSummaryMessage):
        v02_message["content"] = message.content

    elif isinstance(message, HandoffMessage):
        v02_message["content"] = message.content

    elif isinstance(message, StopMessage):
        v02_message["content"] = message.content

    return v02_message


def convert_to_v04_message(
    v02_message: Dict[str, Any]
) -> AgentEvent | ChatMessage:
    """Convert a v0.2 message to a v0.4 AgentChat message.

    Args:
        v02_message (Dict[str, Any]): The v0.2 message to convert.

    Returns:
        AgentEvent | ChatMessage: The converted v0.4 AgentChat message.
    """
    if "function_call" in v02_message:
        return ToolCallRequestEvent(
            function_call=FunctionCall(
                name=v02_message["function_call"]["name"],
                arguments=v02_message["function_call"]["arguments"],
            )
        )

    if "function_execution" in v02_message:
        return ToolCallExecutionEvent(
            function_execution=FunctionExecutionResult(
                name=v02_message["function_execution"]["name"],
                result=v02_message["function_execution"]["result"],
            )
        )

    if "images" in v02_message:
        return MultiModalMessage(
            content=[v02_message["content"]] + [
                Image.from_dict(img) for img in v02_message["images"]
            ],
            source="user" if v02_message["role"] == "user" else "assistant",
        )

    return TextMessage(
        content=v02_message["content"],
        source="user" if v02_message["role"] == "user" else "assistant",
    )
```

### Group Chat
In `v0.2`, group chat is created using `GroupChat` class. In `v0.4`, you can use `RoundRobinGroupChat` or `SelectorGroupChat` for more control over message flow.

### Group Chat with Resume
In `v0.4`, you can save and load the state of a group chat using `save_state` and `load_state` methods, similar to individual agents.

### Group Chat with Tool Use
In `v0.4`, you can add tools to agents within a group chat, allowing them to call and execute tools as needed.

### Group Chat with Custom Selector (Stateflow)
In `v0.4`, you can create a custom selector function to control which agent should respond next in a group chat.

### Nested Chat
Nested chat allows for hierarchical agent structures. In `v0.4`, you can create custom agents that trigger nested teams or agents using the `on_messages` method.

### Sequential Chat
`v0.4` does not provide a built-in function for sequential chat. Instead, you can create an event-driven sequential workflow using the Core API.

### GPTAssistantAgent
In `v0.4`, the equivalent is `OpenAIAssistantAgent`, supporting customizable threads and file uploads.

### Long Context Handling
In `v0.4`, use `BufferedChatCompletionContext` to manage message history and limit the context window.

### Observability and Control
In `v0.4`, use `on_messages_stream` and `run_stream` methods to observe agents and teams in real-time.

### Code Executors
`v0.4` supports async API for code executors and introduces `AzureContainerCodeExecutor` for ACA dynamic sessions.

The provided code snippets and instructions detail how to handle different message types and convert between versions in a chat application, as well as how to set up and manage group chats using the AutoGen library. Here's a summary of the key points:

### Message Handling and Conversion
- **Message Types**: The code handles various message types such as `TextMessage`, `MultiModalMessage`, `ToolCallRequestEvent`, and `ToolCallExecutionEvent`.
- **Conversion Functions**: Functions like `convert_to_v04_message` are used to convert messages from version 0.2 to 0.4, handling different message structures and content types.

### Group Chat Management
- **v0.2 Group Chat**: Involves creating a `GroupChat` and managing it with a `GroupChatManager`. Participants like `AssistantAgent` are used to simulate roles such as a writer and a critic.
- **v0.4 Group Chat**: Uses `RoundRobinGroupChat` for similar functionality, with improvements in handling and simplicity. The `run_stream` method is used for asynchronous message streaming.

### Group Chat with Resume
- **State Management**: In v0.4, you can save and load the state of a group chat using `save_state` and `load_state` methods, allowing for easy resumption of chats.

### Tool Use in Group Chats
- **v0.4 Improvements**: Tools are directly executed within the `AssistantAgent`, eliminating the need for a user proxy to manage tool calls.

### Custom Selector and Nested Chat
- **Custom Selector**: `SelectorGroupChat` allows for custom speaker selection logic, useful for scenarios like web search or data analysis.
- **Nested Chat**: Implemented by creating custom agents that can manage nested teams or agents, allowing for hierarchical chat structures.

### Long Context Handling
- **BufferedChatCompletionContext**: Used to manage message history and limit the context window for models, ensuring efficient handling of long conversations.

### Observability and Control
- **Streaming and Cancellation**: The `on_messages_stream` method provides real-time streaming of agent actions, and `CancellationToken` allows for asynchronous cancellation of operations.

### Migration from v0.2 to v0.4
- **Model Client Configuration**: Transition from using `OpenAIWrapper` to `OpenAIChatCompletionClient` with direct configuration.
- **Cache Management**: Use of `ChatCompletionCache` for caching model responses, with support for disk and Redis storage.

These updates and features in v0.4 aim to enhance flexibility, control, and scalability in building interactive agent-based applications. For more detailed examples and usage, refer to the specific sections in the documentation.

```python
([
    TextMessage(content="1", source="user")
], CancellationToken())

print(response.chat_message.content)  # Should print "6" as it counts 5 times starting from 1.

asyncio.run(main())
```

### Summary

This document provides a comprehensive guide on using the `autogen` library for creating chat agents and managing conversations. It covers various functionalities such as creating agents, handling tool use, managing group chats, and implementing custom behaviors. Key features include:

- **Agent Creation**: Use `AssistantAgent` and `UserProxyAgent` to create chat agents with specific roles and behaviors.
- **Tool Use**: Integrate tools directly into agents for executing specific tasks.
- **Group Chats**: Manage conversations with multiple agents using `RoundRobinGroupChat` and `SelectorGroupChat`.
- **State Management**: Save and load agent states to resume conversations.
- **Nested Chats**: Implement hierarchical agent structures for complex interactions.

The document also highlights differences between versions 0.2 and 0.4, providing code examples for transitioning between these versions.

```
"You are a critic."
,
    
llm_config=llm_config,
    
is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("APPROVE"),
)

group_chat = GroupChat(
    participants=[writer, critic],
    max_consecutive_auto_reply=10,
)

group_chat_manager = GroupChatManager(
    group_chat=group_chat,
    user_proxy=None,
)

chat_result = group_chat_manager.initiate_chat(
    message="Write a story about a brave knight.",
    summary_method="reflection_with_llm",
)

print(chat_result.summary)
```

In v0.4, you can use the `RoundRobinGroupChat` to achieve the same behavior. Here's how you can set it up:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    writer = AssistantAgent(
        name="writer",
        system_message="You are a writer.",
        model_client=model_client,
    )

    critic = AssistantAgent(
        name="critic",
        system_message="You are a critic.",
        model_client=model_client,
    )

    termination = TextMentionTermination("APPROVE") | MaxMessageTermination(10)

    group_chat = RoundRobinGroupChat(
        [writer, critic],
        termination_condition=termination
    )

    stream = group_chat.run_stream(task="Write a story about a brave knight.")
    await Console(stream)

asyncio.run(main())
```

### Group Chat with Resume

In v0.4, you can save and load the state of a group chat using the `save_state` and `load_state` methods, similar to individual agents. This allows you to pause and resume group chats.

### Save and Load Group Chat State

To save and load the state of a group chat, you can use the `save_state` and `load_state` methods on the `RoundRobinGroupChat` or any other group chat class.

```python
import asyncio
import json
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    writer = AssistantAgent(
        name="writer",
        system_message="You are a writer.",
        model_client=model_client,
    )

    critic = AssistantAgent(
        name="critic",
        system_message="You are a critic.",
        model_client=model_client,
    )

    group_chat = RoundRobinGroupChat([writer, critic])

    # Save the state
    state = await group_chat.save_state()

    # (Optional) Write state to disk
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)

    # Load the state
    await group_chat.load_state(state)

asyncio.run(main())
```

### Group Chat with Tool Use

In v0.4, you can add tools to agents in a group chat just like you would with a single agent. This allows agents to call and execute tools during the group chat.

### Group Chat with Custom Selector (Stateflow)

In v0.4, you can create custom selectors for group chats using the `Stateflow` feature, allowing you to define custom logic for selecting the next agent to act.

### Nested Chat

In v0.4, you can create nested chats by having agents initiate new chats within an existing chat. This is useful for complex workflows where an agent needs to delegate tasks to other agents.

### Sequential Chat

In v0.4, you can create sequential workflows using the Core API, allowing you to define complex sequences of agent interactions.

### GPTAssistantAgent

In v0.4, the `OpenAIAssistantAgent` class replaces the `GPTAssistantAgent` class from v0.2, providing the same features with additional capabilities like customizable threads and file uploads.

### Long Context Handling

In v0.4, the `ChatCompletionContext` base class manages message history, allowing you to handle long contexts by limiting the message history sent to the model.

### Observability and Control

In v

The provided documentation outlines the process of setting up and managing group chats using the AutoGen library, specifically focusing on the transition from version 0.2 to 0.4. Key features include:

1. **Group Chat Setup**: 
   - Create agents like `writer` and `critic` using `AssistantAgent`.
   - Use `RoundRobinGroupChat` for alternating conversations between agents.
   - Implement termination conditions with `TextMentionTermination`.

2. **Async Operations**:
   - Use `asyncio` for asynchronous operations.
   - `run_stream` method for streaming messages.

3. **State Management**:
   - Save and load chat states using `save_state` and `load_state`.

4. **Tool Integration**:
   - Direct tool execution within `AssistantAgent` without user proxy registration.

5. **Custom Selector**:
   - Implement custom speaker selection with `SelectorGroupChat`.

6. **Nested and Sequential Chats**:
   - Support for nested chat structures and sequential workflows.

7. **Long Context Handling**:
   - Manage message history with `BufferedChatCompletionContext`.

8. **Observability and Control**:
   - Stream agent actions and use `CancellationToken` for control.

9. **Migration Guide**:
   - Transition from v0.2 to v0.4, focusing on new APIs and features.

10. **Code Examples**:
    - Detailed code snippets for setting up agents, managing chats, and integrating tools.

The documentation emphasizes the flexibility and scalability of the new version, encouraging users to adopt the event-driven architecture for better control and observability.

To create a custom agent in version 0.4, you can implement the `on_messages`, `on_reset`, and `produced_message_types` methods. Here's an example of a custom agent:

```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State

In version 0.4, you can save and load an agent's state using `save_state` and `load_state` methods:

```python
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(name="assistant", system_message="You are a helpful assistant.", model_client=model_client)
    cancellation_token = CancellationToken()

    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

    # Save the state
    state = await assistant.save_state()
    with open("assistant_state.json", "w") as f:
        json.dump(state, f)

    # Load the state
    with open("assistant_state.json", "r") as f:
        state = json.load(f)
    await assistant.load_state(state)

    # Continue the chat
    response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)
    print(response)

asyncio.run(main())
```

### Two-Agent Chat

In version 0.4, you can use `RoundRobinGroupChat` to create a two-agent chat:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(name="assistant", system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.", model_client=model_client)
    code_executor = CodeExecutorAgent(name="code_executor", code_executor=LocalCommandLineCodeExecutor(work_dir="coding"))

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)
    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)

    stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
    await Console(stream)

asyncio.run(main())
```

### Tool Use

In version 0.4, you can handle tool use with a single `AssistantAgent`:

```python
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage

def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(name="assistant", system_message="You are a helpful assistant. You can call tools to help user.", model_client=model_client, tools=[get_weather], reflect_on_tool_use=True)

    while True:
        user_input = input("User: ")
        if user_input == "exit":
            break
        response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
        print("Assistant:", response.chat_message.content)

asyncio.run(main())
```

### Group Chat

In version 0.4

To create a chatbot using the AutoGen library, you can follow the example below. This example demonstrates setting up an `AssistantAgent` with a limited message history and handling user input in a loop until the user types "exit".

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient
import asyncio

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
        model_context=BufferedChatCompletionContext(buffer_size=10),
    )

    while True:
        user_input = input("User: ")
        if user_input == "exit":
            break

        response = await assistant.on_messages(
            [TextMessage(content=user_input, source="user")],
            CancellationToken()
        )
        print("Assistant:", response.chat_message.content)

asyncio.run(main())
```

### Key Features

- **BufferedChatCompletionContext**: Limits the model's view to the last 10 messages.
- **CancellationToken**: Allows for asynchronous cancellation of requests.
- **AssistantAgent**: Handles incoming messages and generates responses.

### Observability and Control

- Use `on_messages_stream` to stream the agent's internal thoughts and actions.
- `run_stream` can be used for team conversations, allowing real-time observation.

### Migration Guide Highlights

- **Model Client**: In v0.4, configure model clients directly or via component config.
- **Assistant Agent**: Specify `model_client` instead of `llm_config`.
- **Tool Use**: In v0.4, a single `AssistantAgent` can handle tool calling and execution.
- **Group Chat**: Use `RoundRobinGroupChat` for alternating conversations between agents.

For more advanced features like multi-modal inputs, custom agents, and state management, refer to the detailed migration guide and tutorials provided in the documentation.

```python
ogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.code_executor import LocalCommandLineCodeExecutor

def create_team() -> RoundRobinGroupChat:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    assistant = AssistantAgent(
        name="assistant",
        description="A helpful assistant.",
        system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        description="Executes code.",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(max_messages=10)

    group_chat = RoundRobinGroupChat(
        [assistant, code_executor],
        termination_condition=termination
    )

    return group_chat

async def main() -> None:
    group_chat = create_team()

    stream = group_chat.run_stream(
        task="Write a python script to print 'Hello, world!'"
    )

    await Console(stream)

asyncio.run(main())
```

### Key Changes in v0.4

- **Resuming Group Chat**: In v0.4, you can resume a group chat by calling `run` or `run_stream` again with the same group chat object. Use `save_state` and `load_state` to export and load the chat state.
- **Tool Use**: Tools are directly executed within the `AssistantAgent`, eliminating the need for a user proxy.
- **Custom Selector**: Use `SelectorGroupChat` with `selector_func` for custom speaker selection.
- **Nested Chat**: Implement nested chat by creating a custom agent that triggers nested teams or agents.
- **Sequential Chat**: No built-in function for sequential chat; use the Core API for event-driven workflows.
- **Observability and Control**: Use `on_messages_stream` and `run_stream` for real-time observation and control with `CancellationToken`.
- **Code Executors**: v0.4 supports async API and cancellation with `CancellationToken`.

For more detailed examples and migration steps, refer to the specific sections in the migration guide.

The provided documentation outlines the migration from version 0.2 to 0.4 of the `autogen-agentchat` library, focusing on the new asynchronous, event-driven architecture. Below are key highlights and code snippets for various functionalities:

### Main Functionality
```python
async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
        model_client=model_client,
    )
    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )
    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)
    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)
    stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
    await Console(stream)

asyncio.run(main())
```

### Tool Use
In v0.4, a single `AssistantAgent` can handle both tool calling and execution:
```python
async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. You can call tools to help user.",
        model_client=model_client,
        tools=[get_weather],
        reflect_on_tool_use=True,
    )
    while True:
        user_input = input("User: ")
        if user_input == "exit":
            break
        response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
        print("Assistant:", response.chat_message.content)

asyncio.run(main())
```

### Group Chat
For group chat with a writer and critic:
```python
async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    writer = AssistantAgent(
        name="writer",
        description="A writer.",
        system_message="You are a writer.",
        model_client=model_client,
    )
    critic = AssistantAgent(
        name="critic",
        description="A critic.",
        system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
        model_client=model_client,
    )
    termination = TextMentionTermination("APPROVE")
    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)
    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)

asyncio.run(main())
```

### Conversion between v0.2 and v0.4 Messages
Functions to convert messages between versions:
```python
def convert_to_v02_message(message: AgentEvent | ChatMessage, role: Literal["assistant", "user", "tool"], image_detail: Literal["auto", "high", "low"] = "auto") -> Dict[str, Any]:
    # Conversion logic here

def convert_to_v04_message(message: Dict[str, Any]) -> AgentEvent | ChatMessage:
    # Conversion logic here
```

### Group Chat with Resume
```python
async def main() -> None:
    group_chat = create_team()
    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)
    state = await group_chat.save_state()
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)
    group_chat = create_team()
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)
    await group_chat.load_state(state)
    stream = group_chat.run_stream(task="Translate the story into Chinese.")
    await Console(stream)

asyncio.run(main())
```

### Observability and Control
In v0.4, you can observe agents using `on_messages_stream` and `run_stream` methods, which support cancellation via `CancellationToken`.

### Code Executors
The v0.4 executors support async API and can be canceled using `CancellationToken`.

This guide provides a comprehensive overview of migrating to v0.4, emphasizing the new architecture's flexibility and scalability. For more detailed examples and explanations, refer to the full documentation.

In AutoGen v0.4, creating and managing model clients, agents, and group chats has been streamlined and enhanced with new features. Here's a concise overview of the key functionalities and examples:

### Model Client Creation

**Using Component Config:**
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o",
        "api_key": "sk-xxx"
    }
}

model_client = ChatCompletionClient.load_component(config)
```

**Using Model Client Class Directly:**
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx")
```

### Caching with Model Clients

**Using Disk Cache:**
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache

async def main():
    openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")
    cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache("/tmp/cache"))
    cache_client = ChatCompletionCache(openai_model_client, cache_store)

    response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
    print(response)

asyncio.run(main())
```

### Assistant Agent

**Creating an Assistant Agent:**
```python
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx", seed=42, temperature=0)

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client
)
```

**Handling Messages:**
```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken

async def main():
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], CancellationToken())
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent

**Handling Multi-Modal Inputs:**
```python
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image

message = MultiModalMessage(
    content=["Here is an image:", Image.from_file(Path("test.png"))],
    source="user"
)

response = await assistant.on_messages([message], cancellation_token)
print(response)
```

### Group Chat

**Round Robin Group Chat:**
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console

async def main():
    writer = AssistantAgent(name="writer", system_message="You are a writer.", model_client=model_client)
    critic = AssistantAgent(name="critic", system_message="You are a critic.", model_client=model_client)

    termination = TextMentionTermination("APPROVE")
    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)

    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)

asyncio.run(main())
```

### Tool Use

**Integrating Tools with AssistantAgent:**
```python
def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant. You can call tools to help user.",
    model_client=model_client,
    tools=[get_weather],
    reflect_on_tool_use=True
)
```

### Saving and Loading State

**Saving and Loading Agent State:**
```python
state = await assistant.save_state()
with open("assistant_state.json", "w") as f:
    json.dump(state, f)

with open("assistant_state.json", "r") as f:
    state = json.load(f)

await assistant.load_state(state)
```

These examples illustrate the streamlined process of setting up and managing agents and group chats in AutoGen v0.4, with enhanced support for caching, multi-modal inputs, and tool integration.

### Function Definitions

- **`percentage_change_tool(start: float, end: float) -> float`**: Calculates the percentage change from `start` to `end`.
  ```python
  return ((end - start) / start) * 100
  ```

- **`create_team() -> SelectorGroupChat`**: Sets up a team of agents for task delegation and execution.
  ```python
  model_client = OpenAIChatCompletionClient(model="gpt-4o")
  planning_agent = AssistantAgent("PlanningAgent", description="An agent for planning tasks...", model_client=model_client, system_message="...")
  web_search_agent = AssistantAgent("WebSearchAgent", description="A web search agent.", tools=[search_web_tool], model_client=model_client, system_message="...")
  data_analyst_agent = AssistantAgent("DataAnalystAgent", description="A data analyst agent...", model_client=model_client, tools=[percentage_change_tool], system_message="...")
  termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(max_messages=25)
  selector_func = lambda messages: planning_agent.name if messages[-1].source != planning_agent.name else None
  team = SelectorGroupChat([planning_agent, web_search_agent, data_analyst_agent], model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"), termination_condition=termination, selector_func=selector_func)
  return team
  ```

- **`main() -> None`**: Asynchronously runs the team to handle a specific task.
  ```python
  async def main() -> None:
      team = create_team()
      task = "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?"
      await Console(team.run_stream(task=task))
  asyncio.run(main())
  ```

### Nested Chat

Nested chat allows for hierarchical agent structures. In v0.4, nested chat is implemented by creating custom agents that can trigger nested teams or agents. Example:
```python
class CountingAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        last_number = int(messages[-1].content) if messages else 0
        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))

class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])

async def main() -> None:
    counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
    counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
    nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
    response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
    for message in response.inner_messages:
        print(message)
    print(response.chat_message)

asyncio.run(main())
```

### Sequential Chat

Sequential chat in v0.4 is implemented using the Core API to create event-driven workflows. The `initiate_chats` function from v0.2 is replaced by custom Python code to glue steps together.

### Observability and Control

In v0.4, you can observe agents using `on_messages_stream` and `run_stream` methods, which allow real-time observation of agent interactions. These methods accept a `CancellationToken` to asynchronously cancel the output stream.

### Code Executors

Code executors in v0.4 support async API and can be canceled using `CancellationToken`. The `AzureContainerCodeExecutor` is added for Azure Container Apps dynamic sessions.

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main():
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        api_key="sk-xxx"
    )

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client
    )

    while True:
        user_input = input("User: ")
        if user_input == "exit":
            break

        response = await assistant.on_messages(
            [TextMessage(content=user_input, source="user")],
            CancellationToken()
        )

        print("Assistant:", response.chat_message.content)

asyncio.run(main())
```

### Key Changes from v0.2 to v0.4

- **Model Client**: In v0.4, you use `model_client` instead of `llm_config`.
- **Async API**: The v0.4 API is asynchronous, requiring the use of `async` and `await`.
- **CancellationToken**: This is used to manage and potentially cancel ongoing operations.

### Group Chat

In v0.4, group chat management is simplified with classes like `RoundRobinGroupChat` and `SelectorGroupChat`. Here's a basic example using `RoundRobinGroupChat`:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main():
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        api_key="sk-xxx"
    )

    writer = AssistantAgent(
        name="writer",
        description="A writer.",
        system_message="You are a writer.",
        model_client=model_client
    )

    critic = AssistantAgent(
        name="critic",
        description="A critic.",
        system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
        model_client=model_client
    )

    termination = TextMentionTermination("APPROVE")

    group_chat = RoundRobinGroupChat(
        [writer, critic],
        termination_condition=termination,
        max_turns=12
    )

    stream = group_chat.run_stream(
        task="Write a short story about a robot that discovers it has feelings."
    )

    await Console(stream)

asyncio.run(main())
```

### Key Features in v0.4

- **Async and Event-Driven**: The architecture is designed for asynchronous operations, improving scalability and responsiveness.
- **Observability**: Use `on_messages_stream` and `run_stream` to observe agent interactions in real-time.
- **Tool Use**: Tools are directly integrated into agents, simplifying their use in group chats.
- **State Management**: Easily save and load chat states with `save_state` and `load_state`.

For more detailed examples and advanced features, refer to the official documentation and tutorials.

The provided documentation outlines the usage and features of the `AssistantAgent` and related classes in version 0.4 of a library, focusing on handling messages, multi-modal inputs, user proxies, conversable agents, and group chats. Below are key highlights and code snippets for various functionalities:

### AssistantAgent Usage
In version 0.4, the `AssistantAgent` handles messages asynchronously using `on_messages` or `on_messages_stream`. Here's a basic setup:

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent
The `AssistantAgent` supports multi-modal inputs if the model client supports it:

```python
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image

message = MultiModalMessage(
    content=["Here is an image:", Image.from_file(Path("test.png"))],
    source="user",
)
response = await assistant.on_messages([message], cancellation_token)
```

### User Proxy
In v0.4, a user proxy is simply an agent that takes user input:

```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent("user_proxy")
```

### Conversable Agent
To create a custom agent, implement the `on_messages`, `on_reset`, and `produced_message_types` methods:

```python
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))
```

### Save and Load Agent State
Agents can save and load their state using `save_state` and `load_state` methods:

```python
state = await assistant.save_state()
with open("assistant_state.json", "w") as f:
    json.dump(state, f)

with open("assistant_state.json", "r") as f:
    state = json.load(f)
await assistant.load_state(state)
```

### Two-Agent Chat
Use `RoundRobinGroupChat` for a two-agent chat:

```python
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)
stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
await Console(stream)
```

### Tool Use
In v0.4, `AssistantAgent` can handle tool calling and execution:

```python
def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant. You can call tools to help user.",
    model_client=model_client,
    tools=[get_weather],
    reflect_on_tool_use=True,
)
```

### Group Chat
Use `RoundRobinGroupChat` for group chat scenarios:

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)
stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
await Console(stream)
```

### Nested Chat
Create a custom agent to implement nested chat functionality:

```python
class CountingAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        last_number = int(messages[-1].content) if messages else 0
        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))
```

These examples demonstrate the flexibility and capabilities of the `AssistantAgent` and related classes in handling various chat scenarios, including multi-modal inputs, tool usage, and group chats.

The provided code and documentation describe the implementation of a `NestedCountingAgent` and its usage within a chat system. Here's a summary of the key components and their functionalities:

### `NestedCountingAgent` Class
- **Purpose**: An agent that increments the last number in input messages using a nested counting team.
- **Initialization**: Takes a name and a `RoundRobinGroupChat` as a counting team.
- **Methods**:
  - `on_messages`: Asynchronously processes incoming messages, runs the counting team, and returns the last message produced.
  - `on_reset`: Resets the counting team.
  - `produced_message_types`: Returns the types of messages the agent can produce, specifically `TextMessage`.

### Usage Example
- **Setup**: 
  - Create two `CountingAgent` instances.
  - Initialize a `RoundRobinGroupChat` with these agents.
  - Create a `NestedCountingAgent` with the counting team.
- **Execution**: 
  - Run the `NestedCountingAgent` with a starting message.
  - Print the inner messages and the final chat message.

```python
async def main() -> None:
    counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
    counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
    nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
    
    response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
    assert response.inner_messages is not None
    
    for message in response.inner_messages:
        print(message)
    
    print(response.chat_message)

asyncio.run(main())
```

### Expected Output
The output demonstrates the counting process, showing messages from alternating agents incrementing the count:

```
source='counting_agent_1', content='2'
source='counting_agent_2', content='3'
source='counting_agent_1', content='4'
source='counting_agent_2', content='5'
source='counting_agent_1', content='6'
```

### Additional Information
- **Sequential Chat**: The document mentions changes in sequential chat handling from version 0.2 to 0.4, emphasizing the use of Python code for flexibility.
- **GPTAssistantAgent**: Transitioned to `OpenAIAssistantAgent` in v0.4 with additional features.
- **Long Context Handling**: Introduced `ChatCompletionContext` for managing message history.
- **Observability and Control**: New methods for streaming and observing agent actions.
- **Code Executors**: Support for async API and cancellation tokens, with new executors like `AzureContainerCodeExecutor`.

This setup and functionality are part of a broader framework for building interactive agent-based applications, with enhancements in version 0.4 focusing on flexibility, control, and observability.

implement the `on_messages` method to define the agent's behavior. Here's how you can create a custom agent in `v0.4`:

```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    """A custom agent that replies with a predefined message."""

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        # Custom reply logic here
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)

# Usage
import asyncio

async def main() -> None:
    custom_agent = CustomAgent(name="custom_agent", description="An agent that provides custom replies.")
    response = await custom_agent.on_messages([TextMessage(content="Hello!", source="user")], CancellationToken())
    print(response.chat_message.content)

asyncio.run(main())
```

### Key Differences:
- **Custom Logic**: In `v0.4`, you define the agent's behavior directly in the `on_messages` method, which is more intuitive and flexible than registering a reply function.
- **Asynchronous**: The `on_messages` method is asynchronous, allowing for non-blocking operations and better performance in handling messages.
- **Direct Implementation**: You implement the logic directly in the agent class, making it easier to understand and maintain.

This approach provides a clearer and more structured way to define custom agent behaviors compared to the `v0.2` method of registering reply functions.

To implement the `on_messages`, `on_reset`, and `produced_message_types` methods for a custom chat agent, you can follow the example provided below. This example demonstrates how to create a `CustomAgent` class that inherits from `BaseChatAgent` and implements the required methods.

```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Usage Example

You can use the `CustomAgent` in a similar way to the `AssistantAgent`. Here's a basic example of how you might use this custom agent in an asynchronous context:

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken

async def main() -> None:
    custom_agent = CustomAgent(name="custom_agent")
    cancellation_token = CancellationToken()

    response = await custom_agent.on_messages(
        [TextMessage(content="Hello!", source="user")],
        cancellation_token
    )

    print(response.chat_message.content)

asyncio.run(main())
```

### Key Points

- **on_messages**: This method processes incoming messages and returns a `Response` object. In this example, it simply returns a static "Custom reply".
- **on_reset**: This method is called to reset the agent's state. In this example, it does nothing.
- **produced_message_types**: This property returns the types of messages the agent can produce, which is `TextMessage` in this case.

This setup allows you to create a custom chat agent that can be integrated into a larger chat system, responding to messages and managing its state as needed.

### Overview of `ogen_ext.models.openai` and `OpenAIChatCompletionClient`

The `OpenAIChatCompletionClient` is a client for interacting with OpenAI's chat models, such as "gpt-4o". It allows you to configure parameters like `seed` and `temperature` to control the randomness and determinism of the model's responses.

### Example Usage

Here's a basic example of setting up a chat assistant using `OpenAIChatCompletionClient`:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_agentchat.messages import TextMessage
import asyncio

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client
    )

    while True:
        user_input = input("User: ")
        if user_input == "exit":
            break

        response = await assistant.on_messages(
            [TextMessage(content=user_input, source="user")],
            CancellationToken()
        )
        print("Assistant:", response.chat_message.content)

asyncio.run(main())
```

### Key Features

- **Observability and Control**: Use `on_messages_stream` to stream the agent's internal processes. This is useful for debugging and understanding the agent's decision-making.
- **Code Executors**: Supports async API and can be canceled using `CancellationToken`. You can use `AzureContainerCodeExecutor` for dynamic sessions.
- **Model Client Configuration**: In v0.4, you can configure model clients using a component configuration system or directly using the model client class.
- **Caching**: Use `ChatCompletionCache` with `DiskCacheStore` or `RedisStore` for caching responses.

### Migration from v0.2 to v0.4

- **Model Client**: In v0.4, you directly specify the model client instead of using `llm_config`.
- **Assistant Agent**: The `AssistantAgent` now uses `on_messages` or `on_messages_stream` for handling messages.
- **State Management**: Save and load agent states using `save_state` and `load_state` methods.
- **Group Chat**: Use `RoundRobinGroupChat` for managing group interactions.

### Additional Features

- **Multi-Modal Agent**: Supports multi-modal inputs if the model client supports it.
- **User Proxy**: Simplified in v0.4, no special configuration needed.
- **Conversable Agent**: Implement custom agents by defining `on_messages`, `on_reset`, and `produced_message_types`.
- **Tool Use**: In v0.4, a single `AssistantAgent` can handle both tool calling and execution.

For more detailed information, refer to the respective tutorials and documentation sections on the Core API and AgentChat API.

```python
async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(max_messages=10)

    group_chat = RoundRobinGroupChat(
        [assistant, code_executor],
        termination_condition=termination
    )

    stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
    await Console(stream)

asyncio.run(main())
```

### Group Chat with Resume

In `v0.4`, you can resume a group chat by calling `run` or `run_stream` again with the same group chat object. To export and load the state, use `save_state` and `load_state` methods.

### Save and Load Group Chat State

```python
async def main() -> None:
    group_chat = create_team()

    # Save the state of the group chat and all participants.
    state = await group_chat.save_state()
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    # Create a new team with the same participants configuration.
    group_chat = create_team()

    # Load the state of the group chat and all participants.
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)
    await group_chat.load_state(state)

    # Resume the chat.
    stream = group_chat.run_stream(task="Translate the story into Chinese.")
    await Console(stream)

asyncio.run(main())
```

### Group Chat with Tool Use

In `v0.4`, tools are directly executed within the `AssistantAgent`, eliminating the need for a user proxy to route tool calls.

### Group Chat with Custom Selector (Stateflow)

Use `SelectorGroupChat` with `selector_func` to implement custom speaker selection methods.

```python
def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
    if messages[-1].source != planning_agent.name:
        return planning_agent.name
    return None

team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
    termination_condition=termination,
    selector_func=selector_func,
)
```

### Nested Chat

In `v0.4`, nested chat is implemented by creating a custom agent that takes a team or another agent as a parameter and implements the `on_messages` method.

```python
class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        assert isinstance(result.messages[-1], TextMessage)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])
```

### Sequential Chat

In `v0.4`, create an event-driven sequential workflow using the Core API, as the `initiate_chats` function is no longer provided.

### Long Context Handling

Use `BufferedChatCompletionContext` to manage long contexts by limiting the message history sent to the model.

```python
assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
    model_context=BufferedChatCompletionContext(buffer_size=10),
)
```

### Observability and Control

Use `on_messages_stream` and `run_stream` methods to observe agents and teams in real-time. Use `CancellationToken` to cancel the output stream asynchronously.

### Code Executors

In `v0.4`, code executors support async API and `CancellationToken` for canceling long executions. Use `AzureContainerCodeExecutor` for Azure Container Apps dynamic sessions.

### Migration Guide for v0.2 to v0.4 â€” AutoGen

This guide helps users transition from `v0.2.*` to `v0.4` of `autogen-agentchat`, which introduces new APIs and features. The `v0.4` version includes breaking changes, so careful reading is advised. The `v0.2` version is still maintained in the `0.2` branch, but upgrading to `v0.4` is recommended.

#### Key Changes in v0.4

- **Asynchronous, Event-Driven Architecture**: Enhances observability, flexibility, interactive control, and scalability.
- **Layered API**: 
  - **Core API**: Provides a scalable, event-driven actor framework.
  - **AgentChat API**: Built on Core, offering a high-level framework for interactive agentic applications.

#### Migration Highlights

- **Model Client**: 
  - Use component config or model client class directly.
  - Example for OpenAI-Compatible APIs:
    ```python
    from autogen_ext.models.openai import OpenAIChatCompletionClient

    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )
    ```

- **Assistant Agent**: 
  - Handles both tool calling and execution.
  - Example:
    ```python
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
        tools=[get_weather],
        reflect_on_tool_use=True
    )
    ```

- **Group Chat**: 
  - Use `RoundRobinGroupChat` for alternating between agents.
  - Example:
    ```python
    group_chat = RoundRobinGroupChat(
        [assistant, code_executor],
        termination_condition=termination
    )
    ```

- **Tool Use**: 
  - Directly pass tool functions to `AssistantAgent`.
  - Example:
    ```python
    def get_weather(city: str) -> str:
        return f"The weather in {city} is 72 degree and sunny."
    ```

- **Chat Result**: 
  - `TaskResult` replaces `ChatResult`.
  - Example:
    ```python
    response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
    ```

- **Group Chat with Resume**: 
  - Use `save_state` and `load_state` methods.
  - Example:
    ```python
    state = await group_chat.save_state()
    await group_chat.load_state(state)
    ```

- **Observability and Control**: 
  - Use `on_messages_stream` and `run_stream` for real-time observation.
  - Example:
    ```python
    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)
    ```

- **Code Executors**: 
  - Support async API and `CancellationToken`.
  - Example:
    ```python
    from autogen_ext.models.openai import OpenAIChatCompletionClient
    ```

#### Additional Features

- **Nested Chat**: Implement custom agents for hierarchical structures.
- **Long Context Handling**: Use `BufferedChatCompletionContext` for managing message history.
- **Sequential Chat**: Create event-driven workflows using the Core API.

#### Future Updates

- Features like Model Client Cost, Teachable Agent, and RAG Agent will be added in future releases.

For more detailed examples and explanations, refer to the `v0.4` documentation and tutorials.

```python
(old_value: float, new_value: float) -> str:
    if old_value == 0:
        return "Cannot calculate percentage change from zero."
    change = ((new_value - old_value) / old_value) * 100
    return f"The percentage change is {change:.2f}%."

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    search_agent = AssistantAgent(
        name="search_agent",
        description="A web search agent.",
        system_message="You are a web search agent.",
        model_client=model_client,
        tools=[search_web_tool]
    )

    analysis_agent = AssistantAgent(
        name="analysis_agent",
        description="An analysis agent.",
        system_message="You are an analysis agent.",
        model_client=model_client,
        tools=[percentage_change_tool]
    )

    termination = TextMentionTermination("APPROVE") | MaxMessageTermination(10)

    def selector_func(messages: Sequence[ChatMessage]) -> str:
        if not messages:
            return "search_agent"
        last_message = messages[-1]
        if last_message.source == "search_agent":
            return "analysis_agent"
        return "search_agent"

    group_chat = SelectorGroupChat(
        agents=[search_agent, analysis_agent],
        termination_condition=termination,
        selector_func=selector_func
    )

    stream = group_chat.run_stream(
        task="Analyze the performance of Dwayne Wade in the Miami Heat seasons 2006-2009."
    )

    await Console(stream)

asyncio.run(main())
```

In this example, the `SelectorGroupChat` alternates between a search agent and an analysis agent based on a custom selector function. The search agent uses a mock web search tool, while the analysis agent uses a mock percentage change tool. The chat terminates when the text "APPROVE" is mentioned or after 10 messages.

the chat, and the `cost` which is the cost of the chat. Here's an example:

```python
task_result = await group_chat.run(task="Write a python script to print 'Hello, world!'")

print(task_result.messages)  # Get the chat history.
print(task_result.cost)      # Get the cost of the chat.
```

### Group Chat

In `v0.2`, you create a group chat using `SelectorGroupChat` or `RoundRobinGroupChat`. In `v0.4`, the process is similar, but you have more flexibility with termination conditions and selectors.

```python
from autogen_agentchat.teams import SelectorGroupChat, RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination

# Define agents
planning_agent = AssistantAgent(...)
web_search_agent = AssistantAgent(...)
data_analyst_agent = AssistantAgent(...)

# Define termination conditions
termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(25)

# Define selector function
def selector_func(messages):
    if messages[-1].source != planning_agent.name:
        return planning_agent.name
    return None

# Create group chat
team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
    termination_condition=termination,
    selector_func=selector_func,
)
```

### Nested Chat

Nested chat allows you to create a hierarchical structure of agents. In `v0.4`, you can create a custom agent that takes a team or another agent as a parameter and implements the `on_messages` method to trigger the nested team or agent.

```python
class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name, counting_team):
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages, cancellation_token):
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])

    async def on_reset(self, cancellation_token):
        await self._counting_team.reset()

    @property
    def produced_message_types(self):
        return (TextMessage,)
```

### Sequential Chat

In `v0.4`, you can create an event-driven sequential workflow using the Core API. This approach is more flexible than the `initiate_chats` function in `v0.2`.

### Observability and Control

In `v0.4`, you can observe agents using the `on_messages_stream` method, which returns an async generator to stream the inner thoughts and actions of the agent. For teams, use the `run_stream` method to stream the inner conversation among agents.

### Code Executors

The code executors in `v0.4` support async API and can be canceled using a `CancellationToken`. You can also use the `AzureContainerCodeExecutor` for code execution with Azure Container Apps.

### Migration Summary

- Use `AssistantAgent` for both tool calling and execution.
- Use `RoundRobinGroupChat` or `SelectorGroupChat` for group chats.
- Implement custom agents for specific behaviors.
- Use `run` or `run_stream` for executing tasks and getting results.
- Use `save_state` and `load_state` for managing agent states.
- Use `on_messages_stream` and `run_stream` for observability and control.

This guide provides a comprehensive overview of migrating from `v0.2` to `v0.4`, focusing on the new features and changes in the API.

```python
="sk-xxx"
)

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
)
```

### Multi-Modal Agent

In **v0.2**, you create a multi-modal agent using the `MultiModalAgent` class:

```python
from autogen.agentchat import MultiModalAgent

llm_config = {
    "config_list": [{
        "model": "gpt-4o",
        "api_key": "sk-xxx"
    }],
    "seed": 42,
    "temperature": 0,
}

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal agent.",
    llm_config=llm_config,
)
```

In **v0.4**, you use the `MultiModalAgent` class similarly, but with `model_client`:

```python
from autogen_agentchat.agents import MultiModalAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal agent.",
    model_client=model_client,
)
```

### User Proxy

In **v0.2**, a user proxy is used to register tool functions and manage tool calls:

```python
from autogen.agentchat import UserProxy

user_proxy = UserProxy(
    name="user_proxy",
    description="A proxy for user interactions.",
    llm_config=llm_config,
)
```

In **v0.4**, the concept of a user proxy is simplified, and tools are directly associated with agents:

```python
from autogen_agentchat.agents import AssistantAgent

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
    tools=[your_tool_function],
)
```

### Conversable Agent and Register Reply

In **v0.2**, you register replies using the `register_reply` method:

```python
from autogen.agentchat import ConversableAgent

agent = ConversableAgent(
    name="agent",
    llm_config=llm_config,
)

agent.register_reply("Hello", "Hi there!")
```

In **v0.4**, you use the `on_messages` method to handle messages:

```python
from autogen_agentchat.agents import BaseChatAgent

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages, cancellation_token):
        # Handle messages and return a response
        pass
```

### Save and Load Agent State

In **v0.2**, you manually save and load agent states:

```python
agent.save_state("agent_state.json")
agent.load_state("agent_state.json")
```

In **v0.4**, you use the `save_state` and `load_state` methods:

```python
state = await agent.save_state()
await agent.load_state(state)
```

### Two-Agent Chat

In **v0.2**, you set up a two-agent chat using `GroupChat`:

```python
from autogen.agentchat import GroupChat

group_chat = GroupChat(
    agents=[agent1, agent2],
    messages=[],
    max_round=10
)
```

In **v0.4**, you use `RoundRobinGroupChat`:

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat(
    agents=[agent1, agent2],
    max_turns=10
)
```

### Tool Use

In **v0.2**, tools are registered on a user proxy:

```python
user_proxy.register_tool("tool_name", tool_function)
```

In **v0.4**, tools are directly associated with agents:

```python
assistant = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[tool_function],
)
```

### Chat Result

In **v0.2**, chat results are handled with `ChatResult`:

```python
result = chat_manager.initiate_chat()
print(result.summary)
```

In **v0.4**, you use the `run` or `run_stream` methods:

```python
result = await group_chat.run(task="Your task here")
print(result.summary)
```

### Conversion between v0.2 and v0.4 Messages

Conversion functions are provided to convert between message formats:

```python
def convert_to_v02_message(message, role, image_detail="auto"):
    # Conversion logic
    pass

def convert_to_v04_message(message):
    # Conversion logic
    pass
```

### Group Chat with

```python
    def __init__(self, name: str, nested_team: RoundRobinGroupChat):
        super().__init__(name)
        self.nested_team = nested_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        # Pass the messages to the nested team and get the response.
        response = await self.nested_team.on_messages(messages, cancellation_token)
        return response

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        await self.nested_team.on_reset(cancellation_token)

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)

async def main() -> None:
    # Create a nested team with two counting agents.
    nested_team = RoundRobinGroupChat([CountingAgent("counter1"), CountingAgent("counter2")])

    # Create a nested counting agent with the nested team.
    nested_counting_agent = NestedCountingAgent("nested_counter", nested_team)

    # Run the nested counting agent.
    response = await nested_counting_agent.on_messages([TextMessage(content="0", source="user")], CancellationToken())
    print(response.chat_message.content)  # Should print "2" as it goes through two counting agents.

asyncio.run(main())
```

In this example, the `NestedCountingAgent` uses a `RoundRobinGroupChat` with two `CountingAgent` instances. Each `CountingAgent` increments the number it receives, demonstrating how nested agents can be structured to perform tasks in sequence.

The provided code and documentation describe a system for creating and managing chat agents using an asynchronous, event-driven architecture. Here's a summary of the key components and functionalities:

### Key Components

1. **NestedCountingAgent**: 
   - Inherits from a base class and initializes with a name and a `RoundRobinGroupChat` instance.
   - Handles messages and resets the inner team.

2. **RoundRobinGroupChat**:
   - Manages a group of agents, allowing them to take turns in a round-robin fashion.
   - Supports termination conditions and streaming of inner conversations.

3. **AssistantAgent**:
   - A chat agent that can interact with users and call tools.
   - Supports multi-modal inputs if the model client allows it.

4. **CodeExecutorAgent**:
   - Executes code in a specified environment, such as a local command line.

5. **Model Clients**:
   - `OpenAIChatCompletionClient` and `AzureOpenAIChatCompletionClient` are used to interact with OpenAI models.
   - Support for caching and custom configurations.

6. **Tool Use**:
   - Agents can call and execute tools, with the ability to reflect on tool usage.

7. **Observability and Control**:
   - Methods like `on_messages_stream` and `run_stream` allow real-time observation of agent actions.
   - Use of `CancellationToken` for asynchronous cancellation of tasks.

8. **State Management**:
   - Agents can save and load their state, preserving chat history and other relevant data.

### Usage Examples

- **Creating a Nested Counting Agent**:
  ```python
  nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
  response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
  ```

- **Running an Assistant Agent**:
  ```python
  assistant = AssistantAgent(name="assistant", system_message="You are a helpful assistant.", model_client=model_client)
  response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], CancellationToken())
  ```

- **Tool Use in Assistant Agent**:
  ```python
  def get_weather(city: str) -> str:
      return f"The weather in {city} is 72 degree and sunny."

  assistant = AssistantAgent(name="assistant", system_message="You can call tools to help user.", model_client=model_client, tools=[get_weather])
  ```

- **Observing Agent Actions**:
  ```python
  stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
  await Console(stream)
  ```

### Migration Notes

- **From v0.2 to v0.4**:
  - The architecture has shifted to an asynchronous, event-driven model.
  - The `initiate_chat` function is replaced by more flexible methods like `run` and `run_stream`.
  - State management and tool use are more integrated and streamlined.

This system is designed to be flexible and scalable, allowing for complex workflows and interactions between agents and users.

### Migration Guide for v0.2 to v0.4 â€” AutoGen

This guide helps users transition from `v0.2.*` to `v0.4` of `autogen-agentchat`, which introduces new APIs and features. The `v0.4` version includes breaking changes, so careful reading is advised. The `v0.2` version is still maintained in the `0.2` branch, but upgrading to `v0.4` is recommended.

#### Key Changes in v0.4

- **Asynchronous, Event-Driven Architecture**: Improved observability, flexibility, interactive control, and scalability.
- **Layered API**: 
  - **Core API**: Foundation layer for scalable, event-driven workflows.
  - **AgentChat API**: High-level framework for interactive applications, replacing `v0.2`.

#### Model Client

In `v0.2`, model clients were configured using `OpenAIWrapper`. In `v0.4`, you can use a component configuration system or directly instantiate model client classes.

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)
```

#### Assistant Agent

In `v0.4`, specify `model_client` instead of `llm_config`.

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx",
    seed=42,
    temperature=0
)

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client
)
```

#### Multi-Modal Agent

`AssistantAgent` in `v0.4` supports multi-modal inputs if the model client supports it.

```python
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image

message = MultiModalMessage(
    content=["Here is an image:", Image.from_file(Path("test.png"))],
    source="user"
)
```

#### User Proxy

In `v0.4`, a user proxy is simply an agent that takes user input only.

```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent("user_proxy")
```

#### Conversable Agent and Register Reply

In `v0.4`, create a custom agent by implementing `on_messages`, `on_reset`, and `produced_message_types` methods.

```python
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages, cancellation_token):
        # Custom logic here
        pass
```

#### Group Chat

In `v0.4`, use `RoundRobinGroupChat` for group chat scenarios.

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)
```

#### Observability and Control

Use `on_messages_stream` and `run_stream` methods for real-time observation and control.

```python
stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
await Console(stream)
```

#### Code Executors

`v0.4` supports async API and `CancellationToken` for code execution.

```python
from autogen_ext.models.cache import ChatCompletionCache
from autogen_ext.cache_store.diskcache import DiskCacheStore

cache_client = ChatCompletionCache(openai_model_client, cache_store)
```

This guide provides a comprehensive overview of migrating to `v0.4`, focusing on key changes and new features. For more detailed examples and tutorials, refer to the official documentation.

Here's a concise guide on using the `autogen_agentchat` library for creating custom chat agents, saving/loading agent states, and implementing various chat scenarios.

### Custom Agent Implementation

To create a custom chat agent, subclass `BaseChatAgent` and implement the required methods:

```python
from autogen_agentchat.base import BaseChatAgent, Response
from autogen_agentchat.messages import TextMessage, ChatMessage
from typing import Sequence

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State

In version 0.4, you can save and load an agent's state using `save_state` and `load_state` methods:

```python
state = await assistant.save_state()
with open("assistant_state.json", "w") as f:
    json.dump(state, f)

with open("assistant_state.json", "r") as f:
    state = json.load(f)
await assistant.load_state(state)
```

### Two-Agent Chat

For a two-agent chat setup, use `AssistantAgent` and `CodeExecutorAgent` in a `RoundRobinGroupChat`:

```python
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.teams import RoundRobinGroupChat

assistant = AssistantAgent(name="assistant", system_message="You are a helpful assistant.")
code_executor = CodeExecutorAgent(name="code_executor")

group_chat = RoundRobinGroupChat([assistant, code_executor])
```

### Tool Use

In version 0.4, `AssistantAgent` can handle tool calling and execution:

```python
def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    tools=[get_weather],
    reflect_on_tool_use=True
)
```

### Group Chat

For group chats, use `RoundRobinGroupChat`:

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)
```

### Nested Chat

Implement nested chat by creating a custom agent that uses a team or another agent:

```python
class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name: str, counting_team: RoundRobinGroupChat):
        super().__init__(name)
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token) -> Response:
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])
```

### Long Context Handling

Use `BufferedChatCompletionContext` to manage long contexts:

```python
from autogen_core.model_context import BufferedChatCompletionContext

assistant = AssistantAgent(
    name="assistant",
    context=BufferedChatCompletionContext(max_tokens=4096)
)
```

This guide provides a streamlined overview of setting up and managing chat agents using the `autogen_agentchat` library, focusing on key functionalities and code examples.

Here's a concise summary of the migration guide from AutoGen v0.2 to v0.4, focusing on key changes and code examples:

### Key Changes in v0.4

1. **Asynchronous Architecture**: v0.4 adopts an asynchronous, event-driven architecture for better observability and control.

2. **Model Client Configuration**:
   - **v0.2**: Used `OpenAIWrapper` with a list of configurations.
   - **v0.4**: Use `ChatCompletionClient` or `OpenAIChatCompletionClient` directly.
   ```python
   from autogen_ext.models.openai import OpenAIChatCompletionClient

   model_client = OpenAIChatCompletionClient(
       model="gpt-4o",
       api_key="sk-xxx"
   )
   ```

3. **Assistant Agent**:
   - **v0.2**: Used `llm_config`.
   - **v0.4**: Use `model_client`.
   ```python
   from autogen_agentchat.agents import AssistantAgent

   assistant = AssistantAgent(
       name="assistant",
       system_message="You are a helpful assistant.",
       model_client=model_client
   )
   ```

4. **Message Handling**:
   - **v0.2**: Used `assistant.send`.
   - **v0.4**: Use `assistant.on_messages` or `assistant.on_messages_stream`.
   ```python
   response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], CancellationToken())
   ```

5. **Tool Use**:
   - **v0.2**: Required two agents for tool calling and execution.
   - **v0.4**: Use a single `AssistantAgent` with tools.
   ```python
   assistant = AssistantAgent(
       name="assistant",
       system_message="You are a helpful assistant.",
       model_client=model_client,
       tools=[get_weather],
       reflect_on_tool_use=True
   )
   ```

6. **Group Chat**:
   - **v0.2**: Used `GroupChat` and `GroupChatManager`.
   - **v0.4**: Use `RoundRobinGroupChat`.
   ```python
   from autogen_agentchat.teams import RoundRobinGroupChat

   group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)
   ```

7. **State Management**:
   - **v0.4**: Use `save_state` and `load_state` for agents and group chats.

8. **Code Executors**:
   - **v0.4**: Supports async API and cancellation with `CancellationToken`.

9. **Observability and Control**:
   - **v0.4**: Use `on_messages_stream` and `run_stream` for real-time observation.

10. **Migration**:
    - **v0.4**: Provides conversion functions for messages between v0.2 and v0.4 formats.

For more detailed examples and explanations, refer to the specific sections in the migration guide.

Here's a concise guide on setting up a group chat with agents using the `autogen_agentchat` library, focusing on creating a team of agents, running a chat session, and saving/loading the chat state.

### Setting Up a Group Chat

1. **Import Required Modules:**
   ```python
   import asyncio
   import json
   from autogen_agentchat.agents import AssistantAgent
   from autogen_agentchat.teams import RoundRobinGroupChat
   from autogen_agentchat.conditions import TextMentionTermination
   from autogen_agentchat.ui import Console
   from autogen_ext.models.openai import OpenAIChatCompletionClient
   ```

2. **Create a Team of Agents:**
   Define a function to create a team using `RoundRobinGroupChat` with agents like `writer` and `critic`.
   ```python
   def create_team() -> RoundRobinGroupChat:
       model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
       writer = AssistantAgent(name="writer", description="A writer.", system_message="You are a writer.", model_client=model_client)
       critic = AssistantAgent(name="critic", description="A critic.", system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.", model_client=model_client)
       termination = TextMentionTermination("APPROVE")
       group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)
       return group_chat
   ```

3. **Run the Chat Session:**
   Use an async function to run the chat session and display messages using `Console`.
   ```python
   async def main() -> None:
       group_chat = create_team()
       stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
       await Console(stream)
   ```

4. **Save and Load Chat State:**
   Save the chat state to a file and load it back to resume the chat.
   ```python
   async def save_and_load_state(group_chat):
       state = await group_chat.save_state()
       with open("group_chat_state.json", "w") as f:
           json.dump(state, f)
       with open("group_chat_state.json", "r") as f:
           state = json.load(f)
       await group_chat.load_state(state)
   ```

5. **Execute the Main Function:**
   Run the main function using `asyncio`.
   ```python
   asyncio.run(main())
   ```

### Additional Features

- **Group Chat with Tool Use:** In v0.4, tools are directly executed within the `AssistantAgent`, simplifying the process.
- **Custom Selector:** Use `SelectorGroupChat` with a `selector_func` to customize speaker selection.
- **Nested Chat:** Implement nested chat by creating a custom agent that triggers a nested team or agent.
- **Long Context Handling:** Use `BufferedChatCompletionContext` to manage message history effectively.

This setup allows for flexible and interactive agent-based chat applications, leveraging the capabilities of the `autogen_agentchat` library.

```python
# Example of setting up a group chat with an assistant and code executor

from autogen.agentchat import AssistantAgent, CodeExecutorAgent, RoundRobinGroupChat
from autogen.agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen.agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core import LocalCommandLineCodeExecutor

async def main():
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)

    group_chat = RoundRobinGroupChat(
        [assistant, code_executor],
        termination_condition=termination
    )

    stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
    await Console(stream)

asyncio.run(main())
```

### Tool Use Example

```python
from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function

llm_config = {
    "config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
    "seed": 42,
    "temperature": 0,
}

tool_caller = AssistantAgent(
    name="tool_caller",
    system_message="You are a helpful assistant. You can call tools to help user.",
    llm_config=llm_config,
    max_consecutive_auto_reply=1,
)

tool_executor = UserProxyAgent(
    name="tool_executor",
    human_input_mode="NEVER",
    code_execution_config=False,
    llm_config=False,
)

def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

register_function(get_weather, caller=tool_caller, executor=tool_executor)

while True:
    user_input = input("User: ")
    if user_input == "exit":
        break
    chat_result = tool_executor.initiate_chat(
        tool_caller,
        message=user_input,
        summary_method="reflection_with_llm",
    )
    print("Assistant:", chat_result.summary)
```

### Group Chat with Resume

```python
import asyncio
import json
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

def create_team() -> RoundRobinGroupChat:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    writer = AssistantAgent(
        name="writer",
        description="A writer.",
        system_message="You are a writer.",
        model_client=model_client,
    )

    critic = AssistantAgent(
        name="critic",
        description="A critic.",
        system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
        model_client=model_client,
    )

    termination = TextMentionTermination("APPROVE")

    group_chat = RoundRobinGroupChat(
        [writer, critic],
        termination_condition=termination
    )

    return group_chat

async def main() -> None:
    group_chat = create_team()

    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)

    state = await group_chat.save_state()
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    group_chat = create_team()
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)

    await group_chat.load_state(state)

    stream = group_chat.run_stream(task="Translate the story into Chinese.")
    await Console(stream)

asyncio.run(main())
```

### Long Context Handling

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    assistant =

In version 0.4, there are two primary ways to create a model client: using a component configuration or directly using a model client class. Here's a brief overview of both methods:

### Using Component Configuration

AutoGen 0.4 introduces a generic component configuration system. Here's how you can create an OpenAI chat completion client using this system:

```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o",
        "api_key": "sk-xxx"  # or use os.environ["..."]
    }
}

model_client = ChatCompletionClient.load_component(config)
```

### Using Model Client Class Directly

#### OpenAI

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)
```

#### Azure OpenAI

```python
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient

model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="gpt-4o",
    azure_endpoint="https://<your-endpoint>.openai.azure.com/",
    model="gpt-4o",
    api_version="2024-09-01-preview",
    api_key="sk-xxx"
)
```

### Model Client for OpenAI-Compatible APIs

To connect to an OpenAI-Compatible API, specify the `base_url` and `model_info`:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

custom_model_client = OpenAIChatCompletionClient(
    model="custom-model-name",
    base_url="https://custom-model.com/reset/of/the/path",
    api_key="placeholder",
    model_info={
        "vision": True,
        "function_calling": True,
        "json_output": True,
        "family": "unknown",
    }
)
```

### Model Client Cache

In v0.4, caching is not enabled by default. To use caching, wrap the model client with `ChatCompletionCache` and choose a storage option like `DiskCacheStore` or `RedisStore`.

```python
import asyncio
import tempfile
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache

async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response

asyncio.run(main())
```

### Assistant Agent

In v0.4, you create an assistant agent by specifying a `model_client`:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx",
    seed=42,
    temperature=0
)

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
)
```

To handle messages, use `assistant.on_messages` or `assistant.on_messages_stream`:

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken

async def main() -> None:
    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent

The `AssistantAgent` in v0.4 supports multi-modal inputs if the model client supports it:

```python
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image

async def main() -> None:
    message = MultiModalMessage(
        content=["Here is an image:", Image.from_file(Path("test.png"))],
        source="user",
    )
    response = await assistant.on_messages([message], cancellation_token)
    print(response)

asyncio.run(main())
``

# Migration Guide for v0.2 to v0.4 â€” AutoGen

This guide helps users transition from `v0.2.*` to `v0.4` of `autogen-agentchat`, which introduces new APIs and features. The `v0.4` version includes breaking changes, so careful reading is recommended. The `v0.2` version is still maintained in the `0.2` branch, but upgrading to `v0.4` is highly recommended.

## Key Changes in v0.4

### Model Client

In `v0.4`, model clients are configured using a component configuration system or directly through model client classes. Here's how to create an OpenAI chat completion client:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)
```

### Assistant Agent

In `v0.4`, the `AssistantAgent` requires a `model_client` instead of `llm_config`. Usage involves calling `assistant.on_messages` or `assistant.on_messages_stream` for handling messages:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx")

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client
)
```

### Tool Use

In `v0.4`, a single `AssistantAgent` can handle both tool calling and execution:

```python
from autogen_agentchat.agents import AssistantAgent

def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant. You can call tools to help user.",
    model_client=model_client,
    tools=[get_weather],
    reflect_on_tool_use=True
)
```

### Group Chat

`v0.4` introduces `SelectorGroupChat` for managing multiple agents with a custom selector function:

```python
from autogen_agentchat.teams import SelectorGroupChat

team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
    termination_condition=termination,
    selector_func=selector_func
)
```

### Nested Chat

Nested chat allows for hierarchical agent structures. In `v0.4`, you can create a custom agent that triggers a nested team or agent:

```python
class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])
```

### Observability and Control

`v0.4` provides `on_messages_stream` and `run_stream` methods for real-time observation of agents and teams. These methods use a `CancellationToken` to cancel streams asynchronously.

### Code Executors

`v0.4` supports async API for code executors and introduces `AzureContainerCodeExecutor` for Azure Container Apps dynamic sessions.

### Long Context Handling

`v0.4` introduces `ChatCompletionContext` for managing message history, with implementations like `BufferedChatCompletionContext` to limit message history sent to the model.

```python
from autogen_core.model_context import BufferedChatCompletionContext

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
    model_context=BufferedChatCompletionContext(buffer_size=10)
)
```

This guide provides a comprehensive overview of the changes and new features in `v0.4`, helping users transition smoothly from `v0.2`. For more detailed examples and tutorials, refer to the official documentation.

```python
    model_client=model_client,
)
```

### Multi-Modal Agent

In **v0.2**, you create a multi-modal agent using the `MultiModalAgent` class:

```python
from autogen.agentchat import MultiModalAgent

llm_config = {
    "config_list": [{
        "model": "gpt-4o",
        "api_key": "sk-xxx"
    }],
    "seed": 42,
    "temperature": 0,
}

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal agent.",
    llm_config=llm_config,
)
```

In **v0.4**, you use the `MultiModalAgent` class similarly, but specify `model_client`:

```python
from autogen_agentchat.agents import MultiModalAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx",
    seed=42,
    temperature=0
)

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal agent.",
    model_client=model_client,
)
```

### User Proxy

In **v0.2**, a user proxy is used to register tool functions and manage tool calls:

```python
from autogen.agentchat import UserProxy

user_proxy = UserProxy(
    name="user_proxy",
    description="A user proxy for tool calls.",
    llm_config=llm_config,
)
```

In **v0.4**, the concept of a user proxy is replaced by directly passing tool functions to the `AssistantAgent`:

```python
from autogen_agentchat.agents import AssistantAgent

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
    tools=[your_tool_function],
)
```

### Conversable Agent and Register Reply

In **v0.2**, you use `ConversableAgent` to register replies:

```python
from autogen.agentchat import ConversableAgent

agent = ConversableAgent(
    name="agent",
    llm_config=llm_config,
)

agent.register_reply("Hello", "Hi there!")
```

In **v0.4**, you use `AssistantAgent` and handle replies in the `on_messages` method:

```python
from autogen_agentchat.agents import AssistantAgent

class CustomAgent(AssistantAgent):
    async def on_messages(self, messages, cancellation_token):
        if messages[-1].content == "Hello":
            return Response(chat_message=TextMessage(content="Hi there!", source=self.name))

agent = CustomAgent(
    name="agent",
    model_client=model_client,
)
```

### Save and Load Agent State

In **v0.2**, you manually save and load agent states:

```python
state = agent.save_state()
agent.load_state(state)
```

In **v0.4**, you use `save_state` and `load_state` methods:

```python
state = await agent.save_state()
await agent.load_state(state)
```

### Two-Agent Chat

In **v0.2**, you create a two-agent chat using `TwoAgentChat`:

```python
from autogen.agentchat import TwoAgentChat

chat = TwoAgentChat(agent1, agent2)
result = chat.run("Hello")
```

In **v0.4**, you use `RoundRobinGroupChat` for similar functionality:

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat([agent1, agent2])
result = await group_chat.run("Hello")
```

### Tool Use

In **v0.2**, tools are registered on a user proxy:

```python
user_proxy.register_tool(your_tool_function)
```

In **v0.4**, tools are directly passed to the `AssistantAgent`:

```python
assistant = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[your_tool_function],
)
```

### Chat Result

In **v0.2**, chat results are accessed via `ChatResult`:

```python
result = chat.run("Hello")
print(result.summary)
```

In **v0.4**, results are accessed directly from the `run` method:

```python
result = await group_chat.run("Hello")
print(result.messages)
```

### Conversion between v0.2 and v0.4 Messages

Use the provided conversion functions to convert messages between versions:

```python
from autogen_agentchat.base import convert_to_v02_message, convert_to_v04_message

v02_message = convert_to_v02_message(v04_message, role="assistant

In version 0.4, the usage of the `AssistantAgent` has been updated to handle incoming messages asynchronously using `on_messages` or `on_messages_stream`. These methods allow for streaming the agent's responses. Below are examples and explanations of various functionalities in version 0.4:

### Basic Usage
To call the assistant agent:
```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent
The `AssistantAgent` supports multi-modal inputs if the model client supports it:
```python
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken, Image
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    message = MultiModalMessage(
        content=["Here is an image:", Image.from_file(Path("test.png"))],
        source="user",
    )
    response = await assistant.on_messages([message], cancellation_token)
    print(response)

asyncio.run(main())
```

### User Proxy
In version 0.4, a user proxy is simply an agent that takes user input only:
```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent("user_proxy")
```

### Conversable Agent and Register Reply
To create a custom agent and implement message handling:
```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State
Agents can save and load their state:
```python
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

    # Save the state.
    state = await assistant.save_state()

    # (Optional) Write state to disk.
    with open("assistant_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk.
    with open("assistant_state.json", "r") as f:
        state = json.load(f)
        print(state)  # Inspect the state, which contains the chat history.

    # Carry on the chat.
    response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)
    print(response)

    # Load the state, resulting the agent to revert to the previous state before the last message.
    await assistant.load_state(state)

    # Carry on the same chat again.
    response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)

asyncio.run(main())
```

### Two-Agent Chat
To create a two-agent

The provided code and documentation describe a system for creating and managing chat agents using an asynchronous, event-driven architecture. Here's a summary of the key components and functionalities:

### Key Components

1. **NestedCountingAgent**: 
   - An agent that counts numbers using a team of counting agents.
   - Implements `on_messages` to process messages and `on_reset` to reset the counting team.

2. **RoundRobinGroupChat**:
   - A group chat mechanism that alternates between agents, useful for tasks like counting or tool execution.

3. **AssistantAgent**:
   - A general-purpose agent that can handle messages, execute tools, and manage state.
   - Supports multi-modal inputs if the model client supports it.

4. **CodeExecutorAgent**:
   - An agent designed to execute code, often used in conjunction with an assistant agent for tasks requiring code execution.

5. **Tool Use**:
   - Allows agents to call and execute tools, with the ability to reflect on tool use.

6. **Observability and Control**:
   - Provides methods like `on_messages_stream` and `run_stream` to observe and control agent interactions in real-time.

7. **Model Client**:
   - Supports OpenAI and Azure OpenAI models, with caching capabilities for efficient message handling.

8. **Sequential Chat**:
   - Although not directly supported in v0.4, users can create sequential workflows using the Core API.

### Usage Examples

- **Counting Agent Example**:
  ```python
  async def main() -> None:
      counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
      counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
      counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
      nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
      response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
      for message in response.inner_messages:
          print(message)
      print(response.chat_message)

  asyncio.run(main())
  ```

- **Assistant Agent with Tool Use**:
  ```python
  async def main() -> None:
      model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx")
      assistant = AssistantAgent(
          name="assistant",
          system_message="You are a helpful assistant. You can call tools to help user.",
          model_client=model_client,
          tools=[get_weather],
          reflect_on_tool_use=True
      )
      while True:
          user_input = input("User: ")
          if user_input == "exit":
              break
          response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
          print("Assistant:", response.chat_message.content)

  asyncio.run(main())
  ```

### Migration Notes

- **From v0.2 to v0.4**:
  - The architecture has shifted to an asynchronous, event-driven model.
  - The `AssistantAgent` now uses `model_client` instead of `llm_config`.
  - The `on_messages` and `on_messages_stream` methods replace the previous `send` method.
  - Caching and tool execution are more integrated and flexible.

This system is designed to be flexible and scalable, allowing for complex workflows and interactions between agents. The migration from v0.2 to v0.4 introduces significant changes, focusing on asynchronous operations and enhanced observability.

```python
self,
    messages: Sequence[ChatMessage],
    cancellation_token: CancellationToken
) -> Response:
    # Custom reply logic here
    return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

async def on_reset(self, cancellation_token: CancellationToken) -> None:
    pass

@property
def produced_message_types(self) -> Sequence[type[ChatMessage]]:
    return (TextMessage,)
```

In this example, `CustomAgent` is a custom agent that implements the `on_messages` method to provide a custom reply. The `on_reset` method is used to reset the agent's state, and `produced_message_types` specifies the types of messages the agent can produce.

### Save and Load Agent State

In v0.2, saving and loading agent state was not directly supported. In v0.4, you can use the `save_state` and `load_state` methods to manage agent state.

```python
state = await agent.save_state()
await agent.load_state(state)
```

### Two-Agent Chat

In v0.2, you could create a two-agent chat using `initiate_chat`. In v0.4, you can use the `RoundRobinGroupChat` or `SelectorGroupChat` for more complex scenarios.

### Tool Use

In v0.2, tools were registered on a user proxy. In v0.4, tools are directly associated with the `AssistantAgent`, and the agent can call tools as needed.

### Chat Result

In v0.4, the chat result is returned as a `Response` object, which includes the final message and any intermediate messages.

### Conversion between v0.2 and v0.4 Messages

In v0.4, messages are more structured, using classes like `TextMessage` and `MultiModalMessage`. This provides better type safety and clarity.

### Group Chat

In v0.4, group chat is managed using `RoundRobinGroupChat` or `SelectorGroupChat`, providing more flexibility and control over the chat flow.

### Group Chat with Resume

In v0.4, you can resume a group chat by calling `run` or `run_stream` again with the same group chat object. Use `save_state` and `load_state` to manage chat state.

### Group Chat with Tool Use

In v0.4, tools are directly executed within the `AssistantAgent`, simplifying the process and removing the need for a user proxy.

### Group Chat with Custom Selector (Stateflow)

In v0.4, use `SelectorGroupChat` with a `selector_func` to implement custom speaker selection logic.

### Nested Chat

In v0.4, nested chat is implemented by creating a custom agent that manages a nested team or agent.

### Sequential Chat

In v0.4, sequential chat is not directly supported. Instead, use the Core API to create event-driven workflows.

### GPTAssistantAgent

In v0.4, use `OpenAIAssistantAgent` for similar functionality, with additional features like customizable threads and file uploads.

### Long Context Handling

In v0.4, use `BufferedChatCompletionContext` to manage long contexts, limiting the message history sent to the model.

### Observability and Control

In v0.4, use `on_messages_stream` and `run_stream` to observe and control agent behavior in real-time.

### Code Executors

In v0.4, code executors support async API and can be canceled using `CancellationToken`.

This guide provides a comprehensive overview of migrating from v0.2 to v0.4, focusing on key changes and new features. For more detailed examples and tutorials, refer to the official documentation.

```python
        context=BufferedChatCompletionContext(max_tokens=2048)  # Limit the message history to 2048 tokens.
    )

    cancellation_token = CancellationToken()

    response = await assistant.on_messages(
        [TextMessage(content="Hello!", source="user")],
        cancellation_token
    )

    print(response)

asyncio.run(main())
```

### Custom Agent

You can create a custom agent by implementing the `BaseChatAgent` class. Here's a simple example:

```python
class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State

In v0.4, you can call `save_state` and `load_state` methods on agents to save and load their state.

```python
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    cancellation_token = CancellationToken()
    response = await assistant.on_messages(
        [TextMessage(content="Hello!", source="user")],
        cancellation_token
    )
    print(response)

    # Save the state.
    state = await assistant.save_state()

    # (Optional) Write state to disk.
    with open("assistant_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk.
    with open("assistant_state.json", "r") as f:
        state = json.load(f)
    print(state)  # Inspect the state, which contains the chat history.

    # Carry on the chat.
    response = await assistant.on_messages(
        [TextMessage(content="Tell me a joke.", source="user")],
        cancellation_token
    )
    print(response)

    # Load the state, resulting the agent to revert to the previous state before the last message.
    await assistant.load_state(state)

    # Carry on the same chat again.
    response = await assistant.on_messages(
        [TextMessage(content="Tell me a joke.", source="user")],
        cancellation_token
    )

asyncio.run(main())
```

### Two-Agent Chat

In v0.4, you can use the `AssistantAgent` and `CodeExecutorAgent` together in a `RoundRobinGroupChat`.

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)
    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)

    stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
    await Console(stream)

asyncio.run(main())
```

### Tool Use

In v0.4, you really just need one agent â€“ the `AssistantAgent` â€“ to handle both the tool calling and tool execution.

```python
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage

def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degree and sunny."

async def

```python
ogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    writer = AssistantAgent(
        name="writer",
        description="A writer.",
        system_message="You are a writer.",
        model_client=model_client,
    )

    critic = AssistantAgent(
        name="critic",
        description="A critic.",
        system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
        model_client=model_client,
    )

    termination = TextMentionTermination("APPROVE")

    group_chat = RoundRobinGroupChat(
        [writer, critic],
        termination_condition=termination,
        max_turns=12
    )

    stream = group_chat.run_stream(
        task="Write a short story about a robot that discovers it has feelings."
    )

    await Console(stream)

    # Save the state
    state = await group_chat.save_state()

    # (Optional) Write state to disk
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)

    # Load the state
    await group_chat.load_state(state)

    # Resume the chat
    stream = group_chat.run_stream(
        task="Continue the story."
    )

    await Console(stream)

asyncio.run(main())
```

### Group Chat with Tool Use

In `v0.4`, you can add tools directly to agents in a group chat. Here's an example:

```python
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console

def get_weather(city: str) -> str:
    return f"The weather in {city} is 72 degrees and sunny."

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        seed=42,
        temperature=0
    )

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant. You can call tools to help user.",
        model_client=model_client,
        tools=[get_weather],
        reflect_on_tool_use=True,
    )

    termination = TextMentionTermination("TERMINATE")

    group_chat = RoundRobinGroupChat(
        [assistant],
        termination_condition=termination
    )

    stream = group_chat.run_stream(
        task="Ask about the weather in New York."
    )

    await Console(stream)

asyncio.run(main())
```

### Group Chat with Custom Selector (Stateflow)

For LLM-based speaker selection, use `SelectorGroupChat`. See the `Selector Group Chat Tutorial` for more details.

### Nested Chat

In `v0.4`, you can create nested chats by using the `NestedGroupChat` class. This allows for more complex interactions between agents.

### Sequential Chat

For sequential interactions, use the `SequentialGroupChat` class to manage the order of agent interactions.

### GPTAssistantAgent

The `GPTAssistantAgent` in `v0.4` is designed for more advanced use cases, supporting features like long context handling and multi-modal inputs.

### Long Context Handling

In `v0.4`, long context handling is improved with better memory management and context summarization techniques.

### Observability and Control

Use `on_messages_stream` and `run_stream` methods for real-time observability and control over agent interactions.

### Code Executors

The `v0.4` code executors support async API and can be canceled using `CancellationToken`. Use `AzureContainerCodeExecutor` for dynamic sessions with Azure Container Apps.

### Migration Guide Summary

This guide provides detailed instructions for migrating from `v0.2` to `v0.4`, covering new APIs, features, and best practices for building agentic applications with AutoGen.

```python
    )


    code_executor_agent = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
    )


    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(max_messages=10)


    group_chat = RoundRobinGroupChat(
        [assistant, code_executor_agent],
        termination_condition=termination
    )


    task = "Write a python script to print 'Hello, world!'"
    stream = group_chat.run_stream(task=task)
    await Console(stream)


asyncio.run(main())
```

### Save and Load Group Chat State

In `v0.2`, you need to explicitly save the group chat messages and load them back when you want to resume the chat. In `v0.4`, you can simply call `save_state` and `load_state` methods on the group chat object.

### Group Chat with Tool Use

In `v0.2`, when tools are involved, you need to register the tool functions on a user proxy. In `v0.4`, tools are directly executed within the `AssistantAgent`, which publishes the response from the tool to the group chat.

### Group Chat with Custom Selector (Stateflow)

In `v0.2`, a custom function can override the default speaker selection method. In `v0.4`, use the `SelectorGroupChat` with `selector_func` to achieve the same behavior.

### Nested Chat

In `v0.2`, nested chat is supported by using the `register_nested_chats` method. In `v0.4`, nested chat is an implementation detail of a custom agent.

### Sequential Chat

In `v0.2`, sequential chat is supported by using the `initiate_chats` function. In `v0.4`, you can create an event-driven sequential workflow using the Core API.

### GPTAssistantAgent

In `v0.2`, `GPTAssistantAgent` is backed by the OpenAI Assistant API. In `v0.4`, the equivalent is the `OpenAIAssistantAgent` class.

### Long Context Handling

In `v0.2`, long context is handled by using the `transforms` capability. In `v0.4`, use the `ChatCompletionContext` base class to manage message history.

### Observability and Control

In `v0.4`, observe agents using the `on_messages_stream` method, which returns an async generator to stream the inner thoughts and actions of the agent.

### Code Executors

The code executors in `v0.2` and `v0.4` are nearly identical, except `v0.4` executors support async API. Use `CancellationToken` to cancel a code execution if it takes too long.

### Migration Guide for v0.2 to v0.4

This guide helps users migrate from `v0.2.*` to `v0.4`, which introduces a new set of APIs and features. The `v0.4` version contains breaking changes, so read the guide carefully.

### Migration Guide for v0.2 to v0.4 â€” AutoGen

This guide helps users transition from `v0.2.*` to `v0.4` of `autogen-agentchat`, which introduces new APIs and features. The `v0.4` version includes breaking changes, so careful reading is advised. The `v0.2` version is still maintained in the `0.2` branch, but upgrading to `v0.4` is recommended.

#### Key Changes in v0.4

- **Asynchronous, Event-Driven Architecture**: Enhances observability, flexibility, interactive control, and scalability.
- **Layered API**: 
  - **Core API**: Provides a scalable, event-driven actor framework.
  - **AgentChat API**: Built on Core, offers a high-level framework for interactive applications.

#### Migration Highlights

- **Model Client**: 
  - In `v0.2`, the `OpenAIWrapper` object was used.
  - In `v0.4`, use a specific model configuration instead of a list.

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    seed=42,
    temperature=0
)
```

- **Tool Use**: 
  - In `v0.2`, two agents were required for tool use.
  - In `v0.4`, a single `AssistantAgent` can handle both tool calling and execution.

```python
from autogen_agentchat.agents import AssistantAgent

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant. You can call tools to help user.",
    model_client=model_client,
    tools=[get_weather],
    reflect_on_tool_use=True
)
```

- **Group Chat**:
  - In `v0.2`, a `GroupChat` class was used.
  - In `v0.4`, use `RoundRobinGroupChat` for similar functionality.

```python
from autogen_agentchat.teams import RoundRobinGroupChat

group_chat = RoundRobinGroupChat(
    [writer, critic],
    termination_condition=TextMentionTermination("APPROVE"),
    max_turns=12
)
```

- **Nested Chat**: 
  - In `v0.2`, nested chat was supported via `register_nested_chats`.
  - In `v0.4`, create a custom agent that implements the `on_messages` method.

- **Long Context Handling**:
  - In `v0.2`, handled by `transforms`.
  - In `v0.4`, use `BufferedChatCompletionContext` to manage message history.

```python
from autogen_core.model_context import BufferedChatCompletionContext

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
    model_context=BufferedChatCompletionContext(buffer_size=10)
)
```

- **Observability and Control**:
  - Use `on_messages_stream` and `run_stream` methods for real-time observation.
  - Utilize `CancellationToken` for asynchronous cancellation.

#### Additional Features

- **Code Executors**: Now support async API and `CancellationToken` for long-running tasks.
- **Group Chat with Tool Use**: Tools are directly executed within `AssistantAgent`.

For more detailed examples and further migration steps, refer to the full documentation and tutorials provided with `v0.4`.

To create an OpenAI chat completion client, you can use the `ChatCompletionClient` from the `autogen_core.models` module. Here's a basic setup:

```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o",
        "api_key": "sk-xxx"  # Replace with your actual API key
    }
}

model_client = ChatCompletionClient.load_component(config)
```

### Direct Usage of Model Client Classes

#### OpenAI
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)
```

#### Azure OpenAI
```python
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient

model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="gpt-4o",
    azure_endpoint="https://<your-endpoint>.openai.azure.com/",
    model="gpt-4o",
    api_version="2024-09-01-preview",
    api_key="sk-xxx"
)
```

### Model Client for OpenAI-Compatible APIs
To connect to an OpenAI-Compatible API, specify the `base_url` and `model_info`:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

custom_model_client = OpenAIChatCompletionClient(
    model="custom-model-name",
    base_url="https://custom-model.com/reset/of/the/path",
    api_key="placeholder",
    model_info={
        "vision": True,
        "function_calling": True,
        "json_output": True,
        "family": "unknown",
    }
)
```

### Model Client Cache
In version 0.2, you can set the cache seed through the `cache_seed` parameter. In version 0.4, the cache is not enabled by default, and you need to use a `ChatCompletionCache` wrapper:

```python
import asyncio
import tempfile
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache

async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)

asyncio.run(main())
```

### Assistant Agent
In version 0.4, specify `model_client` instead of `llm_config`:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx",
    seed=42,
    temperature=0
)

assistant = AssistantAgent(
    name="assistant",
    system_message="You are a helpful assistant.",
    model_client=model_client,
)
```

### Multi-Modal Agent
The `AssistantAgent` in version 0.4 supports multi-modal inputs if the model client supports it:

```python
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken, Image
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main():
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    cancellation_token = CancellationToken()
    message = MultiModalMessage(
        content=["Here is an image:", Image.from_file(Path("test.png"))],
        source="user",
    )

    response = await assistant.on_messages([message], cancellation_token)
    print(response)

asyncio.run(main())
```

### Group Chat
In version 0.4, use `RoundRobinGroupChat` for group chat scenarios:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from aut

```python
list.
```

### Group Chat

In `v0.2`, you can create a group chat with multiple agents as follows:

```python
from autogen.agentchat import AssistantAgent, UserProxyAgent, GroupChat

llm_config = {
    "config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
    "seed": 42,
    "temperature": 0,
}

assistant_1 = AssistantAgent(
    name="assistant_1",
    system_message="You are a helpful assistant.",
    llm_config=llm_config,
)

assistant_2 = AssistantAgent(
    name="assistant_2",
    system_message="You are a helpful assistant.",
    llm_config=llm_config,
)

user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    code_execution_config=False,
    llm_config=False,
)

group_chat = GroupChat(
    agents=[assistant_1, assistant_2, user_proxy],
    max_consecutive_auto_reply=10,
)

chat_result = group_chat.initiate_chat(
    message="Hello, everyone!"
)

print(chat_result)
```

In `v0.4`, you can create a group chat using `RoundRobinGroupChat`:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        api_key="sk-xxx",
        seed=42,
        temperature=0
    )

    assistant_1 = AssistantAgent(
        name="assistant_1",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    assistant_2 = AssistantAgent(
        name="assistant_2",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    user_proxy = UserProxyAgent(name="user_proxy")

    group_chat = RoundRobinGroupChat(
        [assistant_1, assistant_2, user_proxy],
        max_turns=10
    )

    stream = group_chat.run_stream(task="Hello, everyone!")
    await Console(stream)

asyncio.run(main())
```

### Group Chat with Resume

In `v0.2`, you can resume a group chat by saving and loading the chat history manually.

In `v0.4`, you can use the `save_state` and `load_state` methods to save and resume the state of a group chat:

```python
import asyncio
import json
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        api_key="sk-xxx",
        seed=42,
        temperature=0
    )

    assistant_1 = AssistantAgent(
        name="assistant_1",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    assistant_2 = AssistantAgent(
        name="assistant_2",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    user_proxy = UserProxyAgent(name="user_proxy")

    group_chat = RoundRobinGroupChat(
        [assistant_1, assistant_2, user_proxy],
        max_turns=10
    )

    # Run the group chat
    stream = group_chat.run_stream(task="Hello, everyone!")
    await Console(stream)

    # Save the state
    state = await group_chat.save_state()

    # (Optional) Write state to disk
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)

    # Load the state
    await group_chat.load_state(state)

    # Resume the group chat
    stream = group_chat.run_stream(task="Continue the conversation.")
    await Console(stream)

asyncio.run(main())
```

### Group Chat with Tool Use

In `v0.2`, you can create a group chat with tool use by registering tools with each agent.

In `v0.4`, you can add tools to agents in a group chat in the same way as you would for a single agent:

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent

# Migration Guide for v0.2 to v0.4 â€” AutoGen

This guide helps users transition from `autogen-agentchat` v0.2 to v0.4, which introduces a new set of APIs and features. The v0.4 version contains breaking changes, so it's important to read this guide carefully. The v0.2 version is still maintained in the `0.2` branch, but upgrading to v0.4 is recommended.

## Conversion between v0.2 and v0.4 Messages

To convert between v0.4 and v0.2 messages, use the following functions:

```python
def convert_to_v02_message(message: AgentEvent | ChatMessage, role: Literal["assistant", "user", "tool"], image_detail: Literal["auto", "high", "low"] = "auto") -> Dict[str, Any]:
    # Conversion logic here

def convert_to_v04_message(message: Dict[str, Any]) -> AgentEvent | ChatMessage:
    # Conversion logic here
```

## Group Chat

### v0.2 Example

```python
from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager

llm_config = {
    "config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
    "seed": 42,
    "temperature": 0,
}

writer = AssistantAgent(name="writer", description="A writer.", system_message="You are a writer.", llm_config=llm_config)
critic = AssistantAgent(name="critic", description="A critic.", system_message="You are a critic.", llm_config=llm_config)

groupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)
manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method="round_robin")

result = editor.initiate_chat(manager, message="Write a short story about a robot that discovers it has feelings.")
print(result.summary)
```

### v0.4 Example

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    writer = AssistantAgent(name="writer", description="A writer.", system_message="You are a writer.", model_client=model_client)
    critic = AssistantAgent(name="critic", description="A critic.", system_message="You are a critic.", model_client=model_client)

    termination = TextMentionTermination("APPROVE")
    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)

    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)

asyncio.run(main())
```

## Group Chat with Resume

In v0.4, you can resume a chat by calling `run` or `run_stream` again with the same group chat object. Use `save_state` and `load_state` methods to export and load the state.

```python
async def main() -> None:
    group_chat = create_team()
    stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
    await Console(stream)

    state = await group_chat.save_state()
    with open("group_chat_state.json", "w") as f:
        json.dump(state, f)

    group_chat = create_team()
    with open("group_chat_state.json", "r") as f:
        state = json.load(f)
    await group_chat.load_state(state)

    stream = group_chat.run_stream(task="Translate the story into Chinese.")
    await Console(stream)

asyncio.run(main())
```

## Group Chat with Tool Use

In v0.4, tools are directly executed within the `AssistantAgent`, eliminating the need for a user proxy to route tool calls.

## Nested Chat

In v0.4, nested chat is implemented by creating a custom agent that takes a team or another agent as a parameter and implements the `on_messages` method to trigger the nested team or agent.

```python
class NestedCountingAgent(BaseChatAgent):
    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken)

### Overview of Asynchronous Agent Methods

The `_stream` methods are asynchronous and return an async generator to stream the agent's internal processes. This allows for real-time interaction with the agent's thought process.

### Using the Assistant Agent in v0.4

To call the assistant agent, you can use the following setup:

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    response = await assistant.on_messages(
        [TextMessage(content="Hello!", source="user")],
        cancellation_token
    )
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent Support

The `AssistantAgent` in v0.4 supports multi-modal inputs if the model client supports it. Here's an example:

```python
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken, Image
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    message = MultiModalMessage(
        content=["Here is an image:", Image.from_file(Path("test.png"))],
        source="user",
    )
    response = await assistant.on_messages([message], cancellation_token)
    print(response)

asyncio.run(main())
```

### User Proxy in v0.4

In v0.4, a user proxy is simply an agent that takes user input only. You can create it as follows:

```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent("user_proxy")
```

### Conversable Agent and Register Reply

In v0.4, you can create a custom agent by implementing the `on_messages`, `on_reset`, and `produced_message_types` methods:

```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State

In v0.4, you can save and load an agent's state using `save_state` and `load_state` methods:

```python
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )
    cancellation_token = CancellationToken()
    response = await assistant.on_messages(
        [TextMessage(content="Hello!", source="user")],
        cancellation_token
    )
    print(response)

    # Save the state.
    state = await assistant.save_state()

    # Write state to disk.
    with open("assistant_state.json", "w") as f:
        json.dump(state, f)

    # Load it back from disk.
    with open("assistant_state.json", "r") as f:
        state = json.load(f)
        print(state)

    # Load the state.
    await assistant.load_state(state)

asyncio.run(main())
```

### Two-Agent Chat

In v0.4, you can use `AssistantAgent` and `CodeExecutorAgent` together in a `RoundRobinGroupChat`:

```python
import asyncio
from autogen_agent

The provided documentation outlines the usage and configuration of various components in the AutoGen framework, focusing on chat completion clients for OpenAI and Azure OpenAI models. Here's a concise summary:

### Key Components and Functions

1. **NestedCountingAgent**:
   - Runs a team of counting agents and returns the last message.
   - Example usage involves creating a team of agents and running them with a starting message.

   ```python
   async def main() -> None:
       counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
       counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
       counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
       nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
       response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
       for message in response.inner_messages:
           print(message)
       print(response.chat_message)
   asyncio.run(main())
   ```

2. **Sequential Chat**:
   - In v0.4, sequential chat is not directly supported by a built-in function. Instead, users can create event-driven workflows using the Core API.

3. **OpenAIAssistantAgent**:
   - Replaced by `OpenAIAssistantAgent` in v0.4, supporting customizable threads and file uploads.

4. **Long Context Handling**:
   - Managed by `ChatCompletionContext` in v0.4, allowing for message history management with implementations like `BufferedChatCompletionContext`.

   ```python
   from autogen_core.model_context import BufferedChatCompletionContext
   assistant = AssistantAgent(
       name="assistant",
       system_message="You are a helpful assistant.",
       model_context=BufferedChatCompletionContext(buffer_size=10)
   )
   ```

5. **Observability and Control**:
   - Use `on_messages_stream` and `run_stream` methods to observe agent actions in real-time.

6. **Code Executors**:
   - v0.4 supports async API and `CancellationToken` for canceling long executions.

7. **Component Configuration**:
   - Components can be configured using `dump_component()` and `load_component()` methods.

   ```python
   from autogen_core.models import ChatCompletionClient
   config = {"provider": "openai_chat_completion_client", "config": {"model": "gpt-4o"}}
   client = ChatCompletionClient.load_component(config)
   ```

8. **OpenAI and Azure OpenAI Clients**:
   - `OpenAIChatCompletionClient` and `AzureOpenAIChatCompletionClient` are used for chat completions with OpenAI and Azure models, respectively.

   ```python
   from autogen_ext.models.openai import OpenAIChatCompletionClient
   openai_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
   result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])
   ```

### Installation and Configuration

- Install necessary packages using pip:
  ```bash
  pip install "autogen-ext[openai]"
  pip install "autogen-ext[openai,azure]"
  ```

- Configure clients with necessary parameters like model, API key, and endpoint.

This summary provides an overview of the key functionalities and usage examples for the AutoGen framework components, focusing on chat completion and configuration.

# Model Clients â€” AutoGen

AutoGen provides built-in model clients for using the ChatCompletion API. All model clients implement the `ChatCompletionClient` protocol class. The available clients are:

- **OpenAIChatCompletionClient**
- **AzureOpenAIChatCompletionClient**
- **AzureAIChatCompletionClient**

## OpenAI

To use the `OpenAIChatCompletionClient`, install the `openai` extra:

```bash
# pip install "autogen-ext[openai]"
```

Provide the API key via the `OPENAI_API_KEY` environment variable or the `api_key` argument.

```python
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    # api_key="sk-...", # Optional if set in the environment.
)

messages = [UserMessage(content="What is the capital of France?", source="user")]
response = await model_client.create(messages=messages)
print(response.content)  # Output: The capital of France is Paris.
print(response.usage)    # Output: RequestUsage(prompt_tokens=15, completion_tokens=7)
```

## Azure OpenAI

To use the `AzureOpenAIChatCompletionClient`, provide the deployment ID, Azure endpoint, API version, and model capabilities. Authentication can be via API key or Azure AD token.

```bash
# pip install "autogen-ext[openai,azure]"
```

Example using Azure AD authentication:

```python
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,
    # api_key="sk-...", # For key-based authentication.
)
```

## Azure AI Foundry

Use the `AzureAIChatCompletionClient` for models hosted on Azure AI Foundry.

```bash
# pip install "autogen-ext[openai,azure]"
```

Example with the Phi-4 model:

```python
import os
from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential

client = AzureAIChatCompletionClient(
    model="Phi-4",
    endpoint="https://models.inference.ai.azure.com",
    credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
    model_info={
        "json_output": False,
        "function_calling": False,
        "vision": False,
        "family": "unknown",
    },
)

result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)  # Output: The capital of France is Paris.
```

## Streaming Response

Use the `create_stream()` method for streaming responses.

```python
messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

stream = model_client.create_stream(messages=messages)

print("Streamed responses:")
async for response in stream:
    if isinstance(response, str):
        print(response, flush=True, end="")
    else:
        print("\n\n------------\n")
        print("The complete response:", flush=True)
        print(response.content, flush=True)
        print("\n\n------------\n")
        print("The token usage was:", flush=True)
        print(response.usage, flush=True)
```

To include usage in streaming responses, set `extra_create_args={"stream_options": {"include_usage": True}}`.

The documentation provides a comprehensive guide on using the `autogen_ext` library for handling OpenAI and Azure OpenAI chat completions with caching and structured output. Here's a summary of key functionalities and examples:

### Streamed Responses
- **Partial and Complete Responses**: The code demonstrates how to handle partial and complete responses from a model, printing the response content and token usage.

### Structured Output
- **Pydantic BaseModel**: You can define a structured response format using a Pydantic `BaseModel`. This is supported by `OpenAIChatCompletionClient` and `AzureOpenAIChatCompletionClient`.
- **Example**:
  ```python
  from pydantic import BaseModel
  from typing import Literal

  class AgentResponse(BaseModel):
      thoughts: str
      response: Literal["happy", "sad", "neutral"]
  ```

### Caching Model Responses
- **ChatCompletionCache**: This class wraps a `ChatCompletionClient` to cache responses, reducing token usage for repeated queries.
- **DiskCacheStore and RedisStore**: Implementations for caching using disk or Redis.
- **Example**:
  ```python
  from autogen_ext.models.cache import ChatCompletionCache
  from autogen_ext.cache_store.diskcache import DiskCacheStore
  from diskcache import Cache

  cache_store = DiskCacheStore(Cache(tmpdirname))
  cache_client = ChatCompletionCache(openai_model_client, cache_store)
  ```

### Building an Agent
- **SimpleAgent**: A basic AI agent using the `ChatCompletionClient` to respond to messages.
- **Example**:
  ```python
  class SimpleAgent(RoutedAgent):
      async def handle_user_message(self, message: Message, ctx: MessageContext) -> Message:
          user_message = UserMessage(content=message.content, source="user")
          response = await self._model_client.create([user_message])
          return Message(content=response.content)
  ```

### API Keys
- **Environment Variables**: API keys can be set via environment variables like `OPENAI_API_KEY` and `AZURE_OPENAI_API_KEY` to avoid hardcoding them in your code.

### Usage Examples
- **Caching Example**:
  ```python
  async def main():
      with tempfile.TemporaryDirectory() as tmpdirname:
          openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")
          cache_store = DiskCacheStore(Cache(tmpdirname))
          cache_client = ChatCompletionCache(openai_model_client, cache_store)
          response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
          print(response)
  asyncio.run(main())
  ```

This documentation provides a robust framework for integrating OpenAI's chat models with caching and structured output, enhancing efficiency and organization in handling AI responses.

### Overview

This documentation provides details on various classes and methods used for managing agents, message handling, and runtime operations in an asynchronous environment. Key components include `InMemoryStore`, `CancellationToken`, `AgentInstantiationContext`, `RoutedAgent`, `ClosureAgent`, and `SingleThreadedAgentRuntime`.

### Key Classes and Methods

#### InMemoryStore
- **`get(key: str, default: T | None = None) -> T | None`**: Retrieve an item from the store.
- **`set(key: str, value: T) -> None`**: Store an item in the store.

#### CancellationToken
- **`cancel() -> None`**: Cancel pending async calls.
- **`is_cancelled() -> bool`**: Check if the token has been used.
- **`add_callback(callback: Callable[[], None]) -> None`**: Attach a callback for cancellation.
- **`link_future(future: Future[Any]) -> Future[Any]`**: Link a future to allow cancellation.

#### AgentInstantiationContext
- **`current_runtime() -> AgentRuntime`**: Access the current runtime.
- **`current_agent_id() -> AgentId`**: Access the current agent ID.

#### RoutedAgent
- **`on_message_impl(message: Any, ctx: MessageContext) -> Any | None`**: Route messages to handlers.
- **`on_unhandled_message(message: Any, ctx: MessageContext) -> None`**: Handle unhandled messages.

#### ClosureAgent
- **`register_closure(...) -> AgentType`**: Define an agent using a closure without a class.

#### SingleThreadedAgentRuntime
- **`send_message(message: Any, recipient: AgentId, ...) -> Any`**: Send a message to an agent.
- **`publish_message(message: Any, topic_id: TopicId, ...) -> None`**: Publish a message to all agents.
- **`start() -> None`**: Start the runtime message processing loop.
- **`stop() -> None`**: Stop the runtime immediately.
- **`stop_when_idle() -> None`**: Stop the runtime when no messages are being processed.

### Usage Examples

#### Creating and Using a RoutedAgent
```python
from dataclasses import dataclass
from autogen_core import MessageContext, RoutedAgent, message_handler

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")
```

#### Running a SingleThreadedAgentRuntime
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, AgentId

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()

asyncio.run(main())
```

This documentation provides a comprehensive guide to managing agents and message handling in an asynchronous environment, offering flexibility and control over agent behavior and runtime operations.

### Overview of AutoGen Agents

AutoGen provides a framework for building multi-agent applications with various preset agents. These agents can respond to messages, use tools, and maintain state. Key methods include:

- `on_messages()`: Sends a sequence of `ChatMessage` to the agent and returns a `Response`.
- `on_messages_stream()`: Similar to `on_messages()` but returns an iterator of `AgentEvent` or `ChatMessage`.
- `on_reset()`: Resets the agent to its initial state.
- `run()` and `run_stream()`: Convenience methods for `on_messages()` and `on_messages_stream()`.

### Assistant Agent

The `AssistantAgent` is a built-in agent that uses a language model and can utilize tools. Here's how to create and use an `AssistantAgent`:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Define a tool
async def web_search(query: str) -> str:
    return "AutoGen is a programming framework for building multi-agent applications."

# Create an agent
model_client = OpenAIChatCompletionClient(model="gpt-4o")
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[web_search],
    system_message="Use tools to solve tasks."
)

# Get a response
async def assistant_run() -> None:
    response = await agent.on_messages([
        TextMessage(content="Find information on AutoGen", source="user")
    ])
    print(response.inner_messages)
    print(response.chat_message)

await assistant_run()
```

### Multi-Modal Input

The `AssistantAgent` can handle multi-modal input using `MultiModalMessage`:

```python
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image
import PIL
import requests
from io import BytesIO

# Create a multi-modal message
pil_image = PIL.Image.open(BytesIO(requests.get("https://picsum.photos/300/200").content))
img = Image(pil_image)
multi_modal_message = MultiModalMessage(content=["Can you describe the content of this image?", img], source="user")

response = await agent.on_messages([multi_modal_message])
print(response.chat_message.content)
```

### Streaming Messages

Stream messages as they are generated using `on_messages_stream()`:

```python
from autogen_agentchat.ui import Console

async def assistant_run_stream() -> None:
    await Console(agent.on_messages_stream([
        TextMessage(content="Find information on AutoGen", source="user")
    ]), output_stats=True)

await assistant_run_stream()
```

### Using Tools

The `AssistantAgent` can use tools to perform specific actions. Tools can be Python functions or subclasses of `BaseTool`. Here's an example of defining a function tool:

```python
from autogen_core.tools import FunctionTool

async def web_search_func(query: str) -> str:
    return "AutoGen is a programming framework for building multi-agent applications."

web_search_function_tool = FunctionTool(web_search_func, description="Find information on the web")
```

### Parallel Tool Calls

For models supporting parallel tool calls, `AssistantAgent` can execute multiple tools simultaneously. To disable parallel calls, set `parallel_tool_calls=False` in the model client.

### Structured Output

Structured output allows models to return JSON text with a pre-defined schema using a `Pydantic BaseModel`. This is supported by `OpenAIChatCompletionClient` and `AzureOpenAIChatCompletionClient`.

```python
from pydantic import BaseModel
from typing import Literal

class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]

model_client = OpenAIChatCompletionClient(model="gpt-4o", response_format=AgentResponse)
agent = AssistantAgent("assistant", model_client=model_client, system_message="Categorize the input as happy, sad, or neutral following the JSON format.")
```

### Streaming Tokens

Enable token streaming by setting `model_client_stream=True` in the `AssistantAgent`. This will yield `ModelClientStreamingChunkEvent` messages.

### Using Model Context

The `AssistantAgent` can use different model contexts, such as `BufferedChatCompletionContext`, to limit the context sent to the model. By default, it uses `UnboundedChatCompletionContext`.

The `autogen_agentchat.agents` module provides various pre-defined agents for use in chat applications. Here's a summary of the key components and examples:

### Key Classes and Functions

- **AssistantAgent**: A versatile agent that can use tools to assist with tasks. It supports tool calls, handoffs, and can operate in streaming mode. It maintains state between calls and is not thread-safe.

  ```python
  class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool] | None = None, ...)
  ```

- **CodeExecutorAgent**: Executes code snippets found in messages. It is recommended to use a Docker container for execution to ensure isolation.

  ```python
  class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, description: str = ...)
  ```

- **BaseChatAgent**: An abstract base class for creating chat agents. Subclasses must implement `on_messages()`, `on_reset()`, and `produced_message_types`.

  ```python
  class BaseChatAgent(name: str, description: str)
  ```

### Usage Examples

1. **Basic Assistant Agent**: Create an agent with a model client to respond to simple tasks.

   ```python
   import asyncio
   from autogen_ext.models.openai import OpenAIChatCompletionClient
   from autogen_agentchat.agents import AssistantAgent
   from autogen_agentchat.messages import TextMessage

   async def main():
       model_client = OpenAIChatCompletionClient(model="gpt-4o")
       agent = AssistantAgent(name="assistant", model_client=model_client)
       response = await agent.on_messages([TextMessage(content="What is the capital of France?", source="user")])
       print(response)

   asyncio.run(main())
   ```

2. **Agent with Tools**: Create an agent with a tool to perform specific tasks, like getting the current time.

   ```python
   async def get_current_time() -> str:
       return "The current time is 12:00 PM."

   agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
   ```

3. **Code Executor Agent**: Set up an agent to execute code snippets using Docker.

   ```python
   from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor

   code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
   code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)
   ```

4. **Agent with Memory**: Use a list-based memory to store and recall information.

   ```python
   from autogen_core.memory import ListMemory, MemoryContent

   memory = ListMemory()
   await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
   agent = AssistantAgent(name="assistant", model_client=model_client, memory=[memory])
   ```

5. **Streaming Mode**: Enable streaming mode to receive token streams from the model client.

   ```python
   agent = AssistantAgent(name="assistant", model_client=model_client, model_client_stream=True)
   ```

### Notes

- **State Management**: Agents are stateful and should only receive new messages since the last call.
- **Thread Safety**: The AssistantAgent is not thread-safe and should not be used concurrently.
- **Tool and Handoff Configuration**: Ensure tool and handoff names are unique to avoid conflicts.

This module provides a flexible framework for building chat agents with various capabilities, including tool usage, code execution, and memory management.

The `autogen_agentchat.agents` module provides various pre-defined agents for chat systems, with `BaseChatAgent` as the base class. Here's a summary of key components and examples:

### Key Classes and Methods

- **AssistantAgent**: An agent that provides assistance with tool use. It can handle messages, execute tool calls, and manage handoffs. It supports streaming mode and can be configured with tools, handoffs, and memory.

  ```python
  class AssistantAgent(name: str, model_client: ChatCompletionClient, ...)
  ```

- **SocietyOfMindAgent**: Uses an inner team of agents to generate responses. It can be configured with a description, instruction, and response prompt.

  ```python
  class SocietyOfMindAgent(name: str, team: RoundRobinGroupChat, ...)
  ```

- **UserProxyAgent**: Represents a human user through an input function. It can handle user input and manage state.

  ```python
  class UserProxyAgent(name: str, description: str = 'A human user', ...)
  ```

- **BaseChatAgent**: Abstract base class for chat agents, providing foundational methods for handling messages and managing state.

### Examples

1. **Basic Assistant Agent**: Create an agent with a model client to respond to simple tasks.

   ```python
   import asyncio
   from autogen_ext.models.openai import OpenAIChatCompletionClient
   from autogen_agentchat.agents import AssistantAgent
   from autogen_agentchat.messages import TextMessage
   from autogen_core import CancellationToken

   async def main() -> None:
       model_client = OpenAIChatCompletionClient(model="gpt-4o")
       agent = AssistantAgent(name="assistant", model_client=model_client)
       response = await agent.on_messages([TextMessage(content="What is the capital of France?", source="user")], CancellationToken())
       print(response)

   asyncio.run(main())
   ```

2. **Agent with Tools**: Demonstrates using an agent with a tool to perform tasks.

   ```python
   async def get_current_time() -> str:
       return "The current time is 12:00 PM."

   agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
   ```

3. **User Proxy Agent**: Represents a human user in a chat system.

   ```python
   from autogen_agentchat.agents import UserProxyAgent
   agent = UserProxyAgent("user_proxy")
   ```

4. **Society of Mind Agent**: Uses a team of agents to generate responses.

   ```python
   from autogen_agentchat.teams import RoundRobinGroupChat
   from autogen_agentchat.conditions import TextMentionTermination

   inner_termination = TextMentionTermination("APPROVE")
   inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)
   society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)
   ```

### Configuration and State Management

- **_from_config()** and **_to_config()**: Methods to create and dump configuration for agents.
- **load_state()** and **save_state()**: Methods to manage agent state.
- **on_messages()** and **on_messages_stream()**: Handle incoming messages and return responses or streams.

### Notes

- Agents are stateful; they maintain state between calls.
- The `o1-mini` model does not support system messages or function calling.
- Use `CancellationToken` to manage asynchronous operations and timeouts.

This module is designed to facilitate the creation and management of chat agents with various capabilities, including tool use, user representation, and team-based response generation.

To implement a custom agent in the `AgentChat` framework, you need to define the following methods and properties:

### Required Methods and Properties

1. **`on_messages()`**: This method handles incoming messages and returns a response. It should only process new messages since the last call and maintain the agent's state between calls.

   ```python
   async def on_messages(
       self,
       messages: Sequence[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]],
       cancellation_token: CancellationToken
   ) -> Response:
       # Implementation here
   ```

2. **`on_reset()`**: This method resets the agent to its initial state. It is crucial for stateful agents to clear any stored state.

   ```python
   async def on_reset(self, cancellation_token: CancellationToken) -> None:
       # Implementation here
   ```

3. **`produced_message_types`**: This property should return the types of messages the agent can produce.

   ```python
   @property
   def produced_message_types(self) -> Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]:
       # Implementation here
   ```

### Optional Method

- **`on_messages_stream()`**: If your agent needs to stream messages, implement this method. It should handle incoming messages and return a stream of messages, with the final item being the response.

  ```python
  async def on_messages_stream(
      self,
      messages: Sequence[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]],
      cancellation_token: CancellationToken
  ) -> AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None]:
      # Implementation here
  ```

### Example: CountDownAgent

Here's a simple example of a custom agent that counts down from a given number:

```python
from typing import AsyncGenerator, List, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage
from autogen_core import CancellationToken

class CountDownAgent(BaseChatAgent):
    def __init__(self, name: str, count: int = 3):
        super().__init__(name, "A simple agent that counts down.")
        self._count = count

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        response: Response | None = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                response = message
        assert response is not None
        return response

    async def on_messages_stream(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:
        inner_messages: List[AgentEvent | ChatMessage] = []
        for i in range(self._count, 0, -1):
            msg = TextMessage(content=f"{i}...", source=self.name)
            inner_messages.append(msg)
            yield msg
        yield Response(chat_message=TextMessage(content="Done!", source=self.name), inner_messages=inner_messages)

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass
```

This example demonstrates how to implement the required methods and properties for a custom agent that counts down from a specified number. Adjust the logic in `on_messages` and `on_messages_stream` to fit your specific use case.

The provided code and documentation describe the implementation of a custom agent using the Google Gemini SDK and its integration into a multi-agent chat system. Here's a concise summary of the key components and usage examples:

### Custom Agent Implementation

The `GeminiAssistantAgent` class is a custom chat agent that uses the Google Gemini SDK to generate responses based on conversation history. It inherits from `BaseChatAgent` and `Component`, allowing it to be serialized and deserialized for configuration management.

#### Key Methods:
- **`on_messages`**: Processes incoming messages and returns a final response.
- **`on_messages_stream`**: An async generator that yields responses as they are generated.
- **`on_reset`**: Clears the model context to reset the agent.

#### Configuration:
- The agent can be configured using the `GeminiAssistantAgentConfig` class, which includes parameters like `name`, `description`, `model`, and `system_message`.

### Usage Example

```python
gemini_assistant = GeminiAssistantAgent("gemini_assistant")
await Console(gemini_assistant.run_stream(task="What is the capital of New York?"))
```

### Multi-Agent Chat System

The custom agent can be integrated into a multi-agent chat system using the `RoundRobinGroupChat` class, which manages a group of agents taking turns to respond to messages.

#### Example Setup:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o")
agent1 = AssistantAgent("Assistant1", model_client=model_client)
agent2 = AssistantAgent("Assistant2", model_client=model_client)

team = RoundRobinGroupChat([agent1, agent2], termination_condition=MaxMessageTermination(3))

await Console(team.run_stream(task="Count from 1 to 10, respond one at a time."))
```

### Declarative Configuration

The `GeminiAssistantAgent` can be serialized to a JSON format using the `dump_component` method and deserialized using the `load_component` method, facilitating easy configuration management and sharing.

### Next Steps

- Extend the Gemini model client to handle function calling.
- Implement a package with a custom agent and experiment with its declarative format in tools like AutoGen Studio.

This setup allows for flexible and scalable multi-agent interactions, leveraging the capabilities of the Google Gemini SDK and the AutoGen framework.

The documentation provides detailed information on setting up and using various components of the `autogen_agentchat` library, focusing on creating and managing chat agents and teams. Below are key highlights and examples from the documentation:

### Key Parameters and Functions

- **allow_repeated_speaker**: Determines if the previous speaker can be selected again. Defaults to `False`.
- **max_selector_attempts**: Sets the maximum attempts to select a speaker. Defaults to 3.
- **selector_func**: A custom function to select the next speaker based on conversation history.

### Error Handling

- **ValueError**: Raised if the number of participants is less than two or if the selector prompt is invalid.

### Examples

#### Team with Multiple Participants

```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def lookup_hotel(location: str) -> str:
        return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."

    async def lookup_flight(origin: str, destination: str) -> str:
        return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."

    async def book_trip() -> str:
        return "Your trip is booked!"

    travel_advisor = AssistantAgent("Travel_Advisor", model_client, tools=[book_trip], description="Helps with travel planning.")
    hotel_agent = AssistantAgent("Hotel_Agent", model_client, tools=[lookup_hotel], description="Helps with hotel booking.")
    flight_agent = AssistantAgent("Flight_Agent", model_client, tools=[lookup_flight], description="Helps with flight booking.")

    termination = TextMentionTermination("TERMINATE")
    team = SelectorGroupChat([travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination)

    await Console(team.run_stream(task="Book a 3-day trip to new york."))

asyncio.run(main())
```

#### Team with Custom Selector Function

```python
import asyncio
from typing import Sequence
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import AgentEvent, ChatMessage

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    def check_calculation(x: int, y: int, answer: int) -> str:
        return "Correct!" if x + y == answer else "Incorrect!"

    agent1 = AssistantAgent("Agent1", model_client, description="For calculation", system_message="Calculate the sum of two numbers")
    agent2 = AssistantAgent("Agent2", model_client, tools=[check_calculation], description="For checking calculation", system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'")

    def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
        if len(messages) == 1 or messages[-1].content == "Incorrect!":
            return "Agent1"
        if messages[-1].source == "Agent1":
            return "Agent2"
        return None

    termination = TextMentionTermination("Correct!")
    team = SelectorGroupChat([agent1, agent2], model_client=model_client, selector_func=selector_func, termination_condition=termination)

    await Console(team.run_stream(task="What is 1 + 1?"))

asyncio.run(main())
```

### Class Definitions

- **Swarm**: A group chat team that selects the next speaker based on a handoff message.
- **AssistantAgent**: An agent that provides assistance with tool use, capable of handling messages and generating responses.

### Usage Notes

- **State Management**: Agents are stateful and should maintain their state between calls. Only new messages should be passed to the agent on each call.
- **Streaming Mode**: Agents can operate in streaming mode, yielding messages as they are generated.

This documentation provides a comprehensive guide to setting up and using chat agents and teams, with examples demonstrating various configurations and functionalities.

The documentation provides an overview of various agent classes and methods used in a chat system, focusing on handling messages, executing tasks, and managing agent states. Below are key components and their functionalities:

### Key Classes and Methods

#### `BaseChatAgent`
- **`on_pause(cancellation_token: CancellationToken) -> None`**: Called when the agent is paused. Subclasses can override this to implement custom behavior.
- **`on_reset(cancellation_token: CancellationToken) -> None`**: Resets the agent to its initial state.
- **`on_resume(cancellation_token: CancellationToken) -> None`**: Called when the agent is resumed from a pause.
- **`produced_message_types`**: Specifies the types of messages the agent can produce.

#### `CodeExecutorAgent`
- Executes code snippets found in messages using a `CodeExecutor`.
- **Parameters**:
  - `name`: Name of the agent.
  - `code_executor`: Responsible for executing code (e.g., `DockerCommandLineCodeExecutor`).
  - `description`: Description of the agent.
  - `sources`: Specifies which agents' messages to check for code execution.
- **Example Usage**:
  ```python
  import asyncio
  from autogen_agentchat.agents import CodeExecutorAgent
  from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
  from autogen_core import CancellationToken

  async def run_code_executor_agent():
      code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
      await code_executor.start()
      code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)
      task = TextMessage(content='''Here is some code\n```python\nprint('Hello world')\n```''', source="user")
      response = await code_executor_agent.on_messages([task], CancellationToken())
      print(response.chat_message)
      await code_executor.stop()

  asyncio.run(run_code_executor_agent())
  ```

#### `SocietyOfMindAgent`
- Utilizes an inner team of agents to generate responses.
- **Parameters**:
  - `name`: Name of the agent.
  - `team`: Team of agents to use.
  - `model_client`: Model client for preparing responses.
  - `description`, `instruction`, `response_prompt`: Optional parameters for customizing agent behavior.
- **Example Usage**:
  ```python
  import asyncio
  from autogen_agentchat.agents import SocietyOfMindAgent, AssistantAgent
  from autogen_ext.models.openai import OpenAIChatCompletionClient
  from autogen_agentchat.teams import RoundRobinGroupChat
  from autogen_agentchat.conditions import TextMentionTermination

  async def main():
      model_client = OpenAIChatCompletionClient(model="gpt-4o")
      agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
      agent2 = AssistantAgent("assistant2", model_client=model_client, system_message="You are an editor, provide critical feedback.")
      inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=TextMentionTermination("APPROVE"))
      society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)
      stream = inner_team.run_stream(task="Write a short story with a surprising ending.")
      await Console(stream)

  asyncio.run(main())
  ```

#### `UserProxyAgent`
- Represents a human user through an input function.
- **Parameters**:
  - `name`: Name of the agent.
  - `description`: Description of the agent.
  - `input_func`: Function to get user input.
- **Example Usage**:
  ```python
  import asyncio
  from autogen_agentchat.agents import UserProxyAgent
  from autogen_agentchat.messages import TextMessage

  async def simple_user_agent():
      agent = UserProxyAgent("user_proxy")
      response = await agent.on_messages([TextMessage(content="What is your name?", source="user")], CancellationToken())
      print(f"Your name is {response.chat_message.content}")

  asyncio.run(simple_user_agent())
  ```

### General Notes
- Agents are stateful and should maintain their state between calls to `on_messages()`.
- The `CodeExecutorAgent` is recommended to use Docker for executing code in an isolated environment.
- The `SocietyOfMindAgent` uses a team of agents to collaboratively generate responses.
- The `UserProxyAgent` can block a running team until user input is received, so it should handle timeouts and exceptions appropriately.

The provided documentation outlines the usage of various classes and functions within the `autogen_agentchat` library, focusing on creating and managing a team of agents for collaborative tasks. Here's a summary of the key components and their functionalities:

### Key Classes and Functions

- **Team**: Inherits from `ABC`, `TaskRunner`, and `ComponentBase`. It represents a group of agents working together. Key methods include:
  - `load_state(state: Mapping[str, Any])`: Load the team's state.
  - `pause()`: Pause the team and its participants.
  - `reset()`: Reset the team to its initial state.
  - `resume()`: Resume the team from a pause.
  - `save_state()`: Save the current state of the team.

- **TerminationCondition**: Determines when a conversation should be terminated. It can be combined using AND/OR operators. Key methods include:
  - `reset()`: Reset the termination condition.
  - `terminated`: Check if the termination condition has been reached.

- **ChatAgent**: Protocol for a chat agent. Key methods include:
  - `close()`: Release resources held by the agent.
  - `load_state(state: Mapping[str, Any])`: Restore agent from saved state.
  - `on_messages(messages: Sequence[...], cancellation_token: CancellationToken)`: Handle incoming messages and return a response.
  - `on_messages_stream(messages: Sequence[...], cancellation_token: CancellationToken)`: Handle incoming messages and return a stream of messages.

- **TaskRunner**: A task runner that can execute tasks and return results. Key methods include:
  - `run(task: str | ... | None, cancellation_token: CancellationToken | None)`: Run the task and return the result.
  - `run_stream(task: str | ... | None, cancellation_token: CancellationToken | None)`: Run the task and produce a stream of messages and the final result.

### Example Usage

The documentation provides an example of using `SelectorGroupChat` to implement a team where participants take turns broadcasting messages. The example involves:

- **Agents**: Specialized agents like `PlanningAgent`, `WebSearchAgent`, and `DataAnalystAgent` are created using the `AssistantAgent` class.
- **Termination Conditions**: Use `TextMentionTermination` and `MaxMessageTermination` to control when the conversation should end.
- **Selector Prompt**: A custom prompt is used to guide the model in selecting the next speaker based on the conversation context.
- **Running the Team**: The team is run with a task to find information about an NBA player, demonstrating the collaborative workflow among agents.

### Custom Selector Function

A custom selector function can be implemented to override the default model-based selection, allowing for more complex selection logic. This function can be used to ensure specific agents, like the `PlanningAgent`, speak immediately after specialized agents to check progress.

```python
def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
    if messages[-1].source != planning_agent.name:
        return planning_agent.name
    return None
```

This function is used to reset the team and run the chat again with the custom selection logic.

Overall, the documentation provides a comprehensive guide to setting up and managing a team of agents using the `autogen_agentchat` library, with detailed examples and explanations of key components and their interactions.

To resume a GroupChat, you can use the `resume` function of the `GroupChatManager`. This function takes the messages from a previous conversation and prepares the GroupChat, GroupChatManager, and agents for resuming. Here's a brief guide on how to do it:

1. **Prepare Messages**: Use the `messages_to_string` function to convert the chat history or messages property into a JSON string.

   ```python
   messages_json = mygroupchatmanager.messages_to_string(previous_chat_result.chat_history)
   ```

2. **Resume Function**: Call the `resume` function with the JSON string. This will return the last agent and message.

   ```python
   last_agent, last_message = mygroupchatmanager.resume(messages_json)
   ```

3. **Initiate Chat**: Use the `initiate_chat` function to continue the conversation.

   ```python
   last_agent.initiate_chat(last_message)
   ```

4. **Handling Termination**: If the previous chat terminated, ensure the termination condition is handled. You can use the `remove_termination_string` parameter if needed.

5. **Example JSON**: Here's an example of a JSON string for resuming:

   ```json
   [
     {"content": "Find the latest paper about gpt-4 on arxiv and find its potential applications in software.", "role": "user", "name": "Admin"},
     {"content": "Plan:\n1. **Engineer**: Search for the latest paper on GPT-4 on arXiv.\n2. **Scientist**: Read the paper and summarize the key findings and potential applications of GPT-4.\n3. **Engineer**: Identify potential software applications where GPT-4 can be utilized based on the scientist's summary.\n4. **Scientist**: Provide insights on the feasibility and impact of implementing GPT-4 in the identified software applications.\n5. **Engineer**: Develop a prototype or proof of concept to demonstrate how GPT-4 can be integrated into the selected software application.\n6. **Scientist**: Evaluate the prototype, provide feedback, and suggest any improvements or modifications.\n7. **Engineer**: Make necessary revisions based on the scientist's feedback and finalize the integration of GPT-4 into the software application.\n8. **Admin**: Review the final software application with GPT-4 integration and approve for further development or implementation.\n\nFeedback from admin and critic is needed for further refinement of the plan.", "role": "user", "name": "Planner"},
     {"content": "Agree", "role": "user", "name": "Admin"},
     {"content": "Great! Let's proceed with the plan outlined earlier. I will start by searching for the latest paper on GPT-4 on arXiv. Once I find the paper, the scientist will summarize the key findings and potential applications of GPT-4. We will then proceed with the rest of the steps as outlined. I will keep you updated on our progress.", "role": "user", "name": "Planner"}
   ]
   ```

By following these steps, you can effectively resume a GroupChat and continue the conversation from where it left off.

The provided documentation outlines how to create and manage a group chat using the `autogen` library, focusing on creating agents, managing group chats, and resuming conversations. Here's a concise summary of the key components and functionalities:

### Creating Agents
- **Planner**: Suggests and revises plans based on feedback.
- **UserProxyAgent (Admin)**: Interacts with the planner and approves plans.
- **Engineer**: Writes and executes code based on approved plans.
- **Scientist**: Summarizes papers and provides insights without writing code.
- **Executor**: Executes code and reports results.

### GroupChat and GroupChatManager
- **GroupChat**: Manages the conversation between agents, with a specified maximum number of rounds.
- **GroupChatManager**: Handles the resumption of group chats and manages the flow of conversation.

### Resuming Group Chats
- **Resume Functionality**: Allows continuation of a chat from a previous state using the `resume` method.
- **Termination Handling**: Manages chat termination using a termination string (e.g., "TERMINATE") and can resume by removing this string if needed.

### Example Usage
- **Code Execution**: The Engineer agent writes a Python script to search for papers on arXiv, which is executed by the Executor.
- **Plan Execution**: The Scientist summarizes findings, and the Engineer identifies applications, leading to admin approval.

### Customizing Speaker Selection
- **StateFlow Model**: Defines a workflow with states (Init, Retrieve, Research, End) and transitions between them.
- **Custom Speaker Selection Function**: Determines the next speaker based on the last speaker and the current state of the group chat.

### Example Code Snippets
```python
# Example of creating an Engineer agent
engineer = autogen.AssistantAgent(
    name="Engineer",
    llm_config=gpt4_config,
    system_message="""Engineer. You follow an approved plan..."""
)

# Example of resuming a group chat
last_agent, last_message = manager.resume(messages=previous_state)
result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)

# Example of custom speaker selection function
def custom_speaker_selection_func(last_speaker, groupchat):
    # Define transitions based on the last speaker
    pass
```

This setup allows for structured, automated conversations between agents, facilitating complex workflows such as research, planning, and execution in a collaborative environment.

Here is a summary of recent papers related to "LLM applications" from the last week, categorized by domain:

| Domain | Title | Authors | Summary | Link |
|--------|-------|---------|---------|------|
| Security | PRSA: Prompt Reverse Stealing Attacks against Large Language Models | Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang | Explores security risks of exposing input-output pairs in LLMs and proposes PRSA, a framework to reverse-steal prompts, threatening intellectual property rights. | [Link](http://arxiv.org/abs/2402.19200v1) |
| Ethics & Evaluation | Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models | Paul RÃ¶ttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich SchÃ¼tze, Dirk Hovy | Challenges current evaluation paradigms for LLMs' values and opinions, proposing more realistic evaluations. | [Link](http://arxiv.org/abs/2402.16786v1) |
| Urban Mobility | Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation | Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao | Introduces an LLM agent framework for personal mobility, aligning LLMs with urban mobility data. | [Link](http://arxiv.org/abs/2402.14744v1) |
| Bioinformatics | An Evaluation of Large Language Models in Bioinformatics Research | Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun | Evaluates LLMs on bioinformatics tasks, highlighting potential and limitations. | [Link](http://arxiv.org/abs/2402.13714v1) |
| Privacy | Privacy-Preserving Instructions for Aligning Large Language Models | Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu | Proposes using synthetic instructions for privacy in LLM data annotation and fine-tuning. | [Link](http://arxiv.org/abs/2402.13659v1) |
| Social Robotics | Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru | Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez | Integrates LLMs into social robots for dynamic and expressive conversations. | [Link](http://arxiv.org/abs/2402.11571v1) |
| Ophthalmology | Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 | Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting | Assesses GPT-4-based evaluation for ophthalmology-related queries. | [Link](http://arxiv.org/abs/2402.10083v1) |
| Privacy & Data Security | Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan VuliÄ‡ | Introduces a method for LLM unlearning to protect sensitive data while maintaining capabilities. | [Link](http://arxiv.org/abs/2402.10052v1) |
| Computational Efficiency | Anchor-based Large Language Models | Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang | Proposes AnLLMs with an anchor-based self-attention network to enhance efficiency. | [Link](http://arxiv.org/abs/2402.07616v2) |
| Enterprise Applications | T-RAG: Lessons from the LLM Trenches | Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla | Shares experiences in deploying LLM applications for enterprise document question answering. | [Link](http://arxiv.org/abs/2402.07483v1) |

These papers cover a range of domains, showcasing the diverse applications of large language models.

The document provides a comprehensive guide on using AutoGen for managing conversations between multiple agents, focusing on different conversation patterns such as two-agent chat, sequential chat, group chat, and nested chat. Here's a summary of the key points and examples:

### Two-Agent Chat
- **Basic Pattern**: Involves two agents communicating. Initiated using the `initiate_chat` method.
- **Example**: A student agent asks a teacher agent about the triangle inequality theorem.
```python
chat_result = student_agent.initiate_chat(
    teacher_agent,
    message="What is triangle inequality?",
    summary_method="reflection_with_llm",
    max_turns=2,
)
```

### Sequential Chat
- **Pattern**: A series of two-agent chats where each chat's summary is carried over to the next.
- **Example**: Arithmetic operations performed sequentially by different agents.
```python
chat_results = number_agent.initiate_chats([
    {"recipient": adder_agent, "message": "14", "max_turns": 2, "summary_method": "last_msg"},
    {"recipient": multiplier_agent, "message": "These are my numbers", "max_turns": 2, "summary_method": "last_msg"},
    # Additional chats...
])
```

### Group Chat
- **Pattern**: Involves more than two agents in a single conversation thread.
- **Strategies**: `round_robin`, `random`, `manual`, `auto`.
- **Example**: Arithmetic agents collaboratively transform a number.
```python
group_chat = GroupChat(
    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],
    max_round=6,
)
group_chat_manager = GroupChatManager(groupchat=group_chat, llm_config={"config_list": [{"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}]})
chat_result = number_agent.initiate_chat(group_chat_manager, message="My number is 3, I want to turn it into 13.")
```

### Nested Chat
- **Pattern**: Packages a workflow into a single agent for reuse.
- **Example**: An arithmetic agent uses nested chats to perform operations, validate with code, and generate a poem.
```python
arithmetic_agent.register_nested_chats(
    nested_chats,
    trigger=lambda sender: sender not in [group_chat_manager_with_intros, code_writer_agent, poetry_agent],
)
reply = arithmetic_agent.generate_reply(messages=[{"role": "user", "content": "I have a number 3 and I want to turn it into 7."}])
```

### Additional Features
- **Constrained Speaker Selection**: Control which agent speaks next using allowed or disallowed transitions.
- **Custom Speaker Role**: Adjust the role of the select speaker message to accommodate API requirements.

These patterns and examples illustrate how AutoGen can be used to manage complex interactions between multiple agents, providing flexibility and control over conversation dynamics.

### Python Script for Arithmetic Operations

The following Python script demonstrates a series of arithmetic operations, verifying each step with assertions:

```python
# defining the initial value
initial_number = 3
# Adding 1 to initial number
initial_number += 1
assert initial_number == 4, "The first operation failed!"
# Multiplying the result by 2
initial_number *= 2
assert initial_number == 8, "The second operation failed!"
# Subtracting 1 from the result
initial_number -= 1
assert initial_number == 7, "The final operation failed!"
print("All operations were carried out successfully!")
```

This script performs the following operations:
1. Adds 1 to the initial number (3), resulting in 4.
2. Multiplies the result by 2, resulting in 8.
3. Subtracts 1, resulting in 7.

The `assert` function checks the result at each step, raising an `AssertionError` if the expected result is not achieved. If all operations are successful, a confirmation message is printed.

### AutoGen AgentChat Overview

The `autogen_agentchat.agents` module provides various pre-defined agents, including the `AssistantAgent`, which assists with tasks using tools. The `AssistantAgent` can handle messages, execute tool calls, and manage state between calls. It supports both synchronous and streaming modes.

#### Key Features:
- **Tool Call Behavior**: Executes tool calls and returns results. Can reflect on tool use for additional processing.
- **Handoff Behavior**: Transfers tasks to other agents when necessary.
- **Context Management**: Limits context size to manage token usage effectively.
- **Streaming Mode**: Supports streaming responses for real-time interaction.

#### Example Usage:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken

async def main():
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(name="assistant", model_client=model_client)
    response = await agent.on_messages([TextMessage(content="What is the capital of France?", source="user")], CancellationToken())
    print(response)

asyncio.run(main())
```

This example demonstrates creating an `AssistantAgent` to answer a simple question using a model client.

### CodeExecutorAgent

The `CodeExecutorAgent` extracts and executes code snippets from messages. It is typically used with another agent that generates code snippets.

#### Example Setup:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken

async def run_code_executor_agent():
    code_executor = DockerCommandLineCodeExecutor(work_dir="/path/to/work_dir")
    agent = CodeExecutorAgent(name="code_executor", code_executor=code_executor)
    # Further implementation...

asyncio.run(run_code_executor_agent())
```

This setup uses a Docker container to execute code snippets, ensuring isolation and security.

To execute code within a Docker container using a `DockerCommandLineCodeExecutor`, you can follow these steps:

1. **Initialize the Code Executor:**
   ```python
   code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
   ```

2. **Start the Code Executor:**
   ```python
   await code_executor.start()
   ```

3. **Create a Code Executor Agent:**
   ```python
   code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)
   ```

4. **Run the Agent with a Code Snippet:**
   ```python
   task = TextMessage(
       content='''Here is some code
       ```python
       print('Hello world')
       ```
       ''',
       source="user",
   )

   response = await code_executor_agent.on_messages([task], CancellationToken())
   print(response.chat_message)
   ```

5. **Stop the Code Executor:**
   ```python
   await code_executor.stop()
   ```

6. **Run the Code Executor Agent:**
   ```python
   asyncio.run(run_code_executor_agent())
   ```

### Class Methods and Properties

- **`_from_config(config: CodeExecutorAgentConfig) -> Self`**: Create a new instance from a configuration object.
- **`_to_config() -> CodeExecutorAgentConfig`**: Dump the configuration for creating a new instance.
- **`component_config_schema`**: Alias of `CodeExecutorAgentConfig`.
- **`component_provider_override`**: Override the provider string for the component.
- **`on_messages(messages: Sequence[...], cancellation_token: CancellationToken) -> Response`**: Handles incoming messages and returns a response.
- **`on_reset(cancellation_token: CancellationToken) -> None`**: Resets the agent state.

### Example Usage

```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
    agent2 = AssistantAgent("assistant2", model_client=model_client, system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.")
    inner_termination = TextMentionTermination("APPROVE")
    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)
    society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)
    agent3 = AssistantAgent("assistant3", model_client=model_client, system_message="Translate the text to Spanish.")
    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)
    stream = team.run_stream(task="Write a short story with a surprising ending.")
    await Console(stream)

asyncio.run(main())
```

This setup allows you to execute Python code snippets within a Docker container, manage agents, and handle messages asynchronously.

To facilitate agent introductions in a group chat, set `send_introductions=True` in the `GroupChat` configuration. This ensures that each agent is aware of the others before the chat begins.

```python
group_chat_with_introductions = GroupChat(
    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],
    messages=[],
    max_round=6,
    send_introductions=True,
)
```

In a sequential chat, the `GroupChatManager` can be used as a regular agent. Here's how to initiate a sequence of two-agent chats:

```python
group_chat_manager_with_intros = GroupChatManager(
    groupchat=group_chat_with_introductions,
    llm_config={
        "config_list": [
            {"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}
        ]
    },
)

chat_result = number_agent.initiate_chats([
    {"recipient": group_chat_manager_with_intros, "message": "My number is 3, I want to turn it into 13."},
    {"recipient": group_chat_manager_with_intros, "message": "Turn this number to 32."},
])
```

To manage speaker transitions in a group chat, use the `allowed_or_disallowed_speaker_transitions` argument:

```python
allowed_transitions = {
    number_agent: [adder_agent, number_agent],
    adder_agent: [multiplier_agent, number_agent],
    subtracter_agent: [divider_agent, number_agent],
    multiplier_agent: [subtracter_agent, number_agent],
    divider_agent: [adder_agent, number_agent],
}

constrained_graph_chat = GroupChat(
    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],
    allowed_or_disallowed_speaker_transitions=allowed_transitions,
    speaker_transitions_type="allowed",
    messages=[],
    max_round=12,
    send_introductions=True,
)
```

For nested chats, use the `register_nested_chats` method to define conditions and sequences:

```python
nested_chats = [
    {"recipient": group_chat_manager_with_intros, "summary_method": "reflection_with_llm"},
    {"recipient": code_writer_agent, "message": "Write a Python script to verify the arithmetic operations is correct.", "summary_method": "reflection_with_llm"},
    {"recipient": poetry_agent, "message": "Write a poem about it.", "max_turns": 1, "summary_method": "last_msg"},
]

arithmetic_agent.register_nested_chats(
    nested_chats,
    trigger=lambda sender: sender not in [group_chat_manager_with_intros, code_writer_agent, poetry_agent],
)

reply = arithmetic_agent.generate_reply(
    messages=[{"role": "user", "content": "I have a number 3 and I want to turn it into 7."}]
)
```

This setup allows for complex workflows by combining group chats, sequential chats, and nested chats, enabling agents to perform tasks collaboratively and efficiently.

# AutoGen Core Model Context

## Classes and Methods

### `BufferedChatCompletionContext`

A buffered chat completion context that maintains a view of the last `n` messages, where `n` is the buffer size set during initialization.

```python
class BufferedChatCompletionContext(buffer_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)
```

- **Parameters:**
  - `buffer_size (int)`: Size of the buffer.
  - `initial_messages (List[LLMMessage] | None)`: Initial messages.

- **Methods:**
  - `classmethod _from_config(config: BufferedChatCompletionContextConfig) -> Self`: Create an instance from a configuration object.
  - `_to_config() -> BufferedChatCompletionContextConfig`: Dump the configuration for creating a new instance.
  - `async get_messages() -> List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]`: Get recent messages up to `buffer_size`.

### `ChatCompletionContext`

An abstract base class for chat completion contexts, allowing agents to store and retrieve LLM messages with different recall strategies.

```python
class ChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)
```

- **Parameters:**
  - `initial_messages (List[LLMMessage] | None)`: Initial messages.

- **Methods:**
  - `async add_message(message: Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) -> None`: Add a message to the context.
  - `async clear() -> None`: Clear the context.
  - `abstract async get_messages() -> List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]`: Abstract method to get messages.
  - `async load_state(state: Mapping[str, Any]) -> None`: Load state.
  - `async save_state() -> Mapping[str, Any]`: Save state.

### `HeadAndTailChatCompletionContext`

A context that keeps a view of the first `n` and last `m` messages.

```python
class HeadAndTailChatCompletionContext(head_size: int, tail_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)
```

- **Parameters:**
  - `head_size (int)`: Size of the head.
  - `tail_size (int)`: Size of the tail.
  - `initial_messages (List[LLMMessage] | None)`: Initial messages.

### `UnboundedChatCompletionContext`

An unbounded context that keeps a view of all messages.

```python
class UnboundedChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)
```

- **Methods:**
  - `async get_messages() -> List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]`: Get all messages.

## Termination

### Termination Conditions

Termination conditions determine when to stop a run. They are stateful and reset automatically after each run.

- **Built-In Conditions:**
  - `MaxMessageTermination`: Stops after a specified number of messages.
  - `TextMentionTermination`: Stops when specific text is mentioned.
  - `TokenUsageTermination`: Stops based on token usage.
  - `TimeoutTermination`: Stops after a specified duration.
  - `HandoffTermination`: Stops on a handoff request.
  - `SourceMatchTermination`: Stops after a specific agent responds.
  - `ExternalTermination`: Allows external control of termination.
  - `StopMessageTermination`: Stops when a `StopMessage` is produced.
  - `TextMessageTermination`: Stops when a `TextMessage` is produced.
  - `FunctionCallTermination`: Stops when a `ToolCallExecutionEvent` with a matching name is produced.

### Usage Example

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from aut

_client=model_client,
)
```

## Multi-Modal Agent

In `v0.2`, you create a multi-modal agent as follows:

```python
from autogen.agentchat import MultiModalAgent

llm_config = {
    "config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
    "seed": 42,
    "temperature": 0,
}

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal assistant.",
    llm_config=llm_config,
)
```

In `v0.4`, it is similar, but you need to specify `model_client` instead of `llm_config`.

```python
from autogen_agentchat.agents import MultiModalAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx", seed=42, temperature=0)

multi_modal_agent = MultiModalAgent(
    name="multi_modal_agent",
    system_message="You are a multi-modal assistant.",
    model_client=model_client,
)
```

## User Proxy

In `v0.2`, you create a user proxy as follows:

```python
from autogen.agentchat import UserProxy

user_proxy = UserProxy(name="user_proxy")
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.agents import UserProxy

user_proxy = UserProxy(name="user_proxy")
```

## Conversable Agent and Register Reply

In `v0.2`, you create a conversable agent and register a reply as follows:

```python
from autogen.agentchat import ConversableAgent

conversable_agent = ConversableAgent(name="conversable_agent")

@conversable_agent.register_reply
def reply(message):
    return "Hello, I am a conversable agent."
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.agents import ConversableAgent

conversable_agent = ConversableAgent(name="conversable_agent")

@conversable_agent.register_reply
def reply(message):
    return "Hello, I am a conversable agent."
```

## Save and Load Agent State

In `v0.2`, you save and load an agent's state as follows:

```python
agent.save_state("agent_state.json")
agent.load_state("agent_state.json")
```

In `v0.4`, it is similar:

```python
agent.save_state("agent_state.json")
agent.load_state("agent_state.json")
```

## Two-Agent Chat

In `v0.2`, you create a two-agent chat as follows:

```python
from autogen.agentchat import TwoAgentChat

two_agent_chat = TwoAgentChat(agent1, agent2)
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.teams import TwoAgentChat

two_agent_chat = TwoAgentChat(agent1, agent2)
```

## Tool Use

In `v0.2`, you register a tool as follows:

```python
from autogen.agentchat import Tool

tool = Tool(name="tool", function=some_function)
agent.register_tool(tool)
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.tools import Tool

tool = Tool(name="tool", function=some_function)
agent.register_tool(tool)
```

## Chat Result

In `v0.2`, you get the chat result as follows:

```python
result = chat.run()
```

In `v0.4`, it is similar:

```python
result = await chat.run()
```

## Conversion between v0.2 and v0.4 Messages

In `v0.2`, you convert messages as follows:

```python
from autogen.agentchat import convert_v0_2_to_v0_4

v0_4_messages = convert_v0_2_to_v0_4(v0_2_messages)
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.utils import convert_v0_2_to_v0_4

v0_4_messages = convert_v0_2_to_v0_4(v0_2_messages)
```

## Group Chat

In `v0.2`, you create a group chat as follows:

```python
from autogen.agentchat import GroupChat

group_chat = GroupChat([agent1, agent2, agent3])
```

In `v0.4`, it is similar:

```python
from autogen_agentchat.teams import GroupChat

group_chat = GroupChat([agent1, agent2, agent3])

In `v0.4`, the usage of the `AssistantAgent` has changed. Instead of using `assistant.send`, you now use `assistant.on_messages` or `assistant.on_messages_stream` for handling incoming messages. These methods are asynchronous, with `on_messages_stream` returning an async generator for streaming the agent's inner thoughts.

### Basic Usage Example

```python
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

asyncio.run(main())
```

### Multi-Modal Agent

The `AssistantAgent` in `v0.4` supports multi-modal inputs if the model client supports it.

```python
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken, Image
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    cancellation_token = CancellationToken()
    message = MultiModalMessage(
        content=["Here is an image:", Image.from_file(Path("test.png"))],
        source="user",
    )
    response = await assistant.on_messages([message], cancellation_token)
    print(response)

asyncio.run(main())
```

### User Proxy

In `v0.4`, a user proxy is simply an agent that takes user input only, with no special configuration needed.

```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent("user_proxy")
```

### Conversable Agent and Register Reply

In `v0.4`, you can create a custom agent by implementing the `on_messages`, `on_reset`, and `produced_message_types` methods.

```python
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CustomAgent(BaseChatAgent):
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        return Response(chat_message=TextMessage(content="Custom reply", source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)
```

### Save and Load Agent State

In `v0.4`, you can save and load an agent's state using `save_state` and `load_state` methods.

```python
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)

    assistant = AssistantAgent(
        name="assistant",
        system_message="You are a helpful assistant.",
        model_client=model_client,
    )

    cancellation_token = CancellationToken()
    response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
    print(response)

    # Save the state.
    state = await assistant.save_state()

    # (Optional) Write state to disk.
    with open("assistant_state.json", "w") as f:
        json.dump(state, f)

    # (Optional) Load it back from disk.
    with open("assistant_state.json", "r") as f:
        state = json.load(f)
        print(state) # Inspect the state, which contains the chat history.

    # Carry on the chat.
    response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)
    print(response)

    # Load the state, resulting the agent to revert to the previous state before the last message

To quickly build applications using preset agents, you can start by creating a single agent that utilizes tools. Here's a step-by-step guide to set up a basic agent using the `autogen-agentchat` package.

### Installation

First, install the necessary packages:

```bash
pip install -U "autogen-agentchat" "autogen-ext[openai,azure]"
```

### Example Setup

This example uses an OpenAI model, but you can substitute it with other models by updating the `model_client`.

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Define a model client
model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    # api_key="YOUR_API_KEY",
)

# Define a simple function tool
async def get_weather(city: str) -> str:
    """Get the weather for a given city."""
    return f"The weather in {city} is 73 degrees and Sunny."

# Define an AssistantAgent
agent = AssistantAgent(
    name="weather_agent",
    model_client=model_client,
    tools=[get_weather],
    system_message="You are a helpful assistant.",
    reflect_on_tool_use=True,
    model_client_stream=True,  # Enable streaming tokens from the model client.
)

# Run the agent and stream the messages to the console
async def main() -> None:
    await Console(agent.run_stream(task="What is the weather in New York?"))

# If running inside a Python script, use asyncio.run(main())
await main()
```

### Output

When you run the above code, the agent will respond to the task:

```
---------- user ----------
What is the weather in New York?
---------- weather_agent ----------
[FunctionCall(id='call_bE5CYAwB7OlOdNAyPjwOkej1', arguments='{"city":"New York"}', name='get_weather')]
---------- weather_agent ----------
[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', call_id='call_bE5CYAwB7OlOdNAyPjwOkej1', is_error=False)]
---------- weather_agent ----------
The current weather in New York is 73 degrees and sunny.
```

### Next Steps

Now that you have a basic understanding of using a single agent, consider exploring the tutorial for more features of AgentChat.

The provided code and documentation describe a framework for creating and managing agents in a runtime environment. Here's a concise summary of the key components and their usage:

### Key Components

1. **Agent Runtime**: 
   - Manages the lifecycle of agents and message delivery.
   - Can be local (e.g., `SingleThreadedAgentRuntime`) or distributed.

2. **Agents**:
   - Implement logic for handling messages.
   - Use decorators like `@message_handler` to define message handling methods.

3. **Message Handling**:
   - Agents receive messages and process them based on defined handlers.
   - Handlers can be decorated with `@event` or `@rpc` for specific routing.

4. **Registration and Execution**:
   - Agents are registered with the runtime using factory functions.
   - Example of agent registration and message sending:
   ```python
   await Modifier.register(runtime, "modifier", lambda: Modifier(modify_val=lambda x: x - 1))
   await Checker.register(runtime, "checker", lambda: Checker(run_until=lambda x: x <= 1))
   runtime.start()
   await runtime.send_message(Message(10), AgentId("checker", "default"))
   await runtime.stop_when_idle()
   ```

5. **Agent Classes**:
   - `RoutedAgent`: Base class for agents that route messages to handlers.
   - `ClosureAgent`: Allows defining an agent using a closure without a class.

6. **Utilities**:
   - `AgentId`, `AgentMetadata`, `MessageContext`, and `TopicId` are used for identifying agents, handling messages, and defining message topics.

### Installation

- **Python Version**: Requires Python 3.10 or later.
- **Virtual Environment**: Recommended to use a virtual environment for installation.
  ```bash
  python3 -m venv .venv
  source .venv/bin/activate
  ```
- **Package Installation**:
  ```bash
  pip install "autogen-core"
  ```

### Example Usage

The example demonstrates a `Modifier` agent that decrements a value and a `Checker` agent that checks if the value is less than or equal to 1. The agents are registered and run in a `SingleThreadedAgentRuntime`.

### Additional Features

- **Distributed Runtime**: Supports agents running on different processes or machines.
- **Docker for Code Execution**: Recommended for executing model-generated code.

This framework provides a robust infrastructure for building and managing agents with decoupled logic and communication, suitable for both local and distributed environments.

### Quick Start

To get started with the core APIs, let's look at a simple example involving two agents that count down from 10 to 1. We'll define two agent classes: `Modifier` and `Checker`. The `Modifier` agent modifies a number, and the `Checker` agent checks the value against a condition. We'll also define a `Message` data class for communication between agents.

```python
from dataclasses import dataclass
from typing import Callable
from autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler

@dataclass
class Message:
    content: int

@default_subscription
class Modifier(RoutedAgent):
    def __init__(self, modify_val: Callable[[int], int]) -> None:
        super().__init__("A modifier agent.")
        self._modify_val = modify_val

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        val = self._modify_val(message.content)
        print(f"{'-'*80}\nModifier:\nModified {message.content} to {val}")
        await self.publish_message(Message(content=val), DefaultTopicId())  # type: ignore

@default_subscription
class Checker(RoutedAgent):
    def __init__(self, run_until: Callable[[int], bool]) -> None:
        super().__init__("A checker agent.")
        self._run_until = run_until

    @message_handler
    async def handle_message(self, message: Message, ctx: MessageContext) -> None:
        if not self._run_until(message.content):
            print(f"{'-'*80}\nChecker:\n{message.content} passed the check, continue.")
            await self.publish_message(Message(content=message.content), DefaultTopicId())
        else:
            print(f"{'-'*80}\nChecker:\n{message.content} failed the check, stopping.")
```

### Key Concepts

- **Agent Logic**: The logic of agents, whether using a model or code executor, is decoupled from message delivery.
- **Agent Runtime**: The framework provides a communication infrastructure called the Agent Runtime, while agents handle their own logic.

### Agent Runtime Overview

The agent runtime is crucial for managing the lifecycle of agents and delivering messages. The creation and management of agents are handled by the runtime. Below is an example of how to register and run agents using `SingleThreadedAgentRuntime`, a local embedded agent runtime implementation.

```python
from autogen_core import AgentId, SingleThreadedAgentRuntime

# Create a local embedded runtime.
runtime = SingleThreadedAgentRuntime()

# Register the modifier and checker agents.
await Modifier.register(
    runtime,
    "modifier",
    lambda: Modifier(modify_val=lambda x: x - 1),
)

await Checker.register(
    runtime,
    "checker",
    lambda: Checker(run_until=lambda x: x <= 1),
)

# Start the runtime and send a message to the checker.
runtime.start()
await runtime.send_message(Message(10), AgentId("checker", "default"))
await runtime.stop_when_idle()
```

### LangChain Tool Adapter

The `LangChainToolAdapter` class allows you to wrap a LangChain tool and make it available to AutoGen. This requires the `langchain` extra for the `autogen-ext` package.

```bash
pip install -U "autogen-ext[langchain]"
```

#### Example: Using `PythonAstREPLTool`

This example demonstrates how to use the `PythonAstREPLTool` to interact with a Pandas DataFrame.

```python
import asyncio
import pandas as pd
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from autogen_ext.tools.langchain import LangChainToolAdapter
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken

async def main() -> None:
    df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")
    tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        tools=[tool],
        model_client=model_client,
        system_message="Use the `df` variable to access the dataset.",
    )
    await Console(agent.on_messages_stream(
        [TextMessage(content="What's the average age of the passengers?", source="user")],
        CancellationToken()
    ))

asyncio.run(main())
```

#### Example: Using `SQLDatabaseToolkit`

This example shows how to use the `SQLDatabaseToolkit` to interact with an SQLite database.

```python
import asyncio
import sqlite3
import requests
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from sqlalchemy import Engine, create_engine
from sqlalchemy.pool import StaticPool

def get_engine_for_chinook_db() -> Engine:
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text
    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )

async def main() -> None:
    engine = get_engine_for_chinook_db()
    db = SQLDatabase(engine)
    llm = ChatOpenAI(temperature=0)
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)
    tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        model_client=model_client,
        tools=tools,
        model_client_stream=True,
        system_message="Respond with 'TERMINATE' if the task is completed.",
    )
    termination = TextMentionTermination("TERMINATE")
    chat = RoundRobinGroupChat([agent], termination_condition=termination)
    await Console(chat.run_stream(task="Show some tables in the database"))

if __name__ == "__main__":
    asyncio.run(main())
```

These examples demonstrate how

The provided documentation outlines how to use the `SQLDatabaseToolkit` from the `langchain_community` package to interact with an SQLite database. It demonstrates the use of the `RoundRobinGroupChat` to iterate a single agent over multiple steps. Here's a concise summary with key code snippets:

### Key Components and Usage

1. **Setup and Imports**:
   - Import necessary modules and classes for database interaction, agent setup, and chat handling.
   - Use `sqlite3` and `requests` to fetch and execute SQL scripts.

   ```python
   import asyncio
   import sqlite3
   import requests
   from autogen_agentchat.agents import AssistantAgent
   from autogen_agentchat.conditions import TextMentionTermination
   from autogen_agentchat.teams import RoundRobinGroupChat
   from autogen_agentchat.ui import Console
   from autogen_ext.models.openai import OpenAIChatCompletionClient
   from autogen_ext.tools.langchain import LangChainToolAdapter
   from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
   from langchain_community.utilities.sql_database import SQLDatabase
   from langchain_openai import ChatOpenAI
   from sqlalchemy import Engine, create_engine
   from sqlalchemy.pool import StaticPool
   ```

2. **Database Engine Creation**:
   - Define a function to create an in-memory SQLite database engine using the Chinook database.

   ```python
   def get_engine_for_chinook_db() -> Engine:
       url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
       response = requests.get(url)
       sql_script = response.text
       connection = sqlite3.connect(":memory:", check_same_thread=False)
       connection.executescript(sql_script)
       return create_engine(
           "sqlite://",
           creator=lambda: connection,
           poolclass=StaticPool,
           connect_args={"check_same_thread": False},
       )
   ```

3. **Main Function**:
   - Set up the database, toolkit, and agent.
   - Use `RoundRobinGroupChat` to manage chat interactions.

   ```python
   async def main() -> None:
       engine = get_engine_for_chinook_db()
       db = SQLDatabase(engine)
       llm = ChatOpenAI(temperature=0)
       toolkit = SQLDatabaseToolkit(db=db, llm=llm)
       tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]
       model_client = OpenAIChatCompletionClient(model="gpt-4o")
       agent = AssistantAgent(
           "assistant",
           model_client=model_client,
           tools=tools,
           model_client_stream=True,
           system_message="Respond with 'TERMINATE' if the task is completed.",
       )
       termination = TextMentionTermination("TERMINATE")
       chat = RoundRobinGroupChat([agent], termination_condition=termination)
       await Console(chat.run_stream(task="Show some tables in the database"))

   if __name__ == "__main__":
       asyncio.run(main())
   ```

4. **LangChainToolAdapter**:
   - Wraps a LangChain tool to make it available to AutoGen.

   ```python
   class LangChainToolAdapter(langchain_tool: LangChainTool):
       def __init__(self, langchain_tool: LangChainTool):
           self._langchain_tool = langchain_tool
           # Extract name and description
           name = self._langchain_tool.name
           description = self._langchain_tool.description or ""
           # Determine the callable method
           if hasattr(self._langchain_tool, "func") and callable(self._langchain_tool.func):
               self._callable = self._langchain_tool.func
           elif hasattr(self._langchain_tool, "_run") and callable(self._langchain_tool._run):
               self._callable = self._langchain_tool._run
           else:
               raise AttributeError(f"The provided LangChain tool '{name}' does not have a callable 'func' or '_run' method.")
           # Ensure args_type is a subclass of BaseModel
           if not issubclass(args_type, BaseModel):
               raise ValueError(f"Failed to create a valid Pydantic v2 model for {name}")
           super().__init__(args_type, return_type, name, description)
   ```

This setup allows for interactive querying and manipulation of an SQLite database using a conversational agent, leveraging the LangChain and AutoGen frameworks.

The provided documentation outlines several classes and functions related to code execution and agent management in a distributed system. Here's a summary of the key components:

### Classes and Methods

#### `FunctionWithRequirements`
- **Purpose**: Wraps a function to track its dependencies, such as required Python packages and global imports.
- **Methods**:
  - `from_callable(func, python_packages=[], global_imports=[])`: Creates an instance from a callable.
  - `from_str(func, python_packages=[], global_imports=[])`: Creates an instance from a string representation of a function.

#### `FunctionWithRequirementsStr`
- **Purpose**: Similar to `FunctionWithRequirements`, but specifically for functions represented as strings.

#### `ImportFromModule`
- **Purpose**: Represents an import statement from a module.
- **Attributes**:
  - `module`: The module name.
  - `imports`: A tuple of strings or aliases representing the imports.

#### `Alias`
- **Purpose**: Represents an alias for an import.
- **Attributes**:
  - `name`: The original name.
  - `alias`: The alias name.

#### `CodeBlock`
- **Purpose**: Represents a block of code with a specified language.
- **Attributes**:
  - `code`: The code as a string.
  - `language`: The language of the code.

#### `CodeExecutor`
- **Purpose**: Abstract base class for executing code blocks.
- **Methods**:
  - `execute_code_blocks(code_blocks, cancellation_token)`: Executes code blocks and returns the result.
  - `restart()`: Restarts the code executor.

#### `CodeResult`
- **Purpose**: Represents the result of a code execution.
- **Attributes**:
  - `exit_code`: The exit code of the execution.
  - `output`: The output of the execution.

#### `with_requirements`
- **Purpose**: Decorator to wrap a function with its package and import requirements.
- **Parameters**:
  - `python_packages`: List of required Python packages.
  - `global_imports`: List of required global imports.

### Example Usage

```python
import pandas
from autogen_core.code_executor import with_requirements, CodeBlock

@with_requirements(
    python_packages=["pandas"],
    global_imports=["pandas"]
)
def load_data() -> pandas.DataFrame:
    """Load some sample data."""
    data = {
        "name": ["John", "Anna", "Peter", "Linda"],
        "location": ["New York", "Paris", "Berlin", "London"],
        "age": [24, 13, 53, 33],
    }
    return pandas.DataFrame(data)
```

### Docker Command Line Code Executor

#### `DockerCommandLineCodeExecutor`
- **Purpose**: Executes code in a Docker container.
- **Attributes**:
  - `image`: Docker image to use.
  - `container_name`: Name of the Docker container.
  - `timeout`: Timeout for code execution.
  - `work_dir`: Working directory for code execution.
- **Methods**:
  - `start()`: Starts the Docker container.
  - `stop()`: Stops the Docker container.
  - `execute_code_blocks(code_blocks, cancellation_token)`: Executes code blocks in the Docker container.

This documentation provides a comprehensive overview of how to manage code execution with dependencies and how to execute code in a Docker environment, along with agent management in a distributed system.

The provided documentation outlines various classes and functions for managing agents, messages, and subscriptions in an asynchronous runtime environment. Below are some key components and their usage:

### Agent Registration
To register an agent factory with a runtime, use the `register_factory` method. This is a low-level API, and it's recommended to use the agent class's `register` method for automatic subscription handling.

```python
async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())
```

### Agent and Message Handling
Agents can be created by subclassing `RoutedAgent` and defining message handlers using decorators like `@event` or `@rpc`.

```python
class MyAgent(RoutedAgent):
    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)
```

### Runtime Management
The `SingleThreadedAgentRuntime` is suitable for development and standalone applications. It processes messages using a single asyncio queue.

```python
runtime = SingleThreadedAgentRuntime()
runtime.start()
await runtime.stop_when_idle()
```

### Subscriptions
Subscriptions define the topics an agent is interested in. Use classes like `TypeSubscription` or `DefaultSubscription` to manage these.

```python
subscription = TypeSubscription(topic_type="t1", agent_type="a1")
```

### Message Serialization
Add message serializers to the runtime using `add_message_serializer`.

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]])
```

### State Management
Save and load the state of the runtime or individual agents using `save_state` and `load_state`.

```python
async def save_state() -> Mapping[str, Any]:
    # Save the state of the runtime
```

### Closure Agents
Define agents using closures with `ClosureAgent.register_closure`.

```python
await ClosureAgent.register_closure(runtime, "output_result", output_result)
```

### Example Usage
Here's a simple example of creating a runtime, registering an agent, sending a message, and stopping the runtime:

```python
import asyncio
from dataclasses import dataclass
from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print("Received message:", message.content)

async def main():
    runtime = SingleThreadedAgentRuntime()
    await runtime.register_factory("my_agent", lambda: MyAgent())
    await runtime.send_message(MyMessage("Hello, world!"), AgentId("my_agent", "default"))
    await runtime.stop_when_idle()

asyncio.run(main())
```

This documentation provides a comprehensive guide to managing agents and messages in an asynchronous environment, focusing on registration, handling, and runtime management.

### Overview

This documentation provides an overview of setting up and using an agent runtime system, including creating agents, sending messages, and handling messages. It also covers the use of Docker for executing code in a containerized environment.

### Agent Runtime Example

The following example demonstrates how to create a runtime, register an agent, send a message, and stop the runtime:

```python
import asyncio
from dataclasses import dataclass
from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)

@dataclass
class MyMessage:
    content: str

@default_subscription
class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

asyncio.run(main())
```

### Key Functions

- **send_message**: Sends a message to an agent and gets a response.
  ```python
  async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any
  ```

- **publish_message**: Publishes a message to all agents in a given namespace.
  ```python
  async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None
  ```

- **start**: Starts the runtime message processing loop.
  ```python
  start() -> None
  ```

- **stop_when_idle**: Stops the runtime when there are no outstanding messages.
  ```python
  async stop_when_idle() -> None
  ```

### Docker Command Line Code Executor

The `DockerCommandLineCodeExecutor` class allows executing code in a Docker container. It supports Python and shell scripts.

```python
class DockerCommandLineCodeExecutor(
    image: str = 'python:3-slim',
    container_name: str | None = None,
    *,
    timeout: int = 60,
    work_dir: Path | str = Path('.'),
    bind_dir: Path | str | None = None,
    auto_remove: bool = True,
    stop_container: bool = True,
    functions: Sequence[FunctionWithRequirements[Any, A] | Callable[..., Any] | FunctionWithRequirementsStr] = [],
    functions_module: str = 'functions',
    extra_volumes: Dict[str, Dict[str, str]] | None = None,
    extra_hosts: Dict[str, str] | None = None,
    init_command: str | None = None
)
```

#### Parameters

- **image**: Docker image for code execution (default: `python:3-slim`).
- **container_name**: Name of the Docker container (autogenerated if None).
- **timeout**: Timeout for code execution (default: 60 seconds).
- **work_dir**: Working directory for code execution.
- **auto_remove**: Automatically remove the Docker container when stopped.
- **stop_container**: Automatically stop the container when the process exits.

#### Usage

To use the `DockerCommandLineCodeExecutor`, ensure the `autogen-ext` package is installed with Docker support:

```bash
pip install "autogen-ext[docker]"
```

This setup allows for executing code blocks in a controlled Docker environment, supporting both Python and shell scripts.

The `DockerCommandLineCodeExecutor` class is designed to execute code within a Docker container using a command line environment. It supports Python and shell scripts and requires the `docker` extra for the `autogen-ext` package.

### Key Features:
- **Docker Image**: Defaults to `python:3-slim`.
- **Container Management**: Automatically manages container lifecycle with options to auto-remove and stop containers.
- **Timeout**: Code execution timeout is configurable, defaulting to 60 seconds.
- **Working Directory**: Code execution occurs in a specified working directory, defaulting to the current directory.
- **Function Support**: Allows defining functions that can be accessed within the container.
- **Volume and Host Configuration**: Supports additional volume mounts and host mappings.

### Usage Example:
```python
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor

executor = DockerCommandLineCodeExecutor(
    image='python:3-slim',
    timeout=60,
    work_dir=Path('.'),
    auto_remove=True,
    stop_container=True
)

# Execute code blocks
async def run_code():
    code_blocks = [CodeBlock(code='print("Hello, World!")', language='python')]
    result = await executor.execute_code_blocks(code_blocks, CancellationToken())
    print(result.output)

# Run the async function
asyncio.run(run_code())
```

### Important Methods:
- `async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) -> CommandLineCodeResult`: Executes the provided code blocks and returns the result.
- `async restart() -> None`: Restarts the code executor.
- `async start() -> None`: Starts the code executor.
- `async stop() -> None`: Stops the code executor.

### Configuration:
- **`extra_volumes`**: Allows mounting additional directories to the container.
- **`extra_hosts`**: Adds host mappings to the container.
- **`init_command`**: Runs a shell command before each shell operation.

### Supported Languages:
- `bash`, `shell`, `sh`, `pwsh`, `powershell`, `ps1`, `python`

This class is useful for executing code in isolated environments, ensuring dependencies are managed, and providing a consistent execution context.

The `DockerCommandLineCodeExecutor` class is designed to execute code within a Docker container environment. It supports Python and shell scripts, executing code blocks in the order they are received. The class requires the `docker` extra for the `autogen-ext` package, which can be installed using:

```bash
pip install "autogen-ext[docker]"
```

### Key Features and Parameters:

- **Docker Image**: Specify the Docker image for execution, defaulting to `python:3-slim`.
- **Container Name**: Optionally name the Docker container; if not provided, a name is autogenerated.
- **Timeout**: Set a timeout for code execution, defaulting to 60 seconds.
- **Working Directory**: Define the working directory for code execution, defaulting to the current directory.
- **Bind Directory**: Specify a directory to bind to the container, useful for nested container scenarios.
- **Auto Remove**: Automatically remove the container upon stopping, defaulting to `True`.
- **Stop Container**: Automatically stop the container when the context manager exits or the process ends, defaulting to `True`.
- **Functions**: Provide a list of functions available to the executor.
- **Functions Module**: Name the module for storing functions, defaulting to "functions".
- **Extra Volumes**: Mount additional volumes to the container.
- **Extra Hosts**: Add host mappings to the container.
- **Init Command**: Run a shell command before each shell operation.

### Methods:

- **`start()`**: Asynchronously starts the Docker container.
- **`stop()`**: Asynchronously stops the Docker container.
- **`restart()`**: Restarts the code executor.
- **`execute_code_blocks(code_blocks, cancellation_token)`**: Executes the provided code blocks and returns the result.
- **`_setup_functions(cancellation_token)`**: Sets up functions for execution.
- **`_execute_code_dont_check_setup(code_blocks, cancellation_token)`**: Executes code without checking setup.
- **`_to_config()`**: Converts the component to a configuration object.
- **`_from_config(config)`**: Creates a component from a configuration object.

### Usage Example:

```python
executor = DockerCommandLineCodeExecutor(image='python:3.8', timeout=120)
async with executor:
    result = await executor.execute_code_blocks([CodeBlock(code='print("Hello, World!")', language='python')])
    print(result.output)
```

This class is part of the `autogen_ext.code_executors.docker` module and provides a robust way to execute code in isolated Docker environments, supporting both Python and shell script execution.

The `DockerCommandLineCodeExecutor` class is designed to execute code within a Docker container. It supports Python and shell scripts, and requires the `docker` extra for the `autogen-ext` package. Here's a concise overview of its key components and usage:

### Class: `DockerCommandLineCodeExecutor`

#### Initialization
```python
DockerCommandLineCodeExecutor(
    image: str = 'python:3-slim',
    container_name: Optional[str] = None,
    *,
    timeout: int = 60,
    work_dir: Union[Path, str] = Path('.'),
    bind_dir: Optional[Union[Path, str]] = None,
    auto_remove: bool = True,
    stop_container: bool = True,
    functions: Sequence[Union[FunctionWithRequirements[Any, A], Callable[..., Any], FunctionWithRequirementsStr]] = [],
    functions_module: str = 'functions',
    extra_volumes: Optional[Dict[str, Dict[str, str]]] = None,
    extra_hosts: Optional[Dict[str, str]] = None,
    init_command: Optional[str] = None
)
```

#### Key Properties
- **`SUPPORTED_LANGUAGES`**: Supports languages like 'bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python'.
- **`bind_dir`**: The binding directory for the code execution container.
- **`timeout`**: The timeout for code execution.
- **`work_dir`**: The working directory for the code execution.

#### Methods
- **`async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) -> CommandLineCodeResult`**: Executes the code blocks and returns the result.
- **`async restart() -> None`**: Restarts the code executor.
- **`async start() -> None`**: Starts the code executor.
- **`async stop() -> None`**: Stops the code executor.

#### Usage Example
To use the `DockerCommandLineCodeExecutor`, you need to install the `autogen-ext` package with the `docker` extra:
```bash
pip install "autogen-ext[docker]"
```

### Configuration Class: `DockerCommandLineCodeExecutorConfig`
Defines the configuration for the executor, including parameters like `image`, `container_name`, `timeout`, `work_dir`, `bind_dir`, `auto_remove`, `stop_container`, `functions_module`, `extra_volumes`, `extra_hosts`, and `init_command`.

### Notes
- The executor saves each code block in a file in the working directory and executes it in the container.
- The class requires Docker to be installed and running on the host machine.
- The `FUNCTION_PROMPT_TEMPLATE` provides a template for accessing user-defined functions within the module.

This class is part of the `autogen_ext.code_executors.docker` module and is used for executing code in a controlled Docker environment, making it suitable for scenarios where isolation and environment consistency are required.

The `DockerCommandLineCodeExecutor` class is designed to execute code within a Docker container environment. It supports Python and shell scripts, and requires the `docker` extra for the `autogen-ext` package. Here's a concise overview of its key features and methods:

### Class: `DockerCommandLineCodeExecutor`

#### Initialization Parameters:
- **image**: Docker image for execution (default: `'python:3-slim'`).
- **container_name**: Name of the Docker container (default: `None`).
- **timeout**: Timeout for code execution (default: `60` seconds).
- **work_dir**: Working directory for code execution (default: `Path('.')`).
- **bind_dir**: Directory to bind to the container (default: `None`).
- **auto_remove**: Automatically remove the container when stopped (default: `True`).
- **stop_container**: Automatically stop the container on exit (default: `True`).
- **functions**: List of functions available to the executor (default: `[]`).
- **functions_module**: Module name for storing functions (default: `'functions'`).
- **extra_volumes**: Additional volumes to mount (default: `None`).
- **extra_hosts**: Host mappings for the container (default: `None`).
- **init_command**: Shell command to run before execution (default: `None`).

#### Key Methods:
- **`execute_code_blocks`**: Executes a list of code blocks and returns the result.
  ```python
  async def execute_code_blocks(
      self, 
      code_blocks: List[CodeBlock], 
      cancellation_token: CancellationToken
  ) -> CommandLineCodeResult
  ```

- **`start`**: Starts the Docker container.
  ```python
  async def start(self) -> None
  ```

- **`stop`**: Stops the Docker container.
  ```python
  async def stop(self) -> None
  ```

- **`restart`**: Restarts the code executor.
  ```python
  async def restart(self) -> None
  ```

- **`_setup_functions`**: Sets up functions by writing them to a file in the working directory.

- **`_execute_code_dont_check_setup`**: Executes code blocks without checking setup.
  ```python
  async def _execute_code_dont_check_setup(
      self, 
      code_blocks: List[CodeBlock], 
      cancellation_token: CancellationToken
  ) -> CommandLineCodeResult
  ```

#### Properties:
- **`timeout`**: Returns the timeout for code execution.
- **`work_dir`**: Returns the working directory for code execution.
- **`bind_dir`**: Returns the binding directory for the container.

### Usage Example:
To use this class, ensure Docker is installed and running, and the `autogen-ext` package is installed with the `docker` extra:
```bash
pip install "autogen-ext[docker]"
```

This class is useful for executing code in isolated environments, ensuring dependencies are managed within the container, and supporting both Python and shell script execution.

The `autogen_ext.runtimes.grpc` module provides classes and methods for managing agent runtimes using gRPC. Key components include:

### Classes

- **GrpcWorkerAgentRuntime**: Manages remote or cross-language agents using protobufs for messaging. It supports adding message serializers, subscriptions, and agent factories. It can start, stop, and manage agent states.

- **GrpcWorkerAgentRuntimeHost**: Hosts the server for agent communication, allowing it to start and stop based on signals.

- **GrpcWorkerAgentRuntimeHostServicer**: A gRPC servicer for message delivery to agents, with methods for managing subscriptions and channels.

### Key Methods

- **start()**: Starts the runtime or server in a background task.
- **stop()**: Stops the runtime or server immediately.
- **stop_when_signal(signals)**: Stops the runtime or server when specified signals are received.
- **add_message_serializer(serializer)**: Adds a message serializer to the runtime.
- **add_subscription(subscription)**: Adds a subscription for processing published messages.
- **register_factory(type, agent_factory, expected_class)**: Registers an agent factory with the runtime.
- **send_message(message, recipient, sender, cancellation_token, message_id)**: Sends a message to an agent and returns a response.
- **publish_message(message, topic_id, sender, cancellation_token, message_id)**: Publishes a message to all agents in a namespace.
- **try_get_underlying_agent_instance(id, type)**: Attempts to retrieve an agent instance by ID and type.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

### Exceptions

- **CantHandleException**: Raised if the recipient cannot handle the message.
- **UndeliverableException**: Raised if the message cannot be delivered.
- **LookupError**: Raised if an agent or subscription is not found.
- **NotAccessibleError**: Raised if an agent is not accessible.
- **TypeError**: Raised if an agent is not of the expected type.

This module is essential for managing agent communication and lifecycle in a distributed system using gRPC.

The provided code and documentation describe a system for managing agents within a runtime environment. Here's a summary of the key components and functionalities:

### Key Components

1. **Agent**: Represents an entity that can handle messages, save/load state, and be identified by an `AgentId`.
   - `metadata`: Provides metadata about the agent.
   - `id`: Unique identifier for the agent.
   - `on_message`: Handles incoming messages.
   - `save_state` and `load_state`: Manage the agent's state.

2. **AgentId**: Uniquely identifies an agent instance within a runtime.
   - `type`: Associates an agent with a specific factory function.
   - `key`: Identifier for the agent instance.

3. **AgentRuntime**: Manages the lifecycle and communication of agents.
   - `send_message`: Sends a message to an agent and awaits a response.
   - `publish_message`: Publishes a message to all agents in a given namespace.
   - `register_factory`: Registers an agent factory with the runtime.
   - `add_subscription` and `remove_subscription`: Manage message subscriptions.

4. **AgentProxy**: Allows interaction with an `AgentId` as if it were the actual agent.
   - `send_message`: Sends a message through the proxy.

5. **AgentMetadata**: Contains metadata about an agent, such as type and description.

### Key Functions

- **Message Handling**: Agents can process messages using the `on_message` method, which is called by the runtime.
- **State Management**: Agents can save and load their state, which must be JSON serializable.
- **Factory Registration**: Factories for creating agents can be registered with the runtime, allowing for dynamic agent instantiation.
- **Signal Handling**: The runtime can be stopped gracefully upon receiving specific signals using `stop_when_signal`.
- **Background Tasks**: The runtime manages background tasks, ensuring they are completed or canceled appropriately.

### Example Usage

```python
async def main() -> None:
    runtime: AgentRuntime = ...  # Initialize runtime
    await runtime.register_factory("my_agent", lambda: MyAgent())

asyncio.run(main())
```

This system is designed to facilitate the creation, management, and communication of agents within a distributed runtime environment, providing a robust framework for handling asynchronous tasks and message passing.

The provided documentation outlines various classes and methods for handling agent-based messaging and runtime management. Below are key components and their functionalities:

### Agent Messaging and Runtime

- **`on_message_impl(message: Any, ctx: MessageContext) -> Any`**: Abstract method for handling messages. Subclasses should implement this to define how messages are processed.

- **`send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`**: Sends a message to a specified agent. Refer to `autogen_core.AgentRuntime.send_message()` for more details.

- **`publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) -> None`**: Publishes a message to a topic, following a publish-subscribe model.

- **`save_state() -> Mapping[str, Any]`**: Saves the agent's state, which must be JSON serializable.

- **`load_state(state: Mapping[str, Any]) -> None`**: Loads the agent's state from a saved state.

- **`close() -> None`**: Called when the runtime is closed.

### Registration and Subscriptions

- **`register(runtime: AgentRuntime, type: str, factory: Callable[[], Self | Awaitable[Self]], *, skip_class_subscriptions: bool = False, skip_direct_message_subscription: bool = False) -> AgentType`**: Registers a virtual subclass of an abstract base class (ABC).

- **`TypeSubscription(topic_type: str, agent_type: str | AgentType, id: str | None = None)`**: Matches topics based on type and maps them to agents using the source as the key.

- **`DefaultSubscription(topic_type: str = 'default', agent_type: str | AgentType | None = None)`**: A default subscription for applications needing global agent scope.

### Caching and Storage

- **`CacheStore`**: Defines the interface for store/cache operations. Subclasses manage the lifecycle of storage.

- **`InMemoryStore`**: An implementation of `CacheStore` for in-memory storage.

### Cancellation and Context

- **`CancellationToken`**: Used to cancel pending async calls.

- **`AgentInstantiationContext`**: Provides context for agent instantiation, allowing access to the current runtime and agent ID.

### Message Handling

- **`message_handler`**: Decorator for generic message handlers in a `RoutedAgent` class.

- **`event`**: Decorator for event message handlers.

- **`rpc`**: Decorator for RPC message handlers.

### Runtime Management

- **`SingleThreadedAgentRuntime`**: A single-threaded runtime for processing messages using an asyncio queue. Suitable for development and standalone applications.

- **`start()`**: Starts the runtime message processing loop.

- **`stop()`**: Immediately stops the runtime message processing loop.

- **`stop_when_idle()`**: Stops the runtime when there are no outstanding messages.

### Example Usage

```python
import asyncio
from dataclasses import dataclass
from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()

asyncio.run(main())
```

This example demonstrates creating a runtime, registering an agent, sending a message, and stopping the runtime.

The `autogen_core` library provides a framework for managing agents, their states, and interactions within a runtime environment. Below are some key components and functions:

### Agent Management

- **Agent Metadata and State:**
  - `async agent_metadata(agent: AgentId) -> AgentMetadata`: Retrieve metadata for a specific agent.
  - `async agent_save_state(agent: AgentId) -> Mapping[str, Any]`: Save the state of a single agent.
  - `async agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None`: Load the state of a single agent.

- **Agent Factory Registration:**
  - `async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`: Register an agent factory with the runtime. This is a low-level API, and usually, the agent classâ€™s `register` method should be used.

### Messaging and Subscriptions

- **Message Handling:**
  - `async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`: Send a message to an agent and get a response.
  - `async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None`: Publish a message to all agents in a given namespace.

- **Subscriptions:**
  - `async add_subscription(subscription: Subscription) -> None`: Add a new subscription for processing published messages.
  - `async remove_subscription(id: str) -> None`: Remove a subscription from the runtime.

### Component and Configuration Management

- **Component Classes:**
  - `class Component`: Base class for creating component classes. Requires implementation of `component_config_schema` and `component_type`.
  - `class ComponentModel`: Model class for a component, containing all information required to instantiate a component.

- **Configuration Methods:**
  - `classmethod _from_config(config: FromConfigT) -> Self`: Create a new instance of the component from a configuration object.
  - `dump_component() -> ComponentModel`: Dump the component to a model that can be loaded back in.

### Utility Classes

- **CancellationToken**: Used to cancel pending async calls.
  - `cancel() -> None`: Cancel pending async calls.
  - `is_cancelled() -> bool`: Check if the token has been used.

- **AgentInstantiationContext**: Provides context for agent instantiation, allowing access to the current runtime and agent ID.

### Example Usage

```python
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from dataclasses import dataclass

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, register it with a runtime, and handle messages.

### Function and Class Summaries

#### `bool`
```python
def is_match(topic_id: TopicId) -> bool
```
Checks if a given `topic_id` matches the subscription. Returns `True` if it matches, `False` otherwise.

#### `map_to_agent`
```python
def map_to_agent(topic_id: TopicId) -> AgentId
```
Maps a `topic_id` to an agent. Should be called only if `is_match` returns `True`. Raises `CantHandleException` if the subscription cannot handle the `topic_id`.

#### `MessageContext`
```python
class MessageContext(sender: AgentId | None, topic_id: TopicId | None, is_rpc: bool, cancellation_token: CancellationToken, message_id: str)
```
Represents the context of a message, including sender, topic ID, RPC status, cancellation token, and message ID.

#### `AgentType`
```python
class AgentType(type: str)
```
Represents an agent type with a string representation.

#### `SubscriptionInstantiationContext`
```python
class SubscriptionInstantiationContext
```
Provides context for subscription instantiation.

#### `MessageHandlerContext`
```python
class MessageHandlerContext
```
Provides context for message handling.

#### `MessageSerializer`
```python
class MessageSerializer(*args, **kwargs)
```
Handles serialization and deserialization of messages. Includes properties `data_content_type` and `type_name`.

#### `UnknownPayload`
```python
class UnknownPayload(type_name: str, data_content_type: str, payload: bytes)
```
Represents an unknown payload with type name, content type, and payload data.

#### `Image`
```python
class Image(image: Image)
```
Represents an image with methods to load from various sources like URL, URI, base64, and file. Includes `to_base64` and `to_openai_format`.

#### `RoutedAgent`
```python
class RoutedAgent(description: str)
```
Base class for agents that route messages to handlers. Handlers are added as methods decorated with `event()` or `rpc()`.

#### `ClosureAgent`
```python
class ClosureAgent(description: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn')
```
Defines an agent using a closure. Allows message handling without defining a class. Includes `register_closure` method.

#### `ClosureContext`
```python
class ClosureContext(*args, **kwargs)
```
Provides context for closures, including methods to send and publish messages.

#### `message_handler`
```python
def message_handler(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None)
```
Decorator for generic message handlers in `RoutedAgent` classes. Requires specific method signature.

#### `event`
```python
def event(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None)
```
Decorator for event message handlers in `RoutedAgent` classes. Requires specific method signature.

#### `rpc`
```python
def rpc(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None)
```
Decorator for RPC message handlers in `RoutedAgent` classes. Requires specific method signature.

#### `FunctionCall`
```python
class FunctionCall(id: 'str', arguments: 'str', name: 'str')
```
Represents a function call with ID, arguments, and name.

#### `TypeSubscription`
```python
class TypeSubscription(topic_type: str, agent_type: str | AgentType, id: str | None = None)
```
Matches topics based on type and maps to agents using the source of the topic as the agent key.

#### `DefaultSubscription`
```python
class DefaultSubscription(topic_type: str = 'default', agent_type: str | AgentType | None = None)
```
A default subscription for applications needing global scope for agents.

#### `DefaultTopicId`
```python
class DefaultTopicId(type: str = 'default', source: str | None = None)
```
Provides a default for `topic_id` and source fields.

#### `SingleThreadedAgentRuntime`
```python
class SingleThreadedAgentRuntime(*, intervention_handlers: List[InterventionHandler] | None = None

The provided documentation outlines the structure and functionality of a component model and various classes related to agent runtime and message handling in a system using Pydantic models and gRPC. Here's a concise summary of the key elements:

### ComponentModel
- **Purpose**: Represents a model class for a component, containing all necessary information to instantiate it.
- **Fields**:
  - `provider`: `str` (Required) - Describes how the component can be instantiated.
  - `component_type`: `Literal['model', 'agent', 'tool', 'termination', 'token_provider'] | str | None` - Logical type of the component.
  - `version`: `int | None` - Version of the component specification.
  - `component_version`: `int | None` - Version of the component.
  - `description`: `str | None` - Description of the component.
  - `label`: `str | None` - Human-readable label for the component.
  - `config`: `dict[str, Any]` (Required) - Schema-validated config field.

### ComponentSchemaType
- **Purpose**: Represents the configuration schema of a component.
- **Attributes**:
  - `component_config_schema`: Type of the Pydantic model class for configuration.

### ComponentToConfig
- **Purpose**: Defines methods a class must implement to be a component.
- **Attributes**:
  - `component_type`: Logical type of the component.
  - `component_version`: Version of the component.
  - `component_provider_override`: Override for the provider string.
  - `component_description`: Description of the component.
  - `component_label`: Human-readable label for the component.
- **Methods**:
  - `_to_config()`: Dumps the configuration needed to create a new instance of a component.

### InterventionHandler
- **Purpose**: Class for modifying, logging, or dropping messages processed by `AgentRuntime`.
- **Methods**:
  - `async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) -> Any | type[DropMessage]`
  - `async on_publish(message: Any, *, message_context: MessageContext) -> Any | type[DropMessage]`
  - `async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) -> Any | type[DropMessage]`

### DefaultInterventionHandler
- **Purpose**: Provides default implementations for intervention handler methods, returning messages unchanged.

### GrpcWorkerAgentRuntime
- **Purpose**: An agent runtime for running remote or cross-language agents using gRPC.
- **Methods**:
  - `async start()`: Starts the runtime in a background task.
  - `async stop()`: Stops the runtime immediately.
  - `async stop_when_signal(signals: Sequence[signal.Signals])`: Stops the runtime when a signal is received.
  - `async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, ...) -> Any`
  - `async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, ...) -> None`

### HostConnection
- **Purpose**: Manages gRPC connections for sending and receiving messages.
- **Methods**:
  - `async from_host_address(host_address: str, extra_grpc_config: ChannelArgumentType) -> Self`
  - `async close() -> None`
  - `async send(message: agent_worker_pb2.Message) -> None`
  - `async recv() -> agent_worker_pb2.Message`

This documentation provides a comprehensive overview of the classes and methods involved in managing components and handling messages in a distributed system using gRPC and Pydantic models.

The provided code and documentation describe a system for managing agents in a runtime environment using gRPC. Here's a summary of the key components and functionalities:

### Key Components

1. **Agent**: Represents an entity that can handle messages. It has properties like `metadata`, `id`, and methods for handling messages (`on_message`), saving/loading state, and closing.

2. **AgentId**: Uniquely identifies an agent instance within a runtime. It consists of a `type` and a `key`.

3. **AgentRuntime**: Manages the lifecycle of agents, including sending and receiving messages, registering agent factories, and managing subscriptions.

4. **GrpcWorkerAgentRuntime**: A specific implementation of `AgentRuntime` that uses gRPC for communication. It supports adding message serializers, managing subscriptions, and handling agent state.

5. **GrpcWorkerAgentRuntimeHost**: Manages the server-side operations for hosting agents, including starting and stopping the server.

6. **AgentInstantiationContext**: Provides context during agent instantiation, allowing access to the current runtime and agent ID.

7. **CancellationToken**: Used to cancel pending asynchronous operations.

### Key Functions

- **register_factory**: Registers an agent factory with the runtime. This is used to create agents of a specific type.
  ```python
  async def register_factory(
      self,
      type: str | AgentType,
      agent_factory: Callable[[], T | Awaitable[T]],
      *,
      expected_class: type[T] | None = None
  ) -> AgentType:
  ```

- **send_message**: Sends a message to a specific agent and expects a response.
  ```python
  async def send_message(
      message: Any,
      recipient: AgentId,
      *,
      sender: AgentId | None = None,
      cancellation_token: CancellationToken | None = None,
      message_id: str | None = None
  ) -> Any:
  ```

- **publish_message**: Publishes a message to all agents in a given namespace without expecting responses.
  ```python
  async def publish_message(
      message: Any,
      topic_id: TopicId,
      *,
      sender: AgentId | None = None,
      cancellation_token: CancellationToken | None = None,
      message_id: str | None = None
  ) -> None:
  ```

- **add_subscription**: Adds a new subscription for processing published messages.
  ```python
  async def add_subscription(subscription: Subscription) -> None:
  ```

- **remove_subscription**: Removes a subscription from the runtime.
  ```python
  async def remove_subscription(id: str) -> None:
  ```

- **try_get_underlying_agent_instance**: Attempts to retrieve the underlying agent instance by name and namespace.
  ```python
  async def try_get_underlying_agent_instance(
      id: AgentId,
      type: Type[T] = Agent
  ) -> T:
  ```

### Usage Example

Here's a simplified example of how you might register an agent factory and send a message:

```python
from autogen_core import AgentRuntime, AgentId

async def my_agent_factory():
    return MyAgent()

async def main():
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", my_agent_factory)
    agent_id = AgentId("my_agent", "default")
    response = await runtime.send_message("Hello, Agent!", agent_id)

import asyncio
asyncio.run(main())
```

This system is designed to facilitate the management and communication of agents in a distributed environment, leveraging gRPC for cross-language compatibility.

The provided documentation outlines the usage of an agent-based framework for handling messages in a single-threaded runtime environment. Below are key components and examples:

### Key Classes and Methods

- **AgentInstantiationContext**: Provides methods like `current_runtime()` and `current_agent_id()` to access the current runtime and agent ID.

- **SingleThreadedAgentRuntime**: A runtime that processes messages using a single asyncio queue. Suitable for development and standalone applications.

- **RoutedAgent**: A base class for agents that route messages to handlers based on message type. Handlers are defined using decorators like `@event` and `@rpc`.

- **ClosureAgent**: Allows defining an agent using a closure without needing a class. Useful for simple message handling.

- **MessageContext**: Provides context for messages, including sender, topic ID, and cancellation token.

- **Subscription**: Defines topics an agent is interested in. Includes methods like `is_match()` and `map_to_agent()`.

- **TopicId**: Represents the scope of a broadcast message, adhering to a publish-subscribe model.

### Example Usage

#### Creating and Running a Simple Agent

```python
import asyncio
from dataclasses import dataclass
from autogen_core import SingleThreadedAgentRuntime, MessageContext, RoutedAgent, message_handler

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()

asyncio.run(main())
```

#### Using a ClosureAgent

```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

@dataclass
class MyMessage:
    content: str

async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()])
    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()
    result = await queue.get()
    print(result)

asyncio.run(main())
```

### Decorators

- **@message_handler**: Used for generic message handlers in a `RoutedAgent`.
- **@event**: Decorator for event message handlers.
- **@rpc**: Decorator for RPC message handlers.

### Subscriptions

- **TypeSubscription**: Matches topics based on type and maps to agents using the source as the key.
- **DefaultSubscription**: A default subscription for global scope agents.

### Additional Features

- **Message Serialization**: Add custom serializers using `add_message_serializer()`.
- **State Management**: Save and load state with `save_state()` and `load_state()` methods.

This framework is designed for creating agents that handle messages in a structured and efficient manner, suitable for development and testing environments.

The provided documentation outlines the structure and functionality of a component system using Pydantic models and a component framework. Here's a concise summary with key code snippets and explanations:

### Component Definition

A component is defined using a Pydantic `BaseModel` for configuration and a custom class inheriting from `Component`. The `component_type` and `component_config_schema` are essential attributes.

```python
from pydantic import BaseModel
from autogen_core import Component

class Config(BaseModel):
    value: str

class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)
```

### Component Base Classes

- **ComponentBase**: Inherits from `ComponentToConfig`, `ComponentLoader`, and is generic over `ConfigT`.
- **ComponentFromConfig**: Provides a method to create a component instance from a configuration object.

```python
class ComponentFromConfig:
    @classmethod
    def _from_config(config: FromConfigT) -> Self:
        # Create a new instance from config
```

### Component Loading

Components can be loaded using the `ComponentLoader` class, which provides methods to load components from models or dictionaries.

```python
class ComponentLoader:
    @classmethod
    def load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType] = None) -> ExpectedType:
        # Load a component from a model
```

### Component Model

The `ComponentModel` is a Pydantic model that contains all necessary information to instantiate a component.

```python
class ComponentModel(BaseModel):
    provider: str
    component_type: str | None = None
    version: int | None = None
    component_version: int | None = None
    description: str | None = None
    label: str | None = None
    config: dict[str, Any]
```

### Intervention Handlers

Intervention handlers can modify, log, or drop messages processed by the runtime. They are implemented by subclassing `InterventionHandler`.

```python
class MyInterventionHandler(DefaultInterventionHandler):
    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
        if isinstance(message, MyMessage):
            message.content = message.content.upper()
        return message
```

### GRPC Worker Runtime

The `GrpcWorkerAgentRuntime` class is used for running remote or cross-language agents, utilizing protobufs for messaging.

```python
class GrpcWorkerAgentRuntime(AgentRuntime):
    async def start(self) -> None:
        # Start the runtime in a background task

    async def stop(self) -> None:
        # Stop the runtime immediately
```

This summary captures the essential components and their interactions within the framework, providing a foundation for understanding and utilizing the component system effectively.

The provided code and documentation describe a system for managing agents using gRPC and protobufs. Here's a summary of the key components and functionalities:

### Key Classes and Functions

- **GrpcWorkerAgentRuntime**: This class is responsible for running remote or cross-language agents. It uses protobufs for agent messaging and supports shared protobuf schemas for cross-language communication.

- **GrpcWorkerAgentRuntimeHost**: Manages the server hosting the agent runtime, allowing it to start and stop in response to signals.

- **GrpcWorkerAgentRuntimeHostServicer**: A gRPC servicer that facilitates message delivery for agents.

- **Agent**: A protocol defining the basic interface for agents, including methods for handling messages, saving/loading state, and closing.

- **AgentId**: Represents a unique identifier for an agent instance, consisting of a type and key.

- **AgentProxy**: A helper class that allows interaction with an agent using its `AgentId`.

- **AgentRuntime**: A protocol defining the runtime environment for agents, including methods for sending messages, publishing messages, registering factories, and managing subscriptions.

### Key Methods

- **register_factory**: Registers an agent factory with the runtime. This is a low-level API, and it's recommended to use the agent class's `register` method for handling subscriptions automatically.

  ```python
  async def register_factory(
      self,
      type: str | AgentType,
      agent_factory: Callable[[], T | Awaitable[T]],
      *,
      expected_class: type[T] | None = None
  ) -> AgentType:
  ```

- **add_subscription**: Adds a new subscription for processing published messages.

  ```python
  async def add_subscription(self, subscription: Subscription) -> None:
  ```

- **remove_subscription**: Removes a subscription from the runtime.

  ```python
  async def remove_subscription(self, id: str) -> None:
  ```

- **send_message**: Sends a message to an agent and expects a response.

  ```python
  async def send_message(
      message: Any,
      recipient: AgentId,
      *,
      sender: AgentId | None = None,
      cancellation_token: CancellationToken | None = None,
      message_id: str | None = None
  ) -> Any:
  ```

- **publish_message**: Publishes a message to all agents in a given namespace without expecting responses.

  ```python
  async def publish_message(
      message: Any,
      topic_id: TopicId,
      *,
      sender: AgentId | None = None,
      cancellation_token: CancellationToken | None = None,
      message_id: str | None = None
  ) -> None:
  ```

- **try_get_underlying_agent_instance**: Attempts to retrieve the underlying agent instance by name and namespace.

  ```python
  async def try_get_underlying_agent_instance(
      id: AgentId,
      type: Type[T] = Agent
  ) -> T:
  ```

### Usage Example

Here's a simplified example of how to register an agent factory:

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, create a factory for it, and register the factory with the runtime.

### Class: `InMemoryStore`

The `InMemoryStore` class is a component that extends `CacheStore` and `Component`. It provides an in-memory storage solution with methods to get and set items.

- **Attributes:**
  - `component_provider_override`: Overrides the provider string to prevent internal module names from appearing in the module name.
  - `component_config_schema`: Alias for `InMemoryStoreConfig`.

- **Methods:**
  - ```python
    get(key: str, default: T | None = None) -> T | None
    ```
    Retrieves an item from the store using a key. Returns the default value if the key is not found.

  - ```python
    set(key: str, value: T) -> None
    ```
    Stores an item in the store under the specified key.

  - ```python
    _to_config() -> InMemoryStoreConfig
    ```
    Dumps the configuration needed to create a new instance of a component matching this instance's configuration.

  - ```python
    @classmethod
    _from_config(config: InMemoryStoreConfig) -> Self
    ```
    Creates a new instance of the component from a configuration object.

### Class: `CancellationToken`

A token used to cancel pending async calls.

- **Methods:**
  - ```python
    cancel() -> None
    ```
    Cancels pending async calls linked to this token.

  - ```python
    is_cancelled() -> bool
    ```
    Checks if the token has been used to cancel operations.

  - ```python
    add_callback(callback: Callable[[], None]) -> None
    ```
    Attaches a callback to be called when `cancel` is invoked.

  - ```python
    link_future(future: Future[Any]) -> Future[Any]
    ```
    Links a pending async call to a token for cancellation.

### Class: `AgentInstantiationContext`

Provides context for agent instantiation, allowing access to the current runtime and agent ID.

- **Example Usage:**
  ```python
  import asyncio
  from dataclasses import dataclass
  from autogen_core import (
      AgentId, AgentInstantiationContext, MessageContext, RoutedAgent,
      SingleThreadedAgentRuntime, message_handler
  )

  @dataclass
  class TestMessage:
      content: str

  class TestAgent(RoutedAgent):
      def __init__(self, description: str):
          super().__init__(description)
          _ = AgentInstantiationContext.current_runtime()
          agent_id = AgentInstantiationContext.current_agent_id()
          print(f"Current AgentID from constructor: {agent_id}")

      @message_handler
      async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
          print(f"Received message: {message.content}")

  async def main() -> None:
      runtime = SingleThreadedAgentRuntime()
      runtime.start()
      await runtime.register_factory("test_agent", test_agent_factory)
      await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))
      await runtime.stop()

  asyncio.run(main())
  ```

### Class: `SingleThreadedAgentRuntime`

A single-threaded agent runtime that processes messages using a single asyncio queue.

- **Methods:**
  - ```python
    async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any
    ```
    Sends a message to an agent and gets a response.

  - ```python
    async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None
    ```
    Publishes a message to all agents in the given namespace.

  - ```python
    async save_state() -> Mapping[str, Any]
    ```
    Saves the state of the entire runtime.

  - ```python
    async load_state(state: Mapping[str, Any]) -> None
    ```
    Loads the state of the entire runtime.

  - ```python
    start() -> None
    ```
    Starts the runtime message processing loop.

  - ```python
    async stop() -> None
    ```
    Immediately stops the runtime message processing loop.

  - ```python
    async stop_when_idle() -> None
    ```
    Stops the runtime when there are no outstanding messages.

- **Example Usage:**
  ```python
  import asyncio
  from dataclasses import dataclass
  from autogen_core import (
      AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
  )

  @dataclass
  class MyMessage:
      content: str

The provided documentation outlines various classes and methods related to agent runtime management, message handling, and component configuration. Here's a concise summary of the key elements:

### Agent Factory and Runtime Management

- **`agent_factory`**: A callable that creates an agent, using `autogen_core.AgentInstantiationContext` for accessing runtime variables like agent ID.

- **`expected_class`**: Optional type for runtime validation of the agent factory.

- **`GrpcWorkerAgentRuntime`**: Manages remote or cross-language agents using protobufs and CloudEvents. It includes methods to start, stop, and manage agent messaging.

### Asynchronous Methods

- **`async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`**: Retrieves an agent instance by ID, raising exceptions if not found or accessible.

- **`async add_subscription(subscription: Subscription) -> None`**: Adds a subscription for processing published messages.

- **`async remove_subscription(id: str) -> None`**: Removes a subscription, raising `LookupError` if it doesn't exist.

- **`async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) -> AgentId`**: Retrieves an agent ID based on various identifiers.

### Component Configuration

- **`Component`**: Base class for creating components with a Pydantic model for configuration and a logical type.

- **`ComponentModel`**: Pydantic model containing all information required to instantiate a component, including fields like `provider`, `component_type`, `version`, and `config`.

- **`ComponentLoader`**: Loads components from a model, with methods to handle configuration and versioning.

### Logging and Serialization

- **Loggers**: 
  - `ROOT_LOGGER_NAME`: `'autogen_core'`
  - `EVENT_LOGGER_NAME`: `'autogen_core.events'`
  - `TRACE_LOGGER_NAME`: `'autogen_core.trace'`

- **`add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`**: Adds message serializers to the runtime, deduplicating based on type and content.

### Intervention Handlers

- **`InterventionHandler`**: Modifies, logs, or drops messages processed by `AgentRuntime`. Methods include `on_send`, `on_publish`, and `on_response`.

- **`DefaultInterventionHandler`**: Provides default implementations for intervention methods, returning messages unchanged.

### Utility Classes

- **`QueueAsyncIterable`**: An async iterator for handling message queues.

- **`HostConnection`**: Manages gRPC connections, including sending and receiving messages.

### Example Usage

```python
from pydantic import BaseModel
from autogen_core import Component

class Config(BaseModel):
    value: str

class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)
```

This summary captures the essential functionality and usage patterns of the classes and methods described in the documentation.

The provided code and documentation describe an asynchronous system for managing and interacting with agents using gRPC and protobufs. Here's a summary of the key components and functionalities:

### Key Components

1. **AgentId**: Represents a unique identifier for an agent instance within a runtime. It includes a type and a key.

2. **AgentRuntime**: A protocol for managing agents, including sending messages, publishing messages, registering factories, and handling subscriptions.

3. **GrpcWorkerAgentRuntime**: A specific implementation of `AgentRuntime` for running remote or cross-language agents using gRPC.

4. **Agent**: A protocol that defines the basic structure and behavior of an agent, including handling messages and managing state.

5. **AgentProxy**: A helper class that allows interaction with an agent using its `AgentId`.

6. **GrpcWorkerAgentRuntimeHost**: Manages the server-side operations for hosting agents and handling gRPC requests.

### Key Functions

- **Message Handling**: 
  - `async def _process_request(self, request: agent_worker_pb2.RpcRequest) -> None`: Processes incoming RPC requests.
  - `async def _process_response(self, response: agent_worker_pb2.RpcResponse) -> None`: Processes responses to requests.
  - `async def _process_event(self, event: cloudevent_pb2.CloudEvent) -> None`: Handles incoming events.

- **Agent Management**:
  - `async def register_factory(self, type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`: Registers a factory for creating agents.
  - `async def _get_agent(self, agent_id: AgentId) -> Agent`: Retrieves an agent instance by its ID.
  - `async def try_get_underlying_agent_instance(self, id: AgentId, type: Type[T] = Agent) -> T`: Attempts to get the underlying agent instance.

- **Subscription Management**:
  - `async def add_subscription(self, subscription: Subscription) -> None`: Adds a new subscription.
  - `async def remove_subscription(self, id: str) -> None`: Removes a subscription.

- **State Management**:
  - `async def save_state(self) -> Mapping[str, Any]`: Saves the state of the runtime.
  - `async def load_state(self, state: Mapping[str, Any]) -> None`: Loads the state of the runtime.

- **Message Serialization**:
  - `def add_message_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`: Adds a message serializer to the runtime.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, register it with a runtime, and handle messages using the provided framework.

The provided documentation outlines various classes and methods for handling messages, agents, and subscriptions in an asynchronous environment. Below are key components and their functionalities:

### Message Handling

- **`on_message_impl(message: Any, ctx: MessageContext) -> Any`**: Abstract method to handle messages. Implementations should define how messages are processed.
- **`send_message(message: Any, recipient: AgentId, ...) -> Any`**: Sends a message to a specified agent and returns a response.
- **`publish_message(message: Any, topic_id: TopicId, ...) -> None`**: Publishes a message to a topic without expecting a response.

### State Management

- **`save_state() -> Mapping[str, Any]`**: Saves the current state of the agent, which must be JSON serializable.
- **`load_state(state: Mapping[str, Any]) -> None`**: Loads a previously saved state.

### Agent Registration

- **`register(runtime: AgentRuntime, type: str, factory: Callable, ...) -> AgentType`**: Registers a virtual subclass of an abstract base class (ABC) as an agent.

### Cache and Storage

- **`CacheStore`**: Defines the interface for cache operations with methods like `get` and `set`.
- **`InMemoryStore`**: An implementation of `CacheStore` for in-memory storage.

### Cancellation

- **`CancellationToken`**: Used to cancel pending asynchronous calls with methods like `cancel()` and `is_cancelled()`.

### Agent Context

- **`AgentInstantiationContext`**: Provides context for agent instantiation, allowing access to the current runtime and agent ID.

### Subscriptions

- **`Subscription`**: Defines topics an agent is interested in, with methods like `is_match` and `map_to_agent`.
- **`TypeSubscription`**: Matches topics based on type and maps to agents using the source as the key.

### Message Decorators

- **`message_handler`**: Decorator for generic message handlers in a `RoutedAgent` class.
- **`event`**: Decorator for event message handlers.
- **`rpc`**: Decorator for RPC message handlers.

### Runtime

- **`SingleThreadedAgentRuntime`**: A runtime that processes messages using a single asyncio queue. Suitable for development and standalone applications.

### Example Usage

```python
import asyncio
from dataclasses import dataclass
from autogen_core import (
    AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler
)

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()

asyncio.run(main())
```

This example demonstrates creating a runtime, registering an agent, sending a message, and stopping the runtime.

The provided documentation outlines various asynchronous functions and classes for managing agents and their interactions within a runtime environment. Here's a summary of key components and their functionalities:

### Functions

- **`async agent_metadata(agent: AgentId) -> AgentMetadata`**: Retrieves metadata for a specified agent.
- **`async agent_save_state(agent: AgentId) -> Mapping[str, Any]`**: Saves the state of a specified agent.
- **`async agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None`**: Loads a saved state into a specified agent.
- **`async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`**: Registers an agent factory with the runtime for a specific type.
- **`async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`**: Attempts to retrieve the underlying agent instance by name and namespace.
- **`async add_subscription(subscription: Subscription) -> None`**: Adds a new subscription for processing published messages.
- **`async remove_subscription(id: str) -> None`**: Removes a subscription from the runtime.
- **`async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) -> AgentId`**: Retrieves an agent by ID or type.
- **`add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`**: Adds a message serializer to the runtime.

### Classes

- **`Component`**: Base class for creating component classes with configuration schemas and types.
- **`ComponentBase`**: Base class for components with configuration loading capabilities.
- **`ComponentFromConfig`**: Handles creation of components from configuration objects.
- **`ComponentLoader`**: Loads components from models or dictionaries.
- **`ComponentModel`**: Pydantic model class for component instantiation.
- **`InterventionHandler`**: Protocol for modifying, logging, or dropping messages in the runtime.
- **`DefaultInterventionHandler`**: Provides default implementations for intervention handler methods.
- **`GrpcWorkerAgentRuntime`**: Manages remote or cross-language agents using gRPC and protobufs.

### Usage Examples

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

### Logging

- **`ROOT_LOGGER_NAME`**: 'autogen_core'
- **`EVENT_LOGGER_NAME`**: 'autogen_core.events'
- **`TRACE_LOGGER_NAME`**: 'autogen_core.trace'

This documentation provides a comprehensive guide to managing agent lifecycles, message handling, and runtime configurations in an asynchronous environment.

The `GrpcWorkerAgentRuntime` class is designed for running remote or cross-language agents using gRPC. It utilizes protobufs from `agent_worker.proto` and `CloudEvent` from `cloudevent.proto`. This runtime supports message serialization and deserialization, agent state management, and message publishing and subscription.

### Key Methods and Usage:

- **Message Handling:**
  - Messages can be serialized in JSON or Protobuf formats.
  - The runtime can publish messages to topics and send messages to specific agents.
  - Example of publishing a message:
    ```python
    async def publish_message(
        message: Any, 
        topic_id: TopicId, 
        sender: Optional[AgentId] = None, 
        cancellation_token: Optional[CancellationToken] = None, 
        message_id: Optional[str] = None
    ) -> None:
    ```

- **Agent Management:**
  - Agents can be registered with a factory function.
  - Example of registering an agent factory:
    ```python
    async def register_factory(
        type: str | AgentType, 
        agent_factory: Callable[[], T | Awaitable[T]], 
        expected_class: Optional[type[T]] = None
    ) -> AgentType:
    ```

- **State Management:**
  - The runtime can save and load the state of agents.
  - Example of saving state:
    ```python
    async def save_state() -> Mapping[str, Any]:
    ```

- **Subscription Management:**
  - Subscriptions can be added or removed to manage message delivery.
  - Example of adding a subscription:
    ```python
    async def add_subscription(subscription: Subscription) -> None:
    ```

- **Runtime Control:**
  - The runtime can be started and stopped, and it can handle signals for graceful shutdown.
  - Example of starting the runtime:
    ```python
    async def start() -> None:
    ```

### Additional Features:

- **Serialization Registry:**
  - Allows adding custom serializers for message types.
  - Example:
    ```python
    def add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None:
    ```

- **Error Handling:**
  - The runtime includes mechanisms for handling undeliverable messages and exceptions during message processing.

- **Telemetry:**
  - Integrates with telemetry systems to trace message processing and agent interactions.

This runtime is suitable for scenarios where agents need to communicate across different languages or systems, leveraging gRPC for efficient message passing.

The `GrpcWorkerAgentRuntime` class is designed for running remote or cross-language agents using gRPC. It leverages protobufs for agent messaging and CloudEvents for event handling. This runtime supports various operations such as starting and stopping the runtime, sending and publishing messages, managing agent factories, and handling subscriptions.

### Key Methods

- **Initialization**: 
  ```python
  class GrpcWorkerAgentRuntime(
      host_address: str, 
      tracer_provider: TracerProvider | None = None, 
      extra_grpc_config: Sequence[Tuple[str, Any]] | None = None, 
      payload_serialization_format: str = JSON_DATA_CONTENT_TYPE
  )
  ```

- **Message Handling**:
  - `async send_message(message: Any, recipient: AgentId, ...) -> Any`: Sends a message to an agent and expects a response.
  - `async publish_message(message: Any, topic_id: TopicId, ...) -> None`: Publishes a message to all agents in a given namespace without expecting responses.

- **Runtime Control**:
  - `async start() -> None`: Starts the runtime in a background task.
  - `async stop() -> None`: Stops the runtime immediately.
  - `async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`: Stops the runtime when a specified signal is received.

- **Agent Management**:
  - `async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], ...) -> AgentType`: Registers an agent factory with the runtime.
  - `async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`: Attempts to retrieve the underlying agent instance by name and namespace.

- **State Management**:
  - `async save_state() -> Mapping[str, Any]`: Saves the state of the entire runtime.
  - `async load_state(state: Mapping[str, Any]) -> None`: Loads the state of the entire runtime.

- **Subscription Management**:
  - `async add_subscription(subscription: Subscription) -> None`: Adds a new subscription for processing published messages.
  - `async remove_subscription(id: str) -> None`: Removes a subscription from the runtime.

- **Serialization**:
  - `add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`: Adds a new message serialization serializer to the runtime.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, register it with the runtime, and handle events. The `GrpcWorkerAgentRuntime` provides a robust framework for managing agents and their interactions in a distributed environment.

The provided documentation outlines various classes and methods related to a runtime and agent system, focusing on message handling, agent registration, and state management. Below are key components and their functionalities:

### RuntimeHost
- **Constructor**: `RuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)`
- **Methods**:
  - `start()`: Starts the server in a background task.
  - `async stop(grace: int = 5)`: Stops the server.
  - `async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT))`: Stops the server upon receiving a signal.

### GrpcWorkerAgentRuntimeHostServicer
- A gRPC servicer for hosting message delivery services for agents.
- **Methods**:
  - `async AddSubscription(request, context)`: Adds a subscription.
  - `async GetSubscriptions(request, context)`: Retrieves subscriptions.
  - `async OpenChannel(request_iterator, context)`: Opens a message channel.
  - `async OpenControlChannel(request_iterator, context)`: Opens a control channel.
  - `async RegisterAgent(request, context)`: Registers an agent.
  - `async RemoveSubscription(request, context)`: Removes a subscription.

### Agent
- **Properties**:
  - `metadata`: Metadata of the agent.
  - `id`: ID of the agent.
- **Methods**:
  - `async on_message(message: Any, ctx: MessageContext)`: Handles messages.
  - `async save_state()`: Saves the agent's state.
  - `async load_state(state: Mapping[str, Any])`: Loads the agent's state.
  - `async close()`: Called when the runtime is closed.

### AgentRuntime
- **Methods**:
  - `async send_message(message: Any, recipient: AgentId, ...)`: Sends a message to an agent.
  - `async publish_message(message: Any, topic_id: TopicId, ...)`: Publishes a message to a topic.
  - `async register_factory(type: str | AgentType, agent_factory, ...)`: Registers an agent factory.
  - `async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent)`: Retrieves an agent instance.
  - `async save_state()`: Saves the runtime state.
  - `async load_state(state: Mapping[str, Any])`: Loads the runtime state.

### AgentProxy
- A helper class to use an `AgentId` in place of its associated `Agent`.
- **Methods**:
  - `async send_message(message: Any, ...)`: Sends a message via the proxy.
  - `async save_state()`: Saves the state of the agent.
  - `async load_state(state: Mapping[str, Any])`: Loads the state of the agent.

### Message Handling Decorators
- **message_handler**: Decorator for generic message handlers.
- **event**: Decorator for event message handlers.
- **rpc**: Decorator for RPC message handlers.

### Additional Classes
- **AgentId**: Represents a unique identifier for an agent.
- **AgentMetadata**: Contains metadata for an agent.
- **CancellationToken**: Used to cancel pending async calls.
- **ClosureAgent**: Allows defining an agent using a closure.
- **InMemoryStore**: An in-memory implementation of a cache store.
- **MessageContext**: Provides context for a message, including sender and topic information.
- **Subscription**: Defines topics an agent is interested in.

These components form a comprehensive framework for managing agents, handling messages, and maintaining state within a distributed runtime environment.

The provided documentation outlines various classes and functions related to a message routing and agent runtime system. Here's a concise summary of the key components and their functionalities:

### Classes and Functions

- **FunctionCall**: Represents a function call with `id`, `arguments`, and `name` attributes.

- **TypeSubscription**: Matches topics based on type and maps them to agents using the topic's source as the agent key. Each source has its own agent instance.
  ```python
  subscription = TypeSubscription(topic_type="t1", agent_type="a1")
  ```

- **DefaultSubscription**: A default subscription for applications needing global agent scope. Uses "default" topic type and attempts to detect the agent type.
  ```python
  subscription = DefaultSubscription()
  ```

- **DefaultTopicId**: Provides default values for `type` and `source` fields of a `TopicId`.
  ```python
  topic_id = DefaultTopicId()
  ```

- **SingleThreadedAgentRuntime**: A single-threaded runtime for processing messages using an asyncio queue. Suitable for development and standalone applications.
  ```python
  runtime = SingleThreadedAgentRuntime()
  ```

- **InterventionHandler**: A protocol for modifying, logging, or dropping messages processed by the runtime.
  ```python
  class MyInterventionHandler(DefaultInterventionHandler):
      async def on_send(self, message, *, message_context, recipient):
          # Modify message
          return message
  ```

- **Component**: Base class for creating components with a configuration schema and type.
  ```python
  class MyComponent(Component[Config]):
      component_type = "custom"
      component_config_schema = Config
  ```

- **HostConnection**: Manages a gRPC connection to a host, handling message sending and receiving.
  ```python
  connection = await HostConnection.from_host_address("localhost:50051")
  ```

### Key Methods

- **is_match**: Checks if a `topic_id` matches a subscription.
- **map_to_agent**: Maps a `topic_id` to an agent.
- **send_message**: Sends a message to an agent and gets a response.
- **publish_message**: Publishes a message to all agents in a namespace.
- **save_state** / **load_state**: Save and load the state of the runtime or agents.
- **register_factory**: Registers an agent factory with the runtime.
- **add_subscription** / **remove_subscription**: Manage subscriptions in the runtime.

### Usage Examples

- **Creating a Runtime and Registering an Agent**:
  ```python
  runtime = SingleThreadedAgentRuntime()
  await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
  runtime.start()
  await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
  await runtime.stop()
  ```

- **Publishing a Message**:
  ```python
  await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
  await runtime.stop_when_idle()
  ```

This system is designed for message routing and agent management, providing a framework for handling messages in a structured and scalable way.

The provided code and documentation describe a `GrpcWorkerAgentRuntime` class, which is part of a system for running remote or cross-language agents using gRPC and protobufs. Here's a summary of the key components and functionalities:

### Key Components

- **GrpcWorkerAgentRuntime**: This class extends `AgentRuntime` and is designed to manage agents that communicate using gRPC and protobufs. It handles message serialization, agent registration, and message processing.

- **Message Handling**: The runtime can send, receive, and process messages using protobufs. It supports both direct messages and published messages to topics.

- **Agent Management**: The runtime can register agent factories, instantiate agents, and manage their lifecycle. It supports loading and saving agent states, although these methods are not yet implemented.

- **Subscriptions**: The runtime can add and remove subscriptions, allowing agents to receive messages published to specific topics.

### Key Methods

- ```python
  async def start(self) -> None
  ```
  Starts the runtime in a background task, establishing a connection to the host.

- ```python
  async def stop(self) -> None
  ```
  Stops the runtime, ensuring all background tasks are completed and the host connection is closed.

- ```python
  async def send_message(self, message: Any, recipient: AgentId, ...) -> Any
  ```
  Sends a message to a specific agent.

- ```python
  async def publish_message(self, message: Any, topic_id: TopicId, ...) -> None
  ```
  Publishes a message to all agents subscribed to a given topic.

- ```python
  async def register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], ...) -> AgentType
  ```
  Registers an agent factory with the runtime, associating it with a specific type.

- ```python
  async def add_subscription(self, subscription: Subscription) -> None
  ```
  Adds a new subscription for processing published messages.

- ```python
  async def remove_subscription(self, id: str) -> None
  ```
  Removes a subscription by its ID.

### Usage Example

Here's a simplified example of how to register an agent factory:

```python
from autogen_core import AgentRuntime, RoutedAgent, MessageContext, event
from autogen_core.models import UserMessage

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...  # Initialize runtime
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, create a factory for it, and register the factory with the runtime. The runtime then manages the lifecycle and communication of the agent instances.

Here's a summary of the key functions and classes in the provided documentation:

### Functions

- **`async remove_subscription(id: str) -> None`**
  - Removes a subscription from the runtime.
  - **Parameters**: `id` (str) â€“ ID of the subscription to remove.
  - **Raises**: `LookupError` if the subscription does not exist.

- **`async save_state() -> Mapping[str, Any]`**
  - Saves the state of the entire runtime, including all hosted agents.
  - **Returns**: A JSON serializable object representing the saved state.

- **`async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`**
  - Sends a message to an agent and gets a response.
  - **Parameters**: 
    - `message` (Any) â€“ The message to send.
    - `recipient` (AgentId) â€“ The agent to send the message to.
    - `sender` (optional) â€“ The agent which sent the message.
    - `cancellation_token` (optional) â€“ Token used to cancel an in-progress operation.
  - **Raises**: 
    - `CantHandleException` if the recipient cannot handle the message.
    - `UndeliverableException` if the message cannot be delivered.
  - **Returns**: The response from the agent.

- **`async start() -> None`**
  - Starts the runtime in a background task.

- **`async stop() -> None`**
  - Stops the runtime immediately.

- **`async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`**
  - Stops the runtime when a signal is received.

- **`async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`**
  - Tries to get the underlying agent instance by name and namespace.
  - **Parameters**: 
    - `id` (AgentId) â€“ The agent ID.
    - `type` (optional) â€“ The expected type of the agent.
  - **Returns**: The concrete agent instance.
  - **Raises**: 
    - `LookupError` if the agent is not found.
    - `NotAccessibleError` if the agent is not accessible.
    - `TypeError` if the agent is not of the expected type.

### Classes

- **`GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)`**
  - Manages the gRPC server for agent runtime.

- **`GrpcWorkerAgentRuntimeHostServicer`**
  - A gRPC servicer that hosts message delivery service for agents.

### Usage

The functions and classes are part of a system designed to manage agent runtimes, handle messaging, and maintain state. The `GrpcWorkerAgentRuntimeHost` and `GrpcWorkerAgentRuntimeHostServicer` classes are used to set up and manage gRPC servers and services for agent communication. The async functions provide mechanisms to start, stop, and interact with the runtime and agents.

The provided code and documentation describe a system for managing agents using gRPC, focusing on agent factories, subscriptions, and message handling. Below are key components and functions:

### Key Functions and Classes

#### Agent Factory Registration
```python
async def register_factory(
    type: str | AgentType, 
    agent_factory: Callable[[], T | Awaitable[T]], 
    *, 
    expected_class: type[T] | None = None
) -> AgentType:
    # Registers an agent factory with the runtime.
```
- **Parameters**:
  - `type`: A unique identifier for the agent type.
  - `agent_factory`: A callable that creates an agent instance.
  - `expected_class`: Optional; used for runtime validation.

#### Agent Instantiation
```python
async def _invoke_agent_factory(
    self, 
    agent_factory: Callable[[], T | Awaitable[T]] | Callable[[AgentRuntime, AgentId], T | Awaitable[T]], 
    agent_id: AgentId
) -> T:
    # Invokes the agent factory to create an agent instance.
```
- Handles both zero-argument and two-argument factories, with a deprecation warning for the latter.

#### Agent Retrieval
```python
async def _get_agent(self, agent_id: AgentId) -> Agent:
    # Retrieves an agent instance by its ID.
```
- Checks if the agent is already instantiated or if its type is registered.

#### Subscription Management
```python
async def add_subscription(self, subscription: Subscription) -> None:
    # Adds a new subscription to the runtime.

async def remove_subscription(self, id: str) -> None:
    # Removes a subscription from the runtime.
```
- Manages subscriptions for message processing.

#### Message Handling
```python
async def send_message(
    message: Any, 
    recipient: AgentId, 
    *, 
    sender: AgentId | None = None, 
    cancellation_token: CancellationToken | None = None, 
    message_id: str | None = None
) -> Any:
    # Sends a message to an agent and gets a response.
```
- **Parameters**:
  - `message`: The message to send.
  - `recipient`: The target agent.
  - `sender`: Optional; the agent sending the message.
  - `cancellation_token`: Optional; for canceling the operation.
  - `message_id`: Optional; a unique identifier for the message.

#### State Management
```python
async def save_state() -> Mapping[str, Any]:
    # Saves the state of the entire runtime.

async def load_state(state: Mapping[str, Any]) -> None:
    # Loads the state of the entire runtime.
```
- Manages the state of agents and the runtime.

### Classes

#### `GrpcWorkerAgentRuntime`
- Manages remote or cross-language agents using gRPC.
- Utilizes protobufs for messaging.

#### `Agent`
- Represents an agent with methods for handling messages and managing state.

#### `AgentId`
- Uniquely identifies an agent instance within a runtime.

### Usage Example
```python
from autogen_core import AgentRuntime, RoutedAgent, event

class MyAgent(RoutedAgent):
    @event
    async def handler(self, message, context):
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main():
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```
- Demonstrates registering an agent factory and handling events.

Here's a summary of the provided documentation, focusing on key classes, methods, and their functionalities:

### Key Classes and Methods

#### `BaseAgent`
- **Properties**:
  - `metadata`: `AgentMetadata`
  - `type`: `str`
  - `id`: `AgentId`
  - `runtime`: `AgentRuntime`
- **Methods**:
  - `on_message(message: Any, ctx: MessageContext)`: Handles messages for the agent.
  - `send_message(message: Any, recipient: AgentId, ...)`: Sends a message to another agent.
  - `publish_message(message: Any, topic_id: TopicId, ...)`: Publishes a message to a topic.
  - `save_state()`: Saves the agent's state.
  - `load_state(state: Mapping[str, Any])`: Loads the agent's state.
  - `close()`: Called when the runtime is closed.
  - `register(runtime: AgentRuntime, type: str, factory: Callable, ...)`: Registers a virtual subclass of an ABC.

#### `CacheStore`
- **Methods**:
  - `get(key: str, default: T | None = None)`: Retrieves an item from the store.
  - `set(key: str, value: T)`: Sets an item in the store.

#### `InMemoryStore`
- Inherits from `CacheStore`.
- **Methods**:
  - `get(key: str, default: T | None = None)`: Retrieves an item.
  - `set(key: str, value: T)`: Sets an item.
  - `_to_config()`: Dumps the configuration.
  - `_from_config(config: InMemoryStoreConfig)`: Creates an instance from a configuration.

#### `CancellationToken`
- **Methods**:
  - `cancel()`: Cancels pending async calls.
  - `is_cancelled()`: Checks if the token has been used.
  - `add_callback(callback: Callable)`: Attaches a callback for cancellation.
  - `link_future(future: Future[Any])`: Links a future to allow cancellation.

#### `AgentInstantiationContext`
- **Methods**:
  - `current_runtime()`: Returns the current runtime.
  - `current_agent_id()`: Returns the current agent ID.

#### `TopicId`
- **Methods**:
  - `from_str(topic_id: str)`: Converts a string to a `TopicId`.

#### `Subscription`
- **Methods**:
  - `is_match(topic_id: TopicId)`: Checks if a topic ID matches the subscription.
  - `map_to_agent(topic_id: TopicId)`: Maps a topic ID to an agent.

#### `MessageContext`
- **Attributes**:
  - `sender`: `AgentId | None`
  - `topic_id`: `TopicId | None`
  - `is_rpc`: `bool`
  - `cancellation_token`: `CancellationToken`
  - `message_id`: `str`

#### `SingleThreadedAgentRuntime`
- **Methods**:
  - `send_message(message: Any, recipient: AgentId, ...)`: Sends a message to an agent.
  - `publish_message(message: Any, topic_id: TopicId, ...)`: Publishes a message.
  - `save_state()`: Saves the runtime state.
  - `load_state(state: Mapping[str, Any])`: Loads the runtime state.
  - `process_next()`: Processes the next message in the queue.
  - `start()`: Starts the runtime.

### Decorators

#### `message_handler`
- Decorates methods in a `RoutedAgent` class to handle messages.
- **Parameters**:
  - `func`: The function to be decorated.
  - `strict`: Raises an exception if the message type is not in the target types.
  - `match`: A function for secondary routing.

#### `event`
- Decorates methods to handle event messages.

#### `rpc`
- Decorates methods to handle RPC messages.

### Example Usage

```python
import asyncio
from dataclasses import dataclass
from autogen_core import SingleThreadedAgentRuntime, MessageContext, RoutedAgent, message_handler

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")

async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()

asyncio.run(main())
```

This summary provides an overview of the main components and

The `autogen_core` library provides a framework for managing agent runtimes, message processing, and component configuration. Below are key functionalities and examples of usage:

### Runtime Management

- **Start and Stop Runtime**: The `SingleThreadedAgentRuntime` can be started and stopped using the `start()` and `stop()` methods.
  ```python
  import asyncio
  from autogen_core import SingleThreadedAgentRuntime

  async def main() -> None:
      runtime = SingleThreadedAgentRuntime()
      runtime.start()
      # ... do other things ...
      await runtime.stop()

  asyncio.run(main())
  ```

- **Stopping Conditions**: Various methods are available to stop the runtime based on different conditions:
  - `stop()`: Immediately stops the runtime.
  - `stop_when_idle()`: Stops when no messages are being processed.
  - `stop_when(condition)`: Stops when a specified condition is met (not recommended due to inefficiency).

### Agent Management

- **Agent Metadata and State**: Retrieve and manage agent metadata and state.
  ```python
  async def agent_metadata(agent: AgentId) -> AgentMetadata
  async def agent_save_state(agent: AgentId) -> Mapping[str, Any]
  async def agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None
  ```

- **Registering Factories**: Register agent factories with the runtime.
  ```python
  async def register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType
  ```

### Message Handling

- **Intervention Handlers**: Customize message processing with intervention handlers.
  ```python
  class MyInterventionHandler(DefaultInterventionHandler):
      async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> Any:
          if isinstance(message, MyMessage):
              message.content = message.content.upper()
          return message
  ```

### Component Configuration

- **Component Class**: Define components with specific configurations using Pydantic models.
  ```python
  from pydantic import BaseModel
  from autogen_core import Component

  class Config(BaseModel):
      value: str

  class MyComponent(Component[Config]):
      component_type = "custom"
      component_config_schema = Config

      def __init__(self, value: str):
          self.value = value

      def _to_config(self) -> Config:
          return Config(value=self.value)

      @classmethod
      def _from_config(cls, config: Config) -> MyComponent:
          return cls(value=config.value)
  ```

### Logging

- **Logger Names**: Use predefined logger names for structured logging.
  ```python
  ROOT_LOGGER_NAME = 'autogen_core'
  EVENT_LOGGER_NAME = 'autogen_core.events'
  TRACE_LOGGER_NAME = 'autogen_core.trace'
  ```

### Advanced Usage

- **gRPC Worker Runtime**: For remote or cross-language agents, use `GrpcWorkerAgentRuntime`.
  ```python
  class GrpcWorkerAgentRuntime(AgentRuntime):
      async def start(self) -> None:
          # Start the runtime in a background task
          ...
      async def stop(self) -> None:
          # Stop the runtime immediately
          ...
  ```

This framework provides a robust structure for managing asynchronous agent-based systems, allowing for flexible configuration, message handling, and runtime management.

The provided code is part of a system for managing agent runtimes using gRPC and protobufs. It includes several asynchronous functions and classes for handling agent communication, state management, and message serialization. Below are some key components and their functionalities:

### Key Functions and Classes

- **`GrpcWorkerAgentRuntime`**: A class for running remote or cross-language agents using gRPC. It handles agent messaging with protobufs and CloudEvents.

- **`add_message_serializer`**: Adds a new message serializer to the runtime. This helps in deduplicating serializers based on `type_name` and `data_content_type`.

- **`add_subscription`**: Adds a new subscription for processing published messages.

- **`agent_load_state`**: Loads the state of a single agent.

- **`agent_metadata`**: Retrieves metadata for a specific agent.

- **`agent_save_state`**: Saves the state of a single agent.

- **`get`**: Retrieves an agent by ID or type, with options for lazy loading.

- **`load_state`**: Loads the state of the entire runtime, including all hosted agents.

- **`publish_message`**: Publishes a message to all agents in a given namespace.

- **`register_factory`**: Registers an agent factory with the runtime. This is a low-level API, and the agent classâ€™s `register` method is usually preferred.

- **`remove_subscription`**: Removes a subscription from the runtime.

- **`save_state`**: Saves the state of the entire runtime.

- **`send_message`**: Sends a message to an agent and expects a response.

- **`start`**: Starts the runtime in a background task.

- **`stop`**: Stops the runtime immediately.

- **`try_get_underlying_agent_instance`**: Attempts to get the underlying agent instance by name and namespace.

### Example Usage

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

### Error Handling

- **`CantHandleException`**: Raised when a handler canâ€™t process the exception.
- **`MessageDroppedException`**: Raised when a message is dropped.
- **`NotAccessibleError`**: Raised when trying to access a value that is not accessible.
- **`UndeliverableException`**: Raised when a message canâ€™t be delivered.

This system is designed to facilitate the management and communication of agents in a distributed environment, leveraging gRPC for efficient message passing and state management.

The `GrpcWorkerAgentRuntime` class is designed for running remote or cross-language agents using gRPC and protobufs. It facilitates agent messaging and event handling, supporting both synchronous and asynchronous operations. Below are key functionalities and methods provided by this class:

### Initialization
```python
def __init__(self, host_address: str, tracer_provider: TracerProvider | None = None, extra_grpc_config: ChannelArgumentType | None = None, payload_serialization_format: str = JSON_DATA_CONTENT_TYPE) -> None
```
- Initializes the runtime with a host address, optional tracer provider, gRPC configuration, and serialization format.

### Messaging
- **Send Message:**
  ```python
  async def send_message(self, message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any
  ```
  Sends a message to a specified agent.

- **Publish Message:**
  ```python
  async def publish_message(self, message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None
  ```
  Publishes a message to all agents in a given namespace.

### State Management
- **Save State:**
  ```python
  async def save_state(self) -> Mapping[str, Any]
  ```
  Saves the state of the entire runtime.

- **Load State:**
  ```python
  async def load_state(self, state: Mapping[str, Any]) -> None
  ```
  Loads the state of the entire runtime.

### Agent Management
- **Register Factory:**
  ```python
  async def register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType
  ```
  Registers an agent factory with the runtime.

- **Get Agent:**
  ```python
  async def get(self, id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) -> AgentId
  ```
  Retrieves an agent by ID or type.

### Subscription Management
- **Add Subscription:**
  ```python
  async def add_subscription(self, subscription: Subscription) -> None
  ```
  Adds a new subscription for processing published messages.

- **Remove Subscription:**
  ```python
  async def remove_subscription(self, id: str) -> None
  ```
  Removes a subscription from the runtime.

### Runtime Control
- **Start:**
  ```python
  async def start(self) -> None
  ```
  Starts the runtime in a background task.

- **Stop:**
  ```python
  async def stop(self) -> None
  ```
  Stops the runtime immediately.

- **Stop When Signal:**
  ```python
  async def stop_when_signal(self, signals: Sequence[signal.Signals] = (signal.SIGTERM, signal.SIGINT)) -> None
  ```
  Stops the runtime when a specified signal is received.

### Additional Features
- **Add Message Serializer:**
  ```python
  def add_message_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None
  ```
  Adds a new message serialization serializer to the runtime.

This class is essential for managing agent communication and lifecycle in a distributed system, leveraging gRPC for efficient cross-language operations.

The provided documentation outlines the implementation of a gRPC-based agent runtime system. Below are the key components and functionalities:

### Parameters and Exceptions

- **Parameters:**
  - `message (Any)`: The message to send.
  - `recipient (AgentId)`: The agent to send the message to.
  - `sender (AgentId | None, optional)`: The agent which sent the message. Defaults to `None`.
  - `cancellation_token (CancellationToken | None, optional)`: Token used to cancel an in-progress operation. Defaults to `None`.

- **Exceptions:**
  - `CantHandleException`: Raised if the recipient cannot handle the message.
  - `UndeliverableException`: Raised if the message cannot be delivered.
  - `Other`: Any other exception raised by the recipient.

- **Returns:**
  - `Any`: The response from the agent.

### Key Functions

- **Runtime Control:**
  - ```python
    async start() -> None
    ```
    Starts the runtime in a background task.

  - ```python
    async stop() -> None
    ```
    Stops the runtime immediately.

  - ```python
    async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None
    ```
    Stops the runtime when a specified signal is received.

- **Agent Management:**
  - ```python
    async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T
    ```
    Attempts to retrieve the underlying agent instance by name and namespace.

### Classes

- **GrpcWorkerAgentRuntimeHost:**
  - Manages the gRPC server for agent communication.
  - Methods include `start()`, `stop(grace: int = 5)`, and `stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT))`.

- **GrpcWorkerAgentRuntimeHostServicer:**
  - A gRPC servicer that hosts message delivery services for agents.
  - Methods include `AddSubscription`, `GetSubscriptions`, `OpenChannel`, `OpenControlChannel`, `RegisterAgent`, and `RemoveSubscription`.

### Example Usage

To start the runtime:
```python
async def main():
    runtime = GrpcWorkerAgentRuntimeHost(address="localhost:50051")
    await runtime.start()
```

To stop the runtime:
```python
async def shutdown():
    await runtime.stop()
```

This documentation provides a comprehensive overview of the gRPC-based agent runtime, detailing how to manage runtime operations and agent interactions.

The provided code is part of a system for managing agents in a gRPC-based runtime environment. Here's a summary of the key components and functionalities:

### Agent Factory Invocation
The code handles the invocation of agent factories based on the number of parameters they accept:
- **0 Parameters**: The factory is called directly.
- **2 Parameters**: A warning is issued about deprecation, and the factory is called with `AgentRuntime` and `AgentId`.
- **Other**: Raises a `ValueError`.

### Agent Management
- **`_get_agent`**: Retrieves an agent by `AgentId`, either from instantiated agents or by invoking the appropriate factory.
- **`try_get_underlying_agent_instance`**: Attempts to get an agent instance by `AgentId` and type, raising errors if not found or mismatched.

### Subscription Management
- **`add_subscription`**: Adds a subscription to the runtime, sending a request to the host connection.
- **`remove_subscription`**: Removes a subscription by ID, also interacting with the host connection.

### Message Handling
- **`send_message`**: Sends a message to a specific agent and awaits a response.
- **`publish_message`**: Publishes a message to a topic, with no response expected.

### Runtime Control
- **`start`**: Initiates the runtime, establishing a connection to the host.
- **`stop`**: Stops the runtime and cleans up tasks and connections.
- **`stop_when_signal`**: Stops the runtime upon receiving specific signals.

### Serialization and Deserialization
- **`add_message_serializer`**: Adds serializers to handle message serialization, ensuring deduplication.

### gRPC Connection
- **`HostConnection`**: Manages the gRPC channel and message queues for communication with the host.
- **`QueueAsyncIterable`**: Provides an async iterable interface for message queues.

### Example Usage
```python
async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())
```

### Classes and Methods
- **`GrpcWorkerAgentRuntime`**: Main class for managing the agent runtime, handling agent lifecycle, messaging, and subscriptions.
- **`HostConnection`**: Handles the gRPC connection, including sending and receiving messages.

This system is designed for running remote or cross-language agents, using protobufs for messaging and supporting both JSON and Protobuf serialization formats.

The provided code is a comprehensive implementation of a gRPC-based agent runtime, designed to facilitate communication between agents using protobufs and CloudEvents. Below are key components and functionalities:

### Key Functions and Methods

- **State Management**
  - `_state(self, agent: AgentId, state: Mapping[str, Any]) -> None`: Raises `NotImplementedError` for agent state loading.
  - `save_state(self) -> Mapping[str, Any]`: Raises `NotImplementedError` for saving state.
  - `load_state(self, state: Mapping[str, Any]) -> None`: Raises `NotImplementedError` for loading state.

- **Request and Response Handling**
  - `_get_new_request_id(self) -> str`: Asynchronously increments and returns a new request ID.
  - `_process_request(self, request: agent_worker_pb2.RpcRequest) -> None`: Processes incoming requests, deserializes messages, and invokes the appropriate agent.
  - `_process_response(self, response: agent_worker_pb2.RpcResponse) -> None`: Handles responses, deserializes results, and sets them on the corresponding future.
  - `_process_event(self, event: cloudevent_pb2.CloudEvent) -> None`: Processes CloudEvents, deserializes messages, and sends them to subscribed agents.

- **Agent Management**
  - `register_factory(self, type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`: Registers an agent factory.
  - `_invoke_agent_factory(self, agent_factory: Callable[[], T | Awaitable[T]] | Callable[[AgentRuntime, AgentId], T | Awaitable[T]], agent_id: AgentId) -> T`: Invokes an agent factory to create an agent instance.
  - `_get_agent(self, agent_id: AgentId) -> Agent`: Retrieves or creates an agent instance.
  - `try_get_underlying_agent_instance(self, id: AgentId, type: Type[T] = Agent) -> T`: Attempts to get an agent instance of a specific type.

- **Subscription Management**
  - `add_subscription(self, subscription: Subscription) -> None`: Adds a subscription to the local manager and sends a request to the host.
  - `remove_subscription(self, id: str) -> None`: Removes a subscription from the local manager and sends a request to the host.

- **Message Sending**
  - `send_message(self, message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`: Sends a message to a specific agent.
  - `publish_message(self, message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None`: Publishes a message to a topic.

- **Runtime Control**
  - `start(self) -> None`: Starts the runtime in a background task.
  - `stop(self) -> None`: Stops the runtime immediately.
  - `stop_when_signal(self, signals: Sequence[signal.Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`: Stops the runtime when a specified signal is received.

### Classes

- **QueueAsyncIterable**: An async iterator for handling queues.
- **HostConnection**: Manages the gRPC connection to the host, including sending and receiving messages.
- **GrpcWorkerAgentRuntime**: The main class for managing agent runtime, handling requests, responses, events, and agent lifecycle.

### Usage

The runtime is designed to be started and stopped programmatically, handling agent communication and lifecycle management. It supports asynchronous operations, making it suitable for high-concurrency environments.

### Notes

- The code includes several `NotImplementedError` placeholders for future implementation of state management and agent metadata retrieval.
- The runtime uses a serialization registry to handle message serialization and deserialization, supporting JSON and Protobuf formats.
- The implementation includes detailed logging for debugging and tracing message flow.

The `GrpcWorkerAgentRuntime` class is designed for running remote or cross-language agents using gRPC. It utilizes protobufs for agent messaging and supports shared protobuf schemas for cross-language communication. Below are key methods and their functionalities:

### Methods

- **`_save_state(self, agent: AgentId) -> Mapping[str, Any]`**: Raises `NotImplementedError`. Intended to save the state of a single agent.

- **`agent_load_state(self, agent: AgentId, state: Mapping[str, Any]) -> None`**: Raises `NotImplementedError`. Intended to load the state of a single agent.

- **`_get_new_request_id(self) -> str`**: Generates a new request ID by incrementing an internal counter.

- **`_process_request(self, request: agent_worker_pb2.RpcRequest) -> None`**: Processes incoming RPC requests, deserializes messages, and calls the appropriate agent's message handler.

- **`_process_response(self, response: agent_worker_pb2.RpcResponse) -> None`**: Handles responses from agents, deserializes results, and sets them on the corresponding future.

- **`_process_event(self, event: cloudevent_pb2.CloudEvent) -> None`**: Processes incoming events, deserializes messages, and sends them to subscribed agents.

- **`register_factory(self, type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`**: Registers an agent factory with the runtime, ensuring the type is unique.

- **`_invoke_agent_factory(self, agent_factory: Callable[[], T | Awaitable[T]] | Callable[[AgentRuntime, AgentId], T | Awaitable[T]], agent_id: AgentId) -> T`**: Invokes an agent factory to create an agent instance.

- **`_get_agent(self, agent_id: AgentId) -> Agent`**: Retrieves an agent instance by its ID, invoking the factory if necessary.

- **`try_get_underlying_agent_instance(self, id: AgentId, type: Type[T] = Agent) -> T`**: Attempts to get the underlying agent instance by ID and type, raising exceptions if not found or accessible.

- **`add_subscription(self, subscription: Subscription) -> None`**: Adds a new subscription to the runtime.

- **`remove_subscription(self, id: str) -> None`**: Removes a subscription by its ID.

- **`get(self, id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) -> AgentId`**: Retrieves an agent ID based on the provided type or ID.

- **`add_message_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`**: Adds a message serializer to the runtime, deduplicating based on type name and content type.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent and register it with the runtime using a factory function.

The `GrpcWorkerAgentRuntime` class is an agent runtime designed for running remote or cross-language agents. It uses protobufs from `agent_worker.proto` and `CloudEvent` from `cloudevent.proto` for agent messaging. This runtime supports various operations such as starting and stopping the runtime, sending and publishing messages, managing subscriptions, and handling agent state.

### Key Methods

- **`start()`**: Starts the runtime in a background task.
- **`stop()`**: Stops the runtime immediately.
- **`stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT))`**: Stops the runtime when a specified signal is received.
- **`send_message(message: Any, recipient: AgentId, ...) -> Any`**: Sends a message to an agent and gets a response.
- **`publish_message(message: Any, topic_id: TopicId, ...) -> None`**: Publishes a message to all agents in a given namespace.
- **`register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], ...) -> AgentType`**: Registers an agent factory with the runtime.
- **`add_subscription(subscription: Subscription) -> None`**: Adds a new subscription for processing published messages.
- **`remove_subscription(id: str) -> None`**: Removes a subscription from the runtime.
- **`save_state() -> Mapping[str, Any]`**: Saves the state of the entire runtime.
- **`load_state(state: Mapping[str, Any]) -> None`**: Loads the state of the entire runtime.
- **`try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`**: Attempts to get the underlying agent instance by name and namespace.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

### Additional Classes

- **`GrpcWorkerAgentRuntimeHost`**: Manages the server hosting the runtime.
- **`GrpcWorkerAgentRuntimeHostServicer`**: A gRPC servicer that hosts message delivery services for agents.

This runtime is suitable for scenarios where agents need to communicate across different languages or systems, leveraging gRPC and protobuf for efficient message handling.

The provided code is part of a gRPC-based runtime for managing agents, which are components that can send and receive messages. The code includes several classes and functions for handling agent registration, message sending, and subscription management. Below are some key components and their functionalities:

### Key Classes and Functions

- **`QueueAsyncIterable`**: Implements an asynchronous iterator over a queue, allowing for asynchronous iteration over messages.

- **`HostConnection`**: Manages the connection to a gRPC host. It includes methods for sending and receiving messages, and for establishing a connection from a host address.

  ```python
  async def from_host_address(cls, host_address: str, extra_grpc_config: ChannelArgumentType = DEFAULT_GRPC_CONFIG) -> Self
  ```

- **`GrpcWorkerAgentRuntime`**: Represents the runtime environment for agents. It handles agent registration, message processing, and subscription management.

  - **`start`**: Begins the runtime in a background task.
  
    ```python
    async def start(self) -> None
    ```

  - **`stop`**: Stops the runtime immediately.
  
    ```python
    async def stop(self) -> None
    ```

  - **`send_message`**: Sends a message to a specified agent.
  
    ```python
    async def send_message(self, message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any
    ```

  - **`publish_message`**: Publishes a message to a topic.
  
    ```python
    async def publish_message(self, message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None
    ```

  - **`register_factory`**: Registers a factory for creating agents of a specific type.
  
    ```python
    async def register_factory(self, type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType
    ```

  - **`add_subscription`**: Adds a subscription to the runtime.
  
    ```python
    async def add_subscription(self, subscription: Subscription) -> None
    ```

  - **`remove_subscription`**: Removes a subscription by ID.
  
    ```python
    async def remove_subscription(self, id: str) -> None
    ```

### Usage

The runtime is designed to facilitate communication between agents using gRPC. Agents can be registered, and messages can be sent or published to topics. Subscriptions allow agents to listen for specific types of messages.

### Note

The code includes several TODOs and unimplemented methods, indicating areas for future development, such as state saving/loading and agent metadata handling. Additionally, the code relies on protobufs for message serialization and deserialization, and it uses OpenTelemetry for tracing.

The `GrpcWorkerAgentRuntime` class is part of the `autogen_ext.runtimes.grpc` module, designed for running remote or cross-language agents using gRPC. It utilizes protobufs for agent messaging and requires shared protobuf schemas for cross-language communication.

### Key Methods and Usage

- **add_message_serializer**: Adds a message serializer to the runtime. This method deduplicates serializers based on `type_name` and `data_content_type`.
  ```python
  def add_message_serializer(self, serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None:
      self._serialization_registry.add_serializer(serializer)
  ```

- **add_subscription**: Adds a new subscription for processing published messages.
  ```python
  async def add_subscription(self, subscription: Subscription) -> None:
  ```

- **agent_load_state**: Loads the state of a single agent.
  ```python
  async def agent_load_state(self, agent: AgentId, state: Mapping[str, Any]) -> None:
  ```

- **agent_metadata**: Retrieves metadata for a specific agent.
  ```python
  async def agent_metadata(self, agent: AgentId) -> AgentMetadata:
  ```

- **agent_save_state**: Saves the state of a single agent.
  ```python
  async def agent_save_state(self, agent: AgentId) -> Mapping[str, Any]:
  ```

- **get**: Retrieves an agent by ID or type.
  ```python
  async def get(self, id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) -> AgentId:
  ```

- **load_state**: Loads the state of the entire runtime.
  ```python
  async def load_state(self, state: Mapping[str, Any]) -> None:
  ```

- **publish_message**: Publishes a message to all agents in a given namespace.
  ```python
  async def publish_message(self, message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None:
  ```

- **register_factory**: Registers an agent factory with the runtime.
  ```python
  async def register_factory(self, type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType:
  ```

- **remove_subscription**: Removes a subscription from the runtime.
  ```python
  async def remove_subscription(self, id: str) -> None:
  ```

- **save_state**: Saves the state of the entire runtime.
  ```python
  async def save_state(self) -> Mapping[str, Any]:
  ```

- **send_message**: Sends a message to an agent and gets a response.
  ```python
  async def send_message(self, message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any:
  ```

- **start**: Starts the runtime in a background task.
  ```python
  async def start(self) -> None:
  ```

- **stop**: Stops the runtime immediately.
  ```python
  async def stop(self) -> None:
  ```

- **stop_when_signal**: Stops the runtime when a signal is received.
  ```python
  async def stop_when_signal(self, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None:
  ```

- **try_get_underlying_agent_instance**: Attempts to get the underlying agent instance by name and namespace.
  ```python
  async def try_get_underlying_agent_instance(self, id: AgentId, type: Type[T] = Agent) -> T:
  ```

### Example Usage

```python
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This class and its methods provide a comprehensive framework for managing agent lifecycles, message serialization, and communication in a distributed system using gRPC.

The provided code is a detailed implementation of a gRPC-based agent runtime, designed to handle remote or cross-language agents. Below is a summary of the key components and functionalities:

### Key Classes and Functions

- **GrpcWorkerAgentRuntime**: This class extends `AgentRuntime` and is responsible for managing the lifecycle of agents, handling message serialization, and managing subscriptions. It uses gRPC for communication and supports both JSON and Protobuf serialization formats.

- **HostConnection**: Manages the gRPC connection to the host, including sending and receiving messages. It uses a queue-based approach to handle message streams.

- **QueueAsyncIterable**: A utility class that allows asynchronous iteration over a queue, used for handling incoming and outgoing messages.

- **start()**: Initializes the runtime, establishes a connection to the host, and starts the read loop to process incoming messages.

- **stop()**: Stops the runtime, ensuring all background tasks are completed and the connection is closed.

- **register_factory()**: Registers an agent factory with the runtime, allowing for the creation of agents of a specific type.

- **add_subscription()**: Adds a new subscription to the runtime, enabling it to process messages for specific topics.

- **remove_subscription()**: Removes a subscription from the runtime.

- **send_message()**: Sends a message to a specific agent and waits for a response.

- **publish_message()**: Publishes a message to all agents subscribed to a specific topic.

- **try_get_underlying_agent_instance()**: Attempts to retrieve the underlying agent instance by its ID, with type checking.

### Usage Example

```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dataclass
class MyMessage:
    content: str

class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received:", message.content)

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())

import asyncio
asyncio.run(main())
```

This example demonstrates how to define an agent, register it with the runtime, and handle incoming messages. The runtime manages the lifecycle and communication of agents, providing a robust framework for building distributed systems.

The provided code and documentation describe a gRPC-based agent runtime system for managing and communicating with agents. Below are key components and functionalities:

### Key Components

- **GrpcWorkerAgentRuntime**: This class is responsible for running remote or cross-language agents using gRPC. It supports message serialization, agent registration, and message handling.

- **GrpcWorkerAgentRuntimeHost**: This class manages the gRPC server, allowing it to start and stop, and handles incoming requests.

- **GrpcWorkerAgentRuntimeHostServicer**: A gRPC servicer that facilitates message delivery for agents.

### Important Methods

- **`send_message`**: Sends a message to a specified agent and awaits a response.
  ```python
  async def send_message(self, message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any
  ```

- **`publish_message`**: Publishes a message to all agents subscribed to a specific topic.
  ```python
  async def publish_message(self, message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None
  ```

- **`register_factory`**: Registers an agent factory with the runtime.
  ```python
  async def register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType
  ```

- **`add_subscription`**: Adds a new subscription for processing published messages.
  ```python
  async def add_subscription(self, subscription: Subscription) -> None
  ```

- **`remove_subscription`**: Removes a subscription from the runtime.
  ```python
  async def remove_subscription(self, id: str) -> None
  ```

- **`start` and `stop`**: Methods to start and stop the runtime.
  ```python
  async def start() -> None
  async def stop() -> None
  ```

- **`save_state` and `load_state`**: Methods to save and load the state of the runtime and agents.
  ```python
  async def save_state() -> Mapping[str, Any]
  async def load_state(state: Mapping[str, Any]) -> None
  ```

### Exception Handling

- **CantHandleException**: Raised when a handler cannot process a message.
- **UndeliverableException**: Raised when a message cannot be delivered.
- **NotAccessibleError**: Raised when trying to access a value that is not accessible, such as a remote agent.

### Usage Example

Here's a simplified example of how to register an agent factory and send a message:

```python
from autogen_core import AgentRuntime, AgentId

async def my_agent_factory():
    return MyAgent()

async def main():
    runtime = AgentRuntime(...)
    await runtime.register_factory("my_agent", my_agent_factory)
    agent_id = await runtime.get("my_agent")
    response = await runtime.send_message("Hello", agent_id)

import asyncio
asyncio.run(main())
```

This system is designed to facilitate communication between agents using gRPC, allowing for scalable and flexible agent management.

# `autogen_ext.runtimes.grpc` Documentation Summary

## Overview
The `autogen_ext.runtimes.grpc` module provides classes and methods for running remote or cross-language agents using gRPC. It supports agent messaging with protobufs and CloudEvents, and facilitates cross-language communication by requiring shared protobuf schemas.

## Classes and Methods

### `GrpcWorkerAgentRuntime`
- **Constructor**: 
  ```python
  GrpcWorkerAgentRuntime(host_address: str, tracer_provider: TracerProvider | None = None, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None, payload_serialization_format: str = JSON_DATA_CONTENT_TYPE)
  ```
  - Initializes the runtime for remote agents.

- **Methods**:
  - `add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) -> None`: Adds a message serializer.
  - `async add_subscription(subscription: Subscription) -> None`: Adds a subscription for processing messages.
  - `async agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None`: Loads the state of a single agent.
  - `async agent_metadata(agent: AgentId) -> AgentMetadata`: Retrieves metadata for an agent.
  - `async agent_save_state(agent: AgentId) -> Mapping[str, Any]`: Saves the state of a single agent.
  - `async load_state(state: Mapping[str, Any]) -> None`: Loads the entire runtime state.
  - `async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> None`: Publishes a message to agents.
  - `async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) -> AgentType`: Registers an agent factory.
  - `async remove_subscription(id: str) -> None`: Removes a subscription.
  - `async save_state() -> Mapping[str, Any]`: Saves the entire runtime state.
  - `async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) -> Any`: Sends a message to an agent and gets a response.
  - `async start() -> None`: Starts the runtime.
  - `async stop() -> None`: Stops the runtime.
  - `async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`: Stops the runtime upon receiving a signal.
  - `async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T`: Attempts to get the underlying agent instance.

### `GrpcWorkerAgentRuntimeHost`
- **Constructor**:
  ```python
  GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)
  ```
  - Sets up the server for hosting agents.

- **Methods**:
  - `start() -> None`: Starts the server in a background task.
  - `async stop(grace: int = 5) -> None`: Stops the server.
  - `async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`: Stops the server when a signal is received.

### `GrpcWorkerAgentRuntimeHostServicer`
- A gRPC servicer for hosting message delivery services for agents.

- **Methods**:
  - `async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) -> AddSubscriptionResponse`
  - `async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) -> GetSubscriptionsResponse`
  - `async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) -> AsyncIterator[Message]`
  - `async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) -> AsyncIterator[ControlMessage]`
  - `async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) -> RegisterAgentTypeResponse`
  - `async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) -> RemoveSubscriptionResponse`

## Usage Example
```python
from dataclasses import dataclass
from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage

@dat

### `try_get_underlying_agent_instance`

```python
try_get_underlying_agent_instance(
    id: AgentId, 
    type: Type[T] = Agent
) -> T
```

Attempts to retrieve the underlying agent instance by its name and namespace. This function is generally discouraged but can be useful in specific scenarios. If the agent is not accessible, an exception is raised.

**Parameters:**
- `id (AgentId)`: The agent ID.
- `type (Type[T], optional)`: The expected type of the agent. Defaults to `Agent`.

**Returns:**
- `T`: The concrete agent instance.

**Raises:**
- `LookupError`: If the agent is not found.
- `NotAccessibleError`: If the agent is not accessible, e.g., if it is located remotely.
- `TypeError`: If the agent is not of the expected type.

### `GrpcWorkerAgentRuntimeHost`

```python
class GrpcWorkerAgentRuntimeHost(
    address: str, 
    extra_grpc_config: Sequence[Tuple[str, Any]] | None = None
)
```

A class to host a gRPC server for agent runtime.

**Methods:**

- `start() -> None`: Start the server in a background task.
- `async stop(grace: int = 5) -> None`: Stop the server.
- `async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None`: Stop the server when a signal is received.

### `GrpcWorkerAgentRuntimeHostServicer`

```python
class GrpcWorkerAgentRuntimeHostServicer
```

A gRPC servicer that hosts message delivery service for agents.

**Methods:**

- `async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) -> AddSubscriptionResponse`
- `async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) -> GetSubscriptionsResponse`
- `async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) -> AsyncIterator[Message]`
- `async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) -> AsyncIterator[ControlMessage]`
- `async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) -> RegisterAgentTypeResponse`
- `async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) -> RemoveSubscriptionResponse`

These methods are part of the gRPC servicer and handle various agent-related operations, though specific documentation comments are missing in the `.proto` file.

# `GrpcWorkerAgentRuntime` Class

The `GrpcWorkerAgentRuntime` class is designed for running remote or cross-language agents using gRPC. It extends the `AgentRuntime` class and facilitates agent messaging through protobufs from `agent_worker.proto` and `CloudEvent` from `cloudevent.proto`. Cross-language agents require shared protobuf schemas for message types exchanged between agents.

## Initialization

```python
class GrpcWorkerAgentRuntime(
    host_address: str,
    tracer_provider: TracerProvider | None = None,
    extra_grpc_config: Sequence[Tuple[str, Any]] | None = None,
    payload_serialization_format: str = JSON_DATA_CONTENT_TYPE
)
```

### Parameters:
- `host_address`: The address of the host.
- `tracer_provider`: Optional tracer provider for tracing.
- `extra_grpc_config`: Additional gRPC configuration.
- `payload_serialization_format`: Format for payload serialization, default is `JSON_DATA_CONTENT_TYPE`.

## Methods

### `add_message_serializer`

```python
add_message_serializer(
    serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]
) -> None
```

Adds a new message serialization serializer to the runtime. Deduplicates based on `type_name` and `data_content_type`.

### `async add_subscription`

```python
async add_subscription(subscription: Subscription) -> None
```

Adds a new subscription for processing published messages.

### `async agent_load_state`

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None
```

Loads the state of a single agent.

### `async agent_metadata`

```python
async agent_metadata(agent: AgentId) -> AgentMetadata
```

Retrieves metadata for an agent.

### `async agent_save_state`

```python
async agent_save_state(agent: AgentId) -> Mapping[str, Any]
```

Saves the state of a single agent.

### `async load_state`

```python
async load_state(state: Mapping[str, Any]) -> None
```

Loads the state of the entire runtime, including all hosted agents.

### `async publish_message`

```python
async publish_message(
    message: Any,
    topic_id: TopicId,
    *,
    sender: AgentId | None = None,
    cancellation_token: CancellationToken | None = None,
    message_id: str | None = None
) -> None
```

Publishes a message to all agents in the given namespace. Raises `UndeliverableException` if the message cannot be delivered.

### `async register_factory`

```python
async register_factory(
    type: str | AgentType,
    agent_factory: Callable[[], T | Awaitable[T]],
    *,
    expected_class: type[T] | None = None
) -> AgentType
```

Registers an agent factory with the runtime. The type must be unique. This API does not add any subscriptions.

### `async remove_subscription`

```python
async remove_subscription(id: str) -> None
```

Removes a subscription from the runtime. Raises `LookupError` if the subscription does not exist.

### `async save_state`

```python
async save_state() -> Mapping[str, Any]
```

Saves the state of the entire runtime, including all hosted agents.

### `async send_message`

```python
async send_message(
    message: Any,
    recipient: AgentId,
    *,
    sender: AgentId | None = None,
    cancellation_token: CancellationToken | None = None,
    message_id: str | None = None
) -> Any
```

Sends a message to an agent and expects a response. Raises `CantHandleException`, `UndeliverableException`, or other exceptions raised by the recipient.

### `async start`

```python
async start() -> None
```

Starts the runtime in a background task.

### `async stop`

```python
async stop() -> None
```

Stops the runtime immediately.

### `async stop_when_signal`

```python
async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) -> None
```

Stops the runtime when a specified signal is received.

### `async try_get_underlying_agent_instance`

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) -> T
```

Attempts to get the underlying agent instance by name and namespace. Raises `LookupError`, `NotAccessibleError`, or `TypeError` if the agent is not found, not accessible, or not of the expected type, respectively.

### Function: `get_response`

This function is used to send a message to an agent and receive a response.

#### Parameters:
- `message (Any)`: The message to send.
- `recipient (AgentId)`: The agent to send the message to.
- `sender (AgentId | None, optional)`: The agent which sent the message. Defaults to `None` if sent externally.
- `cancellation_token (CancellationToken | None, optional)`: Token to cancel an in-progress operation. Defaults to `None`.

#### Raises:
- `CantHandleException`: If the recipient cannot handle the message.
- `UndeliverableException`: If the message cannot be delivered.
- `Other`: Any other exception raised by the recipient.

#### Returns:
- `Any`: The response from the agent.

### Function: `start`

```python
async start() â†’ None
```
Starts the runtime in a background task.

### Function: `stop`

```python
async stop() â†’ None
```
Stops the runtime immediately.

### Function: `stop_when_signal`

```python
async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) â†’ None
```
Stops the runtime when a specified signal is received.

### Function: `try_get_underlying_agent_instance`

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) â†’ T
```
Attempts to get the underlying agent instance by name and namespace.

#### Parameters:
- `id (AgentId)`: The agent id.
- `type (Type[T], optional)`: The expected type of the agent. Defaults to `Agent`.

#### Returns:
- `T`: The concrete agent instance.

#### Raises:
- `LookupError`: If the agent is not found.
- `NotAccessibleError`: If the agent is not accessible.
- `TypeError`: If the agent is not of the expected type.

### Class: `GrpcWorkerAgentRuntimeHost`

```python
class GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)
```
A class to host the gRPC server for agent runtime.

#### Methods:
- `start() â†’ None`: Starts the server in a background task.
- `async stop(grace: int = 5) â†’ None`: Stops the server.
- `async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) â†’ None`: Stops the server when a signal is received.

### Class: `GrpcWorkerAgentRuntimeHostServicer`

A gRPC servicer that hosts message delivery service for agents.

#### Methods:
- `async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) â†’ AddSubscriptionResponse`
- `async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) â†’ GetSubscriptionsResponse`
- `async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) â†’ AsyncIterator[Message]`
- `async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) â†’ AsyncIterator[ControlMessage]`
- `async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) â†’ RegisterAgentTypeResponse`
- `async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) â†’ RemoveSubscriptionResponse`

The provided code is a detailed implementation of a gRPC-based messaging system for agents, using Python's `asyncio` for asynchronous operations. Here's a summary of the key components and functionalities:

### Key Classes and Functions

1. **ChannelConnection**:
   - A generic class for managing a connection channel, handling message sending and receiving.
   - Uses an asynchronous queue to manage outgoing messages.
   - Implements `__aiter__` and `__anext__` for asynchronous iteration over messages.

2. **CallbackChannelConnection**:
   - Inherits from `ChannelConnection`.
   - Uses a callback function to handle received messages.

3. **GrpcWorkerAgentRuntimeHostServicer**:
   - A gRPC servicer class that manages message delivery for agents.
   - Manages data and control connections using dictionaries keyed by `ClientConnectionId`.
   - Handles agent registration, message processing, and subscription management.

4. **OpenChannel and OpenControlChannel**:
   - Asynchronous methods to open data and control channels, respectively.
   - Establish connections and manage message flow between clients and the server.

5. **Message Processing**:
   - Methods like `_receive_message`, `_process_request`, `_process_response`, and `_process_event` handle different types of messages and events.
   - Uses `asyncio` tasks to process messages concurrently.

6. **Subscription Management**:
   - Methods for adding, removing, and retrieving subscriptions.
   - Uses a `SubscriptionManager` to manage subscriptions.

7. **Error Handling**:
   - Implements error handling for message delivery failures and invalid operations.
   - Uses logging to track and report errors and important events.

### Usage Example

```python
async def OpenChannel(self, request_iterator, context):
    client_id = await get_client_id_or_abort(context)
    async def handle_callback(message):
        await self._receive_message(client_id, message)
    connection = CallbackChannelConnection(request_iterator, client_id, handle_callback=handle_callback)
    self._data_connections[client_id] = connection
    try:
        async for message in connection:
            yield message
    finally:
        del self._data_connections[client_id]
        await self._on_client_disconnect(client_id)
```

This code snippet demonstrates how a channel is opened, messages are processed, and connections are managed. The system is designed to handle asynchronous communication efficiently, making it suitable for distributed agent-based applications.

```python
def _raise_on_exception(self, task: Task[Any]) -> None:
    exception = task.exception()
    if exception is not None:
        raise exception

async def _receive_message(self, client_id: ClientConnectionId, message: agent_worker_pb2.Message) -> None:
    logger.info(f"Received message from client {client_id}: {message}")
    oneofcase = message.WhichOneof("message")
    match oneofcase:
        case "request":
            request: agent_worker_pb2.RpcRequest = message.request
            task = asyncio.create_task(self._process_request(request, client_id))
            self._background_tasks.add(task)
            task.add_done_callback(self._raise_on_exception)
            task.add_done_callback(self._background_tasks.discard)
        case "response":
            response: agent_worker_pb2.RpcResponse = message.response
            task = asyncio.create_task(self._process_response(response, client_id))
            self._background_tasks.add(task)
            task.add_done_callback(self._raise_on_exception)
            task.add_done_callback(self._background_tasks.discard)
        case "cloudEvent":
            task = asyncio.create_task(self._process_event(message.cloudEvent))
            self._background_tasks.add(task)
            task.add_done_callback(self._raise_on_exception)
            task.add_done_callback(self._background_tasks.discard)
        case None:
            logger.warning("Received empty message")

async def _receive_control_message(self, client_id: ClientConnectionId, message: agent_worker_pb2.ControlMessage) -> None:
    logger.info(f"Received message from client {client_id}: {message}")
    destination = message.destination
    if destination.startswith("agentid="):
        agent_id = AgentId.from_str(destination[len("agentid="):])
        target_client_id = self._agent_type_to_client_id.get(agent_id.type)
        if target_client_id is None:
            logger.error(f"Agent client id not found for agent type {agent_id.type}.")
            return
    elif destination.startswith("clientid="):
        target_client_id = destination[len("clientid="):]
    else:
        logger.error(f"Invalid destination {destination}")
        return

    target_send_queue = self._control_connections.get(target_client_id)
    if target_send_queue is None:
        logger.error(f"Client {target_client_id} not found, failed to deliver message.")
        return

    await target_send_queue.send(message)

async def _process_request(self, request: agent_worker_pb2.RpcRequest, client_id: ClientConnectionId) -> None:
    async with self._agent_type_to_client_id_lock:
        target_client_id = self._agent_type_to_client_id.get(request.target.type)
    if target_client_id is None:
        logger.error(f"Agent {request.target.type} not found, failed to deliver message.")
        return

    target_send_queue = self._data_connections.get(target_client_id)
    if target_send_queue is None:
        logger.error(f"Client {target_client_id} not found, failed to deliver message.")
        return

    await target_send_queue.send(agent_worker_pb2.Message(request=request))

    future = asyncio.get_event_loop().create_future()
    self._pending_responses.setdefault(target_client_id, {})[request.request_id] = future

    send_response_task = asyncio.create_task(self._wait_and_send_response(future, client_id))
    self._background_tasks.add(send_response_task)
    send_response_task.add_done_callback(self._raise_on_exception)
    send_response_task.add_done_callback(self._background_tasks.discard)

async def _wait_and_send_response(self, future: Future[agent_worker_pb2.RpcResponse], client_id: ClientConnectionId) -> None:
    response = await future
    message = agent_worker_pb2.Message(response=response)
    send_queue = self._data_connections.get(client_id)
    if send_queue is None:
        logger.error(f"Client {client_id} not found, failed to send response message.")
        return
    await send_queue.send(message)

async def _process_response(self, response: agent_worker_pb2.RpcResponse, client_id: ClientConnectionId) -> None:
    future = self._pending_responses[client_id].pop(response.request_id)
    future.set_result(response)

async def _process_event(self, event: cloudevent_pb2.CloudEvent) -> None:
    topic_id = TopicId(type=event.type, source=event.source)
    recipients = await self._subscription_manager.get_subscribed_recipients(topic_id)
    async with self._agent_type_to_client_id_lock:
        client_ids: Set[ClientConnectionId] = set()
        for recipient in recipients:
            client_id = self._agent_type_to_client_id.get(recipient.type)
            if client_id is not None:
                client_ids.add(client_id)
            else:
                logger.error(f"

The provided code and documentation describe a gRPC-based system for managing agent runtimes and subscriptions. Here's a summary of the key components and functions:

### gRPC Servicer Functions

- **RegisterAgent**: Registers an agent type with a client ID. If the agent type is already registered, it aborts with an `INVALID_ARGUMENT` error.
  ```python
  async def RegisterAgent(
      self,
      request: agent_worker_pb2.RegisterAgentTypeRequest,
      context: grpc.aio.ServicerContext[
          agent_worker_pb2.RegisterAgentTypeRequest,
          agent_worker_pb2.RegisterAgentTypeResponse
      ],
  ) -> agent_worker_pb2.RegisterAgentTypeResponse:
      # Implementation details...
  ```

- **AddSubscription**: Adds a subscription for a client. If the subscription is invalid, it aborts with an `INVALID_ARGUMENT` error.
  ```python
  async def AddSubscription(
      self,
      request: agent_worker_pb2.AddSubscriptionRequest,
      context: grpc.aio.ServicerContext[
          agent_worker_pb2.AddSubscriptionRequest,
          agent_worker_pb2.AddSubscriptionResponse
      ],
  ) -> agent_worker_pb2.AddSubscriptionResponse:
      # Implementation details...
  ```

- **RemoveSubscription**: Removes a subscription by ID.
  ```python
  async def RemoveSubscription(
      self,
      request: agent_worker_pb2.RemoveSubscriptionRequest,
      context: grpc.aio.ServicerContext[
          agent_worker_pb2.RemoveSubscriptionRequest,
          agent_worker_pb2.RemoveSubscriptionResponse
      ],
  ) -> agent_worker_pb2.RemoveSubscriptionResponse:
      # Implementation details...
  ```

- **GetSubscriptions**: Retrieves all subscriptions for a client.
  ```python
  async def GetSubscriptions(
      self,
      request: agent_worker_pb2.GetSubscriptionsRequest,
      context: grpc.aio.ServicerContext[
          agent_worker_pb2.GetSubscriptionsRequest,
          agent_worker_pb2.GetSubscriptionsResponse
      ],
  ) -> agent_worker_pb2.GetSubscriptionsResponse:
      # Implementation details...
  ```

### GrpcWorkerAgentRuntime Class

- **add_message_serializer**: Adds a message serializer to the runtime.
  ```python
  def add_message_serializer(
      serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]
  ) -> None:
      # Implementation details...
  ```

- **add_subscription**: Adds a new subscription to the runtime.
  ```python
  async def add_subscription(subscription: Subscription) -> None:
      # Implementation details...
  ```

- **agent_load_state**: Loads the state of a single agent.
  ```python
  async def agent_load_state(agent: AgentId, state: Mapping[str, Any]) -> None:
      # Implementation details...
  ```

- **agent_metadata**: Retrieves metadata for an agent.
  ```python
  async def agent_metadata(agent: AgentId) -> AgentMetadata:
      # Implementation details...
  ```

- **agent_save_state**: Saves the state of a single agent.
  ```python
  async def agent_save_state(agent: AgentId) -> Mapping[str, Any]:
      # Implementation details...
  ```

- **publish_message**: Publishes a message to a topic.
  ```python
  async def publish_message(
      message: Any, topic_id: TopicId, *, sender: AgentId | None = None
  ) -> None:
      # Implementation details...
  ```

- **register_factory**: Registers an agent factory with the runtime.
  ```python
  async def register_factory(
      type: str | AgentType,
      agent_factory: Callable[[], T | Awaitable[T]],
      *,
      expected_class: type[T] | None = None
  ) -> AgentType:
      # Implementation details...
  ```

- **remove_subscription**: Removes a subscription from the runtime.
  ```python
  async def remove_subscription(id: str) -> None:
      # Implementation details...
  ```

- **save_state**: Saves the state of the entire runtime.
  ```python
  async def save_state() -> Mapping[str, Any]:
      # Implementation details...
  ```

- **send_message**: Sends a message to an agent and gets a response.
  ```python
  async def send_message(
      message: Any, recipient: AgentId, *, sender: AgentId | None = None
  ) -> Any:
      # Implementation details...
  ```

- **start**: Starts the runtime in a background task.
  ```python
  async def start() -> None:
      # Implementation details...
  ```

- **stop**: Stops the runtime immediately.
  ```python
  async def stop() -> None:
      # Implementation details...
  ```

This system is designed to manage agent communication and state using gRPC, allowing for remote or cross-language agent interactions

### Function Summaries

#### `publish_message`
```python
def publish_message(
    message: Any, 
    topic: TopicId, 
    sender: AgentId | None = None, 
    cancellation_token: CancellationToken | None = None, 
    message_id: str | None = None
) -> None
```
Publishes a message to all agents in a specified namespace. No responses are expected. Raises `UndeliverableException` if the message cannot be delivered.

#### `register_factory`
```python
async def register_factory(
    type: str | AgentType, 
    agent_factory: Callable[[], T | Awaitable[T]], 
    *, 
    expected_class: type[T] | None = None
) -> AgentType
```
Registers an agent factory with the runtime for a specific type. The type must be unique. This is a low-level API; the agent classâ€™s `register` method is usually preferred.

**Example:**
```python
from autogen_core import AgentRuntime

async def my_agent_factory():
    return MyAgent()

async def main() -> None:
    runtime: AgentRuntime = ...
    await runtime.register_factory("my_agent", lambda: MyAgent())
```

#### `remove_subscription`
```python
async def remove_subscription(id: str) -> None
```
Removes a subscription from the runtime. Raises `LookupError` if the subscription does not exist.

#### `save_state`
```python
async def save_state() -> Mapping[str, Any]
```
Saves the state of the entire runtime, including all hosted agents. Returns a JSON serializable object representing the state.

#### `send_message`
```python
async def send_message(
    message: Any, 
    recipient: AgentId, 
    *, 
    sender: AgentId | None = None, 
    cancellation_token: CancellationToken | None = None, 
    message_id: str | None = None
) -> Any
```
Sends a message to an agent and gets a response. Raises `CantHandleException`, `UndeliverableException`, or other exceptions if the recipient cannot handle the message.

#### `start`
```python
async def start() -> None
```
Starts the runtime in a background task.

#### `stop`
```python
async def stop() -> None
```
Stops the runtime immediately.

#### `stop_when_signal`
```python
async def stop_when_signal(
    signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)
) -> None
```
Stops the runtime when a specified signal is received.

#### `try_get_underlying_agent_instance`
```python
async def try_get_underlying_agent_instance(
    id: AgentId, 
    type: Type[T] = Agent
) -> T
```
Attempts to get the underlying agent instance by name and namespace. Raises `LookupError`, `NotAccessibleError`, or `TypeError` if the agent is not found or accessible.

### Class Summaries

#### `GrpcWorkerAgentRuntimeHost`
```python
class GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)
```
Starts and stops the server in a background task.

#### `GrpcWorkerAgentRuntimeHostServicer`
A gRPC servicer that hosts message delivery service for agents. Includes methods like `AddSubscription`, `GetSubscriptions`, `OpenChannel`, `OpenControlChannel`, `RegisterAgent`, and `RemoveSubscription`.

### Creating Your Own Extension

With the new package structure in 0.4, creating and publishing an extension to the AutoGen ecosystem is simplified. Follow best practices for naming, implementing common interfaces, specifying dependencies, and using type hints. For discovery, add relevant topics to your GitHub repo. Changes from version 0.2 encourage separate package publication for flexibility and reduced maintenance.