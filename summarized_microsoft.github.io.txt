# AgentChat — AutoGen

AgentChat is a high-level API for building multi-agent applications, built on top of the `autogen-core` package. It provides intuitive defaults like Agents with preset behaviors and Teams with predefined multi-agent design patterns. It's recommended for beginners, while advanced users might prefer `autogen-core` for more flexibility.

## Installation

To install AgentChat, use a virtual environment to isolate dependencies:

### Using `venv`

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### Using `conda`

```bash
conda create -n autogen python=3.12
conda activate autogen
```

### Install with `pip`

```bash
pip install -U "autogen-agentchat"
```

### Install OpenAI for Model Client

```bash
pip install "autogen-ext[openai]"
```

For Azure OpenAI with AAD authentication:

```bash
pip install "autogen-ext[azure]"
```

## Quickstart

To build applications quickly using preset agents, install the necessary packages:

```bash
pip install -U "autogen-agentchat" "autogen-ext[openai,azure]"
```

Example of creating a single agent using an OpenAI model:

```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o")

async def get_weather(city: str) -> str:
    return f"The weather in {city} is 73 degrees and Sunny."

agent = AssistantAgent(
    name="weather_agent",
    model_client=model_client,
    tools=[get_weather],
    system_message="You are a helpful assistant.",
    reflect_on_tool_use=True,
    model_client_stream=True,
)

async def main() -> None:
    await Console(agent.run_stream(task="What is the weather in New York?"))

await main()
```

## Custom Agents

Create custom agents by inheriting from `BaseChatAgent` and implementing:

- `on_messages()`: Defines agent behavior in response to messages.
- `on_reset()`: Resets the agent to its initial state.
- `produced_message_types`: List of possible `ChatMessage` types.

Example of a `CountDownAgent`:

```python
from typing import AsyncGenerator, List, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage
from autogen_core import CancellationToken

class CountDownAgent(BaseChatAgent):
    def __init__(self, name: str, count: int = 3):
        super().__init__(name, "A simple agent that counts down.")
        self._count = count

    async def on_messages_stream(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:
        for i in range(self._count, 0, -1):
            msg = TextMessage(content=f"{i}...", source=self.name)
            yield msg
        yield Response(chat_message=TextMessage(content="Done!", source=self.name))

async def run_countdown_agent() -> None:
    countdown_agent = CountDownAgent("countdown")
    async for message in countdown_agent.on_messages_stream([], CancellationToken()):
        print(message.content)

await run_countdown_agent()
```

## Selector Group Chat

`SelectorGroupChat` allows multi-agent coordination through a shared context and a centralized, customizable selector. It features model-based speaker selection, configurable participant roles, and customizable selection prompting.

For more detailed guides and examples, refer to the respective sections in the documentation.

### Model-Based Selection with `SelectorGroupChat`

`SelectorGroupChat` is a high-level API for managing group chats with a model-based speaker selection mechanism. It is similar to `RoundRobinGroupChat` but uses a model to determine the next speaker based on the conversation context.

#### How It Works

1. **Analyze Context**: The team analyzes the conversation context, including history and participant attributes, to select the next speaker. By default, the same speaker is not selected consecutively unless it's the only option. This can be changed with `allow_repeated_speaker=True`.

2. **Prompt Response**: The selected agent provides a response, which is broadcasted to all participants.

3. **Check Termination**: The conversation continues until a termination condition is met, after which a `TaskResult` with the conversation history is returned.

4. **Context Persistence**: The conversation context is retained for subsequent tasks unless reset with `reset()`.

#### Example: Web Search/Analysis

```python
from typing import Sequence
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import AgentEvent, ChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Mock tools for demonstration
def search_web_tool(query: str) -> str:
    if "2006-2007" in query:
        return """Here are the total points scored by Miami Heat players in the 2006-2007 season:
        Udonis Haslem: 844 points
        Dwayne Wade: 1397 points
        James Posey: 550 points
        ..."""
    elif "2007-2008" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
    elif "2008-2009" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
    return "No data found."

def percentage_change_tool(start: float, end: float) -> float:
    return ((end - start) / start) * 100

# Create agents
model_client = OpenAIChatCompletionClient(model="gpt-4o")

planning_agent = AssistantAgent(
    "PlanningAgent",
    description="An agent for planning tasks, this agent should be the first to engage when given a new task.",
    model_client=model_client,
    system_message="""
    You are a planning agent.
    Your job is to break down complex tasks into smaller, manageable subtasks.
    Your team members are:
        WebSearchAgent: Searches for information
        DataAnalystAgent: Performs calculations
    You only plan and delegate tasks - you do not execute them yourself.
    When assigning tasks, use this format:
    1. <agent> : <task>
    After all tasks are complete, summarize the findings and end with "TERMINATE".
    """
)

web_search_agent = AssistantAgent(
    "WebSearchAgent",
    description="An agent for searching information on the web.",
    tools=[search_web_tool],
    model_client=model_client,
    system_message="""
    You are a web search agent.
    Your only tool is search_tool - use it to find information.
    You make only one search call at a time.
    Once you have the results, you never do calculations based on them.
    """
)

data_analyst_agent = AssistantAgent(
    "DataAnalystAgent",
    description="An agent for performing calculations.",
    model_client=model_client,
    tools=[percentage_change_tool],
    system_message="""
    You are a data analyst.
    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.
    If you have not seen the data, ask for it.
    """
)

# Termination conditions
text_mention_termination = TextMentionTermination("TERMINATE")
max_messages_termination = MaxMessageTermination(max_messages=25)
termination = text_mention_termination | max_messages_termination

# Selector prompt
selector_prompt = """Select an agent to perform task.
{roles}
Current conversation context:
{history}
Read the above conversation, then select an agent from {participants} to perform the next task.
Make sure the planner agent has assigned tasks before other agents start working.
Only select one agent.
"""

# Create the team
team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=model_client,
    termination_condition=termination,
    selector_prompt=selector_prompt,
    allow_repeated_speaker=True,
)

### Using Reasoning Models

When using reasoning models like `o3-mini`, keep prompts and system messages simple, as these models are adept at generating instructions from context. This eliminates the need for a planning agent, as the `SelectorGroupChat` can handle task breakdowns autonomously.

#### Example Setup with `o3-mini`

```python
model_client = OpenAIChatCompletionClient(model="o3-mini")

web_search_agent = AssistantAgent(
    "WebSearchAgent",
    description="An agent for searching information on the web.",
    tools=[search_web_tool],
    model_client=model_client,
    system_message="Use web search tool to find information."
)

data_analyst_agent = AssistantAgent(
    "DataAnalystAgent",
    description="An agent for performing calculations.",
    model_client=model_client,
    tools=[percentage_change_tool],
    system_message="Use tool to perform calculation. If you have not seen the data, ask for it."
)

user_proxy_agent = UserProxyAgent(
    "UserProxyAgent",
    description="A user to approve or disapprove tasks."
)

selector_prompt = """Select an agent to perform task.

{roles}

Current conversation context:

{history}

Read the above conversation, then select an agent from {participants} to perform the next task.

When the task is complete, let the user approve or disapprove the task.
"""

team = SelectorGroupChat(
    [web_search_agent, data_analyst_agent, user_proxy_agent],
    model_client=model_client,
    termination_condition=termination,
    selector_prompt=selector_prompt,
    allow_repeated_speaker=True,
)

await Console(team.run_stream(task=task))
```

### Swarm Pattern

`Swarm` is a multi-agent design pattern where agents delegate tasks based on capabilities, sharing the same message context. This allows for decentralized task planning.

#### Example: Customer Support with Handoff

```python
def refund_flight(flight_id: str) -> str:
    """Refund a flight"""
    return f"Flight {flight_id} refunded"

model_client = OpenAIChatCompletionClient(model="gpt-4o")

travel_agent = AssistantAgent(
    "travel_agent",
    model_client=model_client,
    handoffs=["flights_refunder", "user"],
    system_message="""You are a travel agent.
    The flights_refunder is in charge of refunding flights.
    If you need information from the user, you must first send your message, then you can handoff to the user.
    Use TERMINATE when the travel planning is complete."""
)

flights_refunder = AssistantAgent(
    "flights_refunder",
    model_client=model_client,
    handoffs=["travel_agent", "user"],
    tools=[refund_flight],
    system_message="""You are an agent specialized in refunding flights.
    You only need flight reference numbers to refund a flight.
    You have the ability to refund a flight using the refund_flight tool.
    If you need information from the user, you must first send your message, then you can handoff to the user.
    When the transaction is complete, handoff to the travel agent to finalize."""
)

termination = HandoffTermination(target="user") | TextMentionTermination("TERMINATE")

team = Swarm([travel_agent, flights_refunder], termination_condition=termination)

task = "I need to refund my flight."

async def run_team_stream() -> None:
    task_result = await Console(team.run_stream(task=task))
    last_message = task_result.messages[-1]

    while isinstance(last_message, HandoffMessage) and last_message.target == "user":
        user_message = input("User: ")
        task_result = await Console(
            team.run_stream(task=HandoffMessage(source="user", target=last_message.source, content=user_message))
        )
        last_message = task_result.messages[-1]

await run_team_stream()
```

This setup demonstrates how agents can autonomously handle tasks and handoffs, ensuring efficient task management without a central orchestrator.

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    api_key="sk-xxx"
)
```

### Model Client for OpenAI-Compatible APIs

In v0.4, you can use the `OpenAIChatCompletionClient` for OpenAI-compatible APIs. This client supports the same interface and can be configured similarly.

### Model Client Cache

To enable caching in v0.4, you can use the `Cache` class to store and retrieve model responses, reducing API calls and improving performance.

### Assistant Agent

In v0.4, the `AssistantAgent` is created using the `AssistantAgent` class. You can configure it with a model client and other parameters.

```python
from autogen_agentchat.agents import AssistantAgent

assistant_agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    system_message="Use tools to solve tasks."
)
```

### Multi-Modal Agent

The `MultimodalWebSurfer` agent in v0.4 can be used for web interactions. It is configured with a model client and can operate in headless mode.

```python
from autogen_ext.agents.web_surfer import MultimodalWebSurfer

web_surfer = MultimodalWebSurfer(
    name="web_surfer",
    model_client=model_client,
    headless=False
)
```

### User Proxy

The `UserProxyAgent` in v0.4 allows for user interaction within the agent framework.

```python
from autogen_agentchat.agents import UserProxyAgent

user_proxy = UserProxyAgent(name="user")
```

### Group Chat

In v0.4, you can create a group chat using the `RoundRobinGroupChat` class, which allows for multiple agents to interact in a round-robin fashion.

```python
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import MaxMessageTermination

team = RoundRobinGroupChat(
    participants=[assistant_agent],
    termination_condition=MaxMessageTermination(2)
)
```

### Serialization

AutoGen v0.4 supports serialization of components, allowing you to save and load configurations easily.

```python
agent_config = assistant_agent.dump_component()
print(agent_config.model_dump_json())

agent_new = assistant_agent.load_component(agent_config)
```

### Logging

To enable logging in v0.4, use Python’s built-in logging module to configure log levels and handlers.

```python
import logging
from autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)

trace_logger = logging.getLogger(TRACE_LOGGER_NAME)
trace_logger.addHandler(logging.StreamHandler())
trace_logger.setLevel(logging.DEBUG)

event_logger = logging.getLogger(EVENT_LOGGER_NAME)
event_logger.addHandler(logging.StreamHandler())
event_logger.setLevel(logging.DEBUG)
```

### Memory

The `Memory` protocol in v0.4 allows for storing and retrieving information, which can be used to maintain context across interactions.

```python
from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType

user_memory = ListMemory()
await user_memory.add(MemoryContent(content="The weather should be in metric units", mime_type=MemoryMimeType.TEXT))
```

### Caution

When using Magentic-One or any multi-agent system, ensure you run tasks in a controlled environment to prevent unintended consequences. Use containers, virtual environments, and monitor logs to safeguard data and system integrity.

For more detailed examples and configurations, refer to the AutoGen documentation and tutorials.

```python
        """
    )

    web_search_agent = AssistantAgent(
        "WebSearchAgent",
        description="An agent for searching the web.",
        model_client=model_client,
        system_message="""
        You are a web search agent.
        Your job is to search the web for information requested by the planning agent.
        """
    )

    data_analyst = AssistantAgent(
        "DataAnalyst",
        description="An agent for analyzing data.",
        model_client=model_client,
        system_message="""
        You are a data analyst.
        Your job is to perform calculations and analyze data as requested by the planning agent.
        """
    )

    # Define a custom selector function
    def selector_func(messages: Sequence[ChatMessage | AgentEvent]) -> str | None:
        if not messages:
            return "PlanningAgent"
        last_message = messages[-1]
        if isinstance(last_message, ChatMessage):
            if "WebSearchAgent" in last_message.content:
                return "WebSearchAgent"
            elif "DataAnalyst" in last_message.content:
                return "DataAnalyst"
        return None

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)

    return SelectorGroupChat(
        agents=[planning_agent, web_search_agent, data_analyst],
        termination_condition=termination,
        selector_func=selector_func
    )

async def main() -> None:
    group_chat = create_team()
    stream = group_chat.run_stream(
        task="Analyze the performance of Dwayne Wade in the Miami Heat seasons 2006-2009."
    )
    await Console(stream)

asyncio.run(main())
```

This example demonstrates how to set up a group chat with a custom selector function to manage task delegation among agents. The `SelectorGroupChat` allows for a more flexible and state-driven approach to agent interaction, enabling complex workflows like web search and data analysis.

```python
web_search_agent = AssistantAgent(
    "WebSearchAgent",
    description="A web search agent.",
    tools=[search_web_tool],
    model_client=model_client,
    system_message="""
        You are a web search agent.
        Your only tool is search_tool - use it to find information.
        You make only one search call at a time.
        Once you have the results, you never do calculations based on them.
    """
)

data_analyst_agent = AssistantAgent(
    "DataAnalystAgent",
    description="A data analyst agent. Useful for performing calculations.",
    model_client=model_client,
    tools=[percentage_change_tool],
    system_message="""
        You are a data analyst.
        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.
    """
)

text_mention_termination = TextMentionTermination("TERMINATE")
max_messages_termination = MaxMessageTermination(max_messages=25)
termination = text_mention_termination | max_messages_termination

def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
    if messages[-1].source != planning_agent.name:
        return planning_agent.name
    return None

team = SelectorGroupChat(
    [planning_agent, web_search_agent, data_analyst_agent],
    model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"),
    termination_condition=termination,
    selector_func=selector_func,
)

return team

async def main() -> None:
    team = create_team()
    task = "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?"
    await Console(team.run_stream(task=task))

asyncio.run(main())
```

### Nested Chat

Nested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group.

In v0.4, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the `on_messages` method to trigger the nested team or agent.

```python
import asyncio
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response

class CountingAgent(BaseChatAgent):
    """An agent that returns a new number by adding 1 to the last number in the input messages."""
    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        if len(messages) == 0:
            last_number = 0
        else:
            assert isinstance(messages[-1], TextMessage)
            last_number = int(messages[-1].content)
        return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)

class NestedCountingAgent(BaseChatAgent):
    """An agent that increments the last number in the input messages multiple times using a nested counting team."""
    def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
        super().__init__(name, description="An agent that counts numbers.")
        self._counting_team = counting_team

    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
        result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
        assert isinstance(result.messages[-1], TextMessage)
        return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        await self._counting_team.reset()

    @property
    def produced_message_types(self) -> Sequence[type[ChatMessage]]:
        return (TextMessage,)

async def main() -> None:
    counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
    counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
    counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
    nested_counting_agent = NestedCountingAgent("